
@article{eichelberger_hazard_1965,
	title = {Hazard {Detection} in {Combinational} and {Sequential} {Switching} {Circuits}},
	volume = {9},
	issn = {0018-8646},
	url = {https://ieeexplore.ieee.org/abstract/document/5392161},
	doi = {10.1147/rd.92.0090},
	abstract = {This paper is concerned with a unified approach to the detection of hazards in both combinational and sequential circuits through the use of ternary algebra. First, hazards in a combinational network resulting from the simultaneous changing of two or more inputs are discussed. A technique is described that will detect hazards resulting from both single- and multiple-input changes. The various types of hazards connected with gate-type sequential circuits are also discussed, and a general technique is described that will detect any type of hazard or race condition that could result in an incorrect terminal state. This technique could be easily implemented in a computer program which would be capable of detecting hazards in circuits containing hundreds of logic blocks.},
	number = {2},
	urldate = {2025-07-05},
	journal = {IBM Journal of Research and Development},
	author = {Eichelberger, E. B.},
	month = mar,
	year = {1965},
	pages = {90--99},
}

@article{fearnley_unique_2020,
	title = {Unique end of potential line},
	volume = {114},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000020300520},
	doi = {10.1016/j.jcss.2020.05.007},
	abstract = {The complexity class CLS was proposed by Daskalakis and Papadimitriou in 2011 to understand   the complexity of important NP search problems that admit both path following and potential optimizing algorithms. Here we identify a subclass of CLS – called UniqueEOPL – that applies a more specific combinatorial principle that guarantees unique solutions. We show that UniqueEOPL contains several important problems such as the P-matrix Linear Complementarity Problem, finding fixed points of Contraction Maps, and solving Unique Sink Orientations (USOs). We identify a problem – closely related to solving contraction maps and USOs – that is complete for UniqueEOPL.},
	urldate = {2025-07-04},
	journal = {Journal of Computer and System Sciences},
	author = {Fearnley, John and Gordon, Spencer and Mehta, Ruta and Savani, Rahul},
	month = dec,
	year = {2020},
	keywords = {Continuous local search, Contraction map, P-matrix Linear Complementarity Problem, TFNP, Total search problems, Unique sink orientation},
	pages = {1--35},
}

@book{kozen2006theory,
	series = {Texts in computer science},
	title = {Theory of computation},
	isbn = {978-1-84628-297-3},
	url = {https://books.google.co.uk/books?id=AolrsLBq3u0C},
	publisher = {Springer London},
	author = {Kozen, D.C.},
	year = {2006},
	note = {tex.lccn: 2005937504},
}

@article{ikenmeyer_positivity_2024,
	title = {Positivity of the {Symmetric} {Group} {Characters} {Is} as {Hard} as the {Polynomial} {Time} {Hierarchy}},
	volume = {2024},
	issn = {1073-7928},
	url = {https://doi.org/10.1093/imrn/rnad273},
	doi = {10.1093/imrn/rnad273},
	abstract = {We prove that deciding the vanishing of the character of the symmetric group is \${\textbackslash}textsf\{C\}\_= {\textbackslash}textsf\{P\}\$-complete. We use this hardness result to prove that the absolute value and also the square of the character are not contained in \${\textbackslash}textsf\{\#P\}\$, unless the polynomial hierarchy collapses to the second level. This rules out the existence of any (unsigned) combinatorial description for the square of the characters. As a byproduct of our proof, we conclude that deciding positivity of the character is \${\textbackslash}textsf\{PP\}\$-complete under many-one reductions, and hence \${\textbackslash}textsf\{PH\}\$-hard under Turing reductions.},
	number = {10},
	urldate = {2025-06-28},
	journal = {International Mathematics Research Notices},
	author = {Ikenmeyer, Christian and Pak, Igor and Panova, Greta},
	month = may,
	year = {2024},
	pages = {8442--8458},
}

@misc{pak_what_2022,
	title = {What is a combinatorial interpretation?},
	url = {http://arxiv.org/abs/2209.06142},
	doi = {10.48550/arXiv.2209.06142},
	abstract = {In this survey we discuss the notion of combinatorial interpretation in the context of Algebraic Combinatorics and related areas. We approach the subject from the Computational Complexity perspective. We review many examples, state a workable definition, discuss many open problems, and present recent results on the subject.},
	urldate = {2025-06-28},
	publisher = {arXiv},
	author = {Pak, Igor},
	month = sep,
	year = {2022},
	note = {arXiv:2209.06142 [math]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Discrete Mathematics, Mathematics - Combinatorics},
}

@inproceedings{inproceedings,
	title = {Krawczyk operator revised},
	author = {Shary, Sergey},
	month = jun,
	year = {2004},
}

@article{daskalakis_complexity_2009,
	title = {The {Complexity} of {Computing} a {Nash} {Equilibrium}},
	volume = {39},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/10.1137/070699652},
	doi = {10.1137/070699652},
	abstract = {We reexamine what it means to compute Nash equilibria and, more generally, what it means to compute a fixed point of a given Brouwer function, and we investigate the complexity of the associated problems. Specifically, we study the complexity of the following problem: given a finite game, \${\textbackslash}Gamma\$, with 3 or more players, and given \${\textbackslash}epsilon{\textgreater}0\$, compute an approximation within \${\textbackslash}epsilon\$ of some (actual) Nash equilibrium. We show that approximation of an actual Nash equilibrium, even to within any nontrivial constant additive factor \${\textbackslash}epsilon{\textless}1/2\$ in just one desired coordinate, is at least as hard as the long-standing square-root sum problem, as well as a more general arithmetic circuit decision problem that characterizes P-time in a unit-cost model of computation with arbitrary precision rational arithmetic; thus, placing the approximation problem in P, or even NP, would resolve major open problems in the complexity of numerical computation. We show similar results for market equilibria: it is hard to estimate with any nontrivial accuracy the equilibrium prices in an exchange economy with a unique equilibrium, where the economy is given by explicit algebraic formulas for the excess demand functions. We define a class, FIXP, which captures search problems that can be cast as fixed point computation problems for functions represented by algebraic circuits (straight line programs) over basis \${\textbackslash}\{+,*,-,/,{\textbackslash}max,{\textbackslash}min{\textbackslash}\}\$ with rational constants. We show that the (exact or approximate) computation of Nash equilibria for 3 or more players is complete for FIXP. The price equilibrium problem for exchange economies with algebraic demand functions is another FIXP-complete problem. We show that the piecewise linear fragment of FIXP equals PPAD. Many other problems in game theory, economics, and probability theory can be cast as fixed point problems for such algebraic functions. We discuss several important such problems: computing the value of Shapley's stochastic games and the simpler games of Condon, extinction probabilities of branching processes, probabilities of stochastic context-free grammars, and termination probabilities of recursive Markov chains. We show that for some of them, the approximation, or even exact computation, problem can be placed in PPAD, while for others, they are at least as hard as the square-root sum and arithmetic circuit decision problems.},
	number = {1},
	urldate = {2025-04-25},
	journal = {SIAM Journal on Computing},
	author = {Daskalakis, Constantinos and Goldberg, Paul W. and Papadimitriou, Christos H.},
	month = jan,
	year = {2009},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {195--259},
}

@article{chen_complexity_2009,
	series = {Automata, {Languages} and {Programming} ({ICALP} 2006)},
	title = {On the complexity of {2D} discrete fixed point problem},
	volume = {410},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S030439750900499X},
	doi = {10.1016/j.tcs.2009.07.052},
	abstract = {We study a computational complexity version of the 2D Sperner problem, which states that any three coloring of vertices of a triangulated triangle, satisfying some boundary conditions, will have a trichromatic triangle. In introducing a complexity class PPAD, Papadimitriou [C.H. Papadimitriou, On graph-theoretic lemmata and complexity classes, in: Proceedings of the 31st Annual Symposium on Foundations of Computer Science, 1990, 794–801] proved that its 3D analogue is PPAD-complete about fifteen years ago. The complexity of 2D-SPERNER itself has remained open since then. We settle this open problem with a PPAD-completeness proof. The result also allows us to derive the computational complexity characterization of a discrete version of the 2D Brouwer fixed point problem, improving a recent result of Daskalakis, Goldberg and Papadimitriou [C. Daskalakis, P.W. Goldberg, C.H. Papadimitriou, The complexity of computing a Nash equilibrium, in: Proceedings of the 38th Annual ACM Symposium on Theory of Computing (STOC), 2006]. Those hardness results for the simplest version of those problems provide very useful tools to the study of other important problems in the PPAD class.},
	number = {44},
	urldate = {2025-04-25},
	journal = {Theoretical Computer Science},
	author = {Chen, Xi and Deng, Xiaotie},
	month = oct,
	year = {2009},
	pages = {4448--4456},
}

@inproceedings{daskalakis_complexity_2021,
	address = {New York, NY, USA},
	series = {{STOC} 2021},
	title = {The complexity of constrained min-max optimization},
	isbn = {978-1-4503-8053-9},
	url = {https://dl.acm.org/doi/10.1145/3406325.3451125},
	doi = {10.1145/3406325.3451125},
	abstract = {Despite its important applications in Machine Learning, min-max optimization of objective functions that are nonconvex-nonconcave remains elusive. Not only are there no known first-order methods converging to even approximate local min-max equilibria (a.k.a.\&nbsp;approximate saddle points), but the computational complexity of identifying them is also poorly understood. In this paper, we provide a characterization of the computational complexity as well as of the limitations of first-order methods in this problem. Specifically, we show that in linearly constrained min-max optimization problems with nonconvex-nonconcave objectives an approximate local min-max equilibrium of large enough approximation is guaranteed to exist, but computing such a point is PPAD-complete. The same is true of computing an approximate fixed point of the (Projected) Gradient Descent/Ascent update dynamics, which is computationally equivalent to computing approximate local min-max equilibria. An important byproduct of our proof is to establish an unconditional hardness result in the\&nbsp;Nemirovsky-Yudin 1983 oracle optimization model, where we are given oracle access to the values of some function f : P → [−1, 1] and its gradient ∇ f, where P ⊆ [0, 1]d is a known convex polytope. We show that any algorithm that uses such first-order oracle access to f and finds an ε-approximate local min-max equilibrium needs to make a number of oracle queries that is exponential in at least one of 1/ε, L, G, or d, where L and G are respectively the smoothness and Lipschitzness of f. This comes in sharp contrast to minimization problems, where finding approximate local minima in the same setting can be done with Projected Gradient Descent using O(L/ε) many queries. Our result is the first to show an exponential separation between these two fundamental optimization problems in the oracle model.},
	urldate = {2025-04-25},
	booktitle = {Proceedings of the 53rd {Annual} {ACM} {SIGACT} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
	month = jun,
	year = {2021},
	pages = {1466--1478},
}

@inproceedings{ikenmeyer_what_2022,
	title = {What is in \#{P} and what is not?},
	url = {https://ieeexplore.ieee.org/abstract/document/9996676},
	doi = {10.1109/FOCS54457.2022.00087},
	abstract = {For several classical nonnegative integer functions we investigate if they are members of the counting complexity class \# P or not. We prove \# P membership in surprising cases, and in other cases we prove non-membership, relying on standard complexity assumptions or on oracle separations. We initiate the study of the polynomial closure properties of \# P on affine varieties, i.e., if all problem instances satisfy algebraic constraints. This is directly linked to classical combinatorial proofs of algebraic identities and inequalities. We investigate \# TFNP and obtain oracle separations that prove the strict inclusion of \# P in all standard syntactic subclasses of \# TFNP minus 1.},
	urldate = {2025-04-24},
	booktitle = {2022 {IEEE} 63rd {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS})},
	author = {Ikenmeyer, Christian and Pak, Igor},
	month = oct,
	year = {2022},
	note = {ISSN: 2575-8454},
	keywords = {\#P, Complexity theory, Computer science, Counting complexity, GapP, Standards, Syntactics, TFNP, combinatorial proofs},
	pages = {860--871},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2025-04-06},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{di_ranking-based_2020,
	address = {Cham},
	title = {Ranking-{Based} {Survival} {Prediction} on {Histopathological} {Whole}-{Slide} {Images}},
	isbn = {978-3-030-59722-1},
	doi = {10.1007/978-3-030-59722-1_41},
	abstract = {Survival prediction for patients based on gigapixel histopathological whole-slide images (WSIs) has attracted increasing attention in recent years. Previous studies mainly focus on the framework of predicting the survival hazard scores based on one individual WSI for each patient directly. These prediction methods ignore the relative survival differences among patients, i.e., the ranking information, which is important for a regression task. Under such circumstances, we propose a ranking-based survival prediction method on WSIs – RankSurv, which takes the ranking information into consideration during the learning process. First, a hypergraph representation is introduced to conduct hazard prediction on each WSI respectively, which is able to learn the high-order correlation among different patches in the WSI. Then, a ranking-based prediction process is conducted using pairwise survival data. Experiments are conducted on three public carcinoma datasets (i.e., LUSC, GBM, and NLST). Quantitative results show that the proposed method significantly outperforms state-of-the-art methods on all three datasets, which demonstrates the effectiveness of the proposed ranking-based survival prediction framework.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Di, Donglin and Li, Shengrui and Zhang, Jun and Gao, Yue},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Hypergraph, Ranking, Survival prediction, WSI},
	pages = {428--438},
}

@article{rahangdale_machine_2019,
	title = {Machine {Learning} {Methods} for {Ranking}},
	volume = {29},
	issn = {0218-1940},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S021819401930001X},
	doi = {10.1142/S021819401930001X},
	abstract = {Learning-to-rank is one of the learning frameworks in machine learning and it aims to organize the objects in a particular order according to their preference, relevance or ranking. In this paper, we give a comprehensive survey for learning-to-rank. First, we discuss the different approaches along with different machine learning methods such as regression, SVM, neural network-based, evolutionary, boosting method. In order to compare different approaches: we discuss the characteristics of each approach. In addition to that, learning-to-rank algorithms combine with other machine learning paradigms such as semi-supervised learning, active learning, reinforcement learning and deep learning. The learning-to-rank models employ with parallel or big data analytics to review computational and storage advantage. Many real-time applications use learning-to-rank for preference learning. In regard to this, we introduce some representative works. Finally, we highlighted future directions to investigate learning-to-rank methods.},
	number = {06},
	urldate = {2025-04-05},
	journal = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Rahangdale, Ashwini and Raut, Shital},
	month = jun,
	year = {2019},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {Learning-to-rank, artificial intelligence, machine learning, supervised learning},
	pages = {729--761},
}

@inproceedings{nguyen_bertweet_2020,
	address = {Online},
	title = {{BERTweet}: {A} pre-trained language model for {English} {Tweets}},
	shorttitle = {{BERTweet}},
	url = {https://aclanthology.org/2020.emnlp-demos.2/},
	doi = {10.18653/v1/2020.emnlp-demos.2},
	abstract = {We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet},
	urldate = {2025-04-05},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Nguyen, Dat Quoc and Vu, Thanh and Tuan Nguyen, Anh},
	editor = {Liu, Qun and Schlangen, David},
	month = oct,
	year = {2020},
	pages = {9--14},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-04-05},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{Musgrave2020PyTorchML,
	title = {{PyTorch} metric learning},
	volume = {abs/2008.09164},
	journal = {ArXiv},
	author = {Musgrave, Kevin and Belongie, Serge J. and Lim, Ser-Nam},
	year = {2020},
}

@article{xin_improved_2022,
	title = {An improved transformer network for skin cancer classification},
	volume = {149},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482522006746},
	doi = {10.1016/j.compbiomed.2022.105939},
	abstract = {Background
Use of artificial intelligence to identify dermoscopic images has brought major breakthroughs in recent years to the early diagnosis and early treatment of skin cancer, the incidence of which is increasing year by year worldwide and poses a great threat to human health. Achievements have been made in the research of skin cancer image classification by using the deep backbone of the convolutional neural network (CNN). This approach, however, only extracts the features of small objects in the image, and cannot locate the important parts.
Objectives
As a result, researchers of the paper turn to vision transformers (VIT) which has demonstrated powerful performance in traditional classification tasks. The self-attention is to improve the value of important features and suppress the features that cause noise. Specifically, an improved transformer network named SkinTrans is proposed.
Innovations
To verify its efficiency, a three step procedure is followed. Firstly, a VIT network is established to verify the effectiveness of SkinTrans in skin cancer classification. Then multi-scale and overlapping sliding windows are used to serialize the image and multi-scale patch embedding is carried out which pay more attention to multi-scale features. Finally, contrastive learning is used which makes the similar data of skin cancer encode similarly so that the encoding results of different data are as different as possible.
Main results
The experiment is carried out based on two datasets, namely (1) HAM10000: a large dataset of multi-source dermatoscopic images of common skin cancers; (2)A clinical dataset of skin cancer collected by dermoscopy. The model proposed has achieved 94.3\% accuracy on HAM10000 and 94.1\% accuracy on our datasets, which verifies the efficiency of SkinTrans.
Conclusions
The transformer network has not only achieved good results in natural language but also achieved ideal results in the field of vision, which also lays a good foundation for skin cancer classification based on multimodal data. This paper is convinced that it will be of interest to dermatologists, clinical researchers, computer scientists and researchers in other related fields, and provide greater convenience for patients.},
	urldate = {2025-04-05},
	journal = {Computers in Biology and Medicine},
	author = {Xin, Chao and Liu, Zhifang and Zhao, Keyu and Miao, Linlin and Ma, Yizhao and Zhu, Xiaoxia and Zhou, Qiongyan and Wang, Songting and Li, Lingzhi and Yang, Feng and Xu, Suling and Chen, Haijiang},
	month = oct,
	year = {2022},
	keywords = {Classification, Contrastive learning, Skin cancer, Vision transformer},
	pages = {105939},
}

@inproceedings{zhang_ynu-hpcc_2018,
	address = {New Orleans, Louisiana},
	title = {{YNU}-{HPCC} at {SemEval}-2018 {Task} 1: {BiLSTM} with {Attention} based {Sentiment} {Analysis} for {Affect} in {Tweets}},
	shorttitle = {{YNU}-{HPCC} at {SemEval}-2018 {Task} 1},
	url = {https://aclanthology.org/S18-1040/},
	doi = {10.18653/v1/S18-1040},
	abstract = {We implemented the sentiment system in all five subtasks for English and Spanish. All subtasks involve emotion or sentiment intensity prediction (regression and ordinal classification) and emotions determining (multi-labels classification). The useful BiLSTM (Bidirectional Long-Short Term Memory) model with attention mechanism was mainly applied for our system. We use BiLSTM in order to get word information extracted from both directions. The attention mechanism was used to find the contribution of each word for improving the scores. Furthermore, based on BiLSTMATT (BiLSTM with attention mechanism) a few deep-learning algorithms were employed for different subtasks. For regression and ordinal classification tasks we used domain adaptation and ensemble learning methods to leverage base model. While a single base model was used for multi-labels task.},
	urldate = {2025-04-05},
	booktitle = {Proceedings of the 12th {International} {Workshop} on {Semantic} {Evaluation}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, You and Wang, Jin and Zhang, Xuejie},
	editor = {Apidianaki, Marianna and Mohammad, Saif M. and May, Jonathan and Shutova, Ekaterina and Bethard, Steven and Carpuat, Marine},
	month = jun,
	year = {2018},
	pages = {273--278},
}

@inproceedings{wei_few-shot_2021,
	address = {Online},
	title = {Few-{Shot} {Text} {Classification} with {Triplet} {Networks}, {Data} {Augmentation}, and {Curriculum} {Learning}},
	url = {https://aclanthology.org/2021.naacl-main.434/},
	doi = {10.18653/v1/2021.naacl-main.434},
	abstract = {Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation—a technique particularly suitable for training with limited data—for this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0\% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation.},
	urldate = {2025-04-04},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wei, Jason and Huang, Chengyu and Vosoughi, Soroush and Cheng, Yu and Xu, Shiqi},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {5493--5500},
}

@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: {A} {Unified} {Embedding} for {Face} {Recognition} and {Clustering}},
	shorttitle = {{FaceNet}},
	url = {http://arxiv.org/abs/1503.03832},
	doi = {10.1109/CVPR.2015.7298682},
	abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
	urldate = {2025-04-04},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	month = jun,
	year = {2015},
	note = {arXiv:1503.03832 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {815--823},
}

@misc{ma2019nlpaug,
	title = {{NLP} augmentation},
	url = {https://github.com/makcedward/nlpaug},
	author = {Ma, Edward},
	year = {2019},
}

@inproceedings{nguyen_bertweet_2020-1,
	address = {Online},
	title = {{BERTweet}: {A} pre-trained language model for {English} {Tweets}},
	shorttitle = {{BERTweet}},
	url = {https://aclanthology.org/2020.emnlp-demos.2/},
	doi = {10.18653/v1/2020.emnlp-demos.2},
	abstract = {We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet},
	urldate = {2025-04-04},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Nguyen, Dat Quoc and Vu, Thanh and Tuan Nguyen, Anh},
	editor = {Liu, Qun and Schlangen, David},
	month = oct,
	year = {2020},
	pages = {9--14},
}

@article{song_deep_2022,
	title = {Deep learning-based automatic segmentation of images in cardiac radiography: {A} promising challenge},
	volume = {220},
	issn = {0169-2607},
	shorttitle = {Deep learning-based automatic segmentation of images in cardiac radiography},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260722002036},
	doi = {10.1016/j.cmpb.2022.106821},
	abstract = {Background
Due to the advancement of medical imaging and computer technology, machine intelligence to analyze clinical image data increases the probability of disease prevention and successful treatment. When diagnosing and detecting heart disease, medical imaging can provide high-resolution scans of every organ or tissue in the heart. The diagnostic results obtained by the imaging method are less susceptible to human interference. They can process numerous patient information, assist doctors in early detection of heart disease, intervene and treat patients, and improve the understanding of heart disease symptoms and clinical diagnosis of great significance. In a computer-aided diagnosis system, accurate segmentation of cardiac scan images is the basis and premise of subsequent thoracic function analysis and 3D image reconstruction.
Existing techniques
This paper systematically reviews automatic methods and some difficulties for cardiac segmentation in radiographic images. Combined with recent advanced deep learning techniques, the feasibility of using deep learning network models for image segmentation is discussed, and the commonly used deep learning frameworks are compared.
Developed insights
There are many standard methods for medical image segmentation, such as traditional methods based on regions and edges and methods based on deep learning. Because of characteristics of non-uniform grayscale, individual differences, artifacts and noise of medical images, the above image segmentation methods have certain limitations. It is tough to obtain the needed results sensitivity and accuracy when performing heart segmentation. The deep learning model proposed has achieved good results in image segmentation. Accurate segmentation improves the accuracy of disease diagnosis and reduces subsequent irrelevant computations.
Summary
There are two requirements for accurate segmentation of radiological images. One is to use image segmentation to improve the development of computer-aided diagnosis. The other is to achieve complete segmentation of the heart. When there are lesions or deformities in the heart, there will be some abnormalities in the radiographic images, and the segmentation algorithm needs to segment the heart altogether. The quantity of processing inside a certain range will no longer be a restriction for real-time detection with the advancement of deep learning and the enhancement of hardware device performance.},
	urldate = {2025-04-04},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Song, Yucheng and Ren, Shengbing and Lu, Yu and Fu, Xianghua and Wong, Kelvin K. L.},
	month = jun,
	year = {2022},
	keywords = {Cardiovascular image segmentation, Deep medicine, Heart disease, Medical image analysis},
	pages = {106821},
}

@article{oshaughnessy_trends_2024,
	title = {Trends and developments in automatic speech recognition research},
	volume = {83},
	issn = {0885-2308},
	url = {https://www.sciencedirect.com/science/article/pii/S0885230823000578},
	doi = {10.1016/j.csl.2023.101538},
	abstract = {This paper discusses how automatic speech recognition systems are and could be designed, in order to best exploit the discriminative information encoded in human speech. This contrasts with many recent machine learning approaches that apply general recognition architectures to signals to identify, with little concern for the nature of the input. The implicit assumption has often been that training can automatically discover the useful properties that exist in signals, with minimal manual intervention. These approaches may be suitable for some tasks such as image recognition, where the diversity of visual input is vast; e.g., an image may be any (natural or synthetic) scene that a camera views. We first examine what makes speech special, i.e., a natural signal from a complex tube, driven by a source that is quasi-periodic and/or noisy, aiming to communicate a wide variety of information, using the different vocal systems of human speakers. Then, we view how pertinent features are extracted from speech via efficient means, related to the objectives of communication. We see how to reliably and efficiently identify the different units of oral language. We learn from the history of attempts to do ASR, e.g., why they succeeded and how improved methods exploited the increasing availability of data and computer power (in particular, deep neural networks). Finally, we suggest ways to render ASR both more accurate and efficient. This work is aimed at both newcomers to ASR and experts, in terms of presenting issues broadly, but without mathematical or algorithmic details, which are readily found in the references.},
	urldate = {2025-04-04},
	journal = {Computer Speech \& Language},
	author = {O'Shaughnessy, Douglas},
	month = jan,
	year = {2024},
	pages = {101538},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423/},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-04-04},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162/},
	doi = {10.3115/v1/D14-1162},
	urldate = {2025-04-04},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
	month = oct,
	year = {2014},
	pages = {1532--1543},
}

@article{miller_wordnet_1995,
	title = {{WordNet}: a lexical database for {English}},
	volume = {38},
	issn = {0001-0782},
	shorttitle = {{WordNet}},
	url = {https://dl.acm.org/doi/10.1145/219717.219748},
	doi = {10.1145/219717.219748},
	abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
	number = {11},
	urldate = {2025-04-04},
	journal = {Commun. ACM},
	author = {Miller, George A.},
	month = nov,
	year = {1995},
	pages = {39--41},
}

@article{taylor_penn_2003,
	title = {The {Penn} {Treebank}: {An} overview},
	issn = {978-1-4020-1335-5},
	shorttitle = {The {Penn} {Treebank}},
	doi = {10.1007/978-94-010-0201-1_1},
	abstract = {The Penn Treebank, in its eight years of operation (1989-1996), produced approximately 7 million words of part-of-speech tagged text, 3 million words of skeletally parsed text, over 2 million words of text parsed for predicateargument structure, and 1.6 million words of transcribed spoken text annotated for speech disfluencies. This paper describes the design of the three annotation schemes used by the Treebank: POS tagging, syntactic bracketing, and disfluency annotation and the methodology employed in production. All available http://www.ldc.upenn.edu.},
	author = {Taylor, Ann and Marcus, Mitchell and Santorini, Beatrice},
	month = jan,
	year = {2003},
}

@misc{Garbe_SymSpell_2012,
	title = {{SymSpell}},
	copyright = {MIT},
	url = {hhttps://github.com/wolfgarbe/SymSpell},
	author = {Garbe, Wolf},
	month = jun,
	year = {2012},
}

@article{sarica_stopwords_2021,
	title = {Stopwords in technical language processing},
	volume = {16},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8341615/},
	doi = {10.1371/journal.pone.0254937},
	abstract = {There are increasing applications of natural language processing techniques for information retrieval, indexing, topic modelling and text classification in engineering contexts. A standard component of such tasks is the removal of stopwords, which are uninformative components of the data. While researchers use readily available stopwords lists that are derived from non-technical resources, the technical jargon of engineering fields contains their own highly frequent and uninformative words and there exists no standard stopwords list for technical language processing applications. Here we address this gap by rigorously identifying generic, insignificant, uninformative stopwords in engineering texts beyond the stopwords in general texts, based on the synthesis of alternative statistical measures such as term frequency, inverse document frequency, and entropy, and curating a stopwords dataset ready for technical language processing applications.},
	number = {8},
	urldate = {2025-04-04},
	journal = {PLoS ONE},
	author = {Sarica, Serhad and Luo, Jianxi},
	month = aug,
	year = {2021},
	pmid = {34351911},
	pmcid = {PMC8341615},
	pages = {e0254937},
}

@inproceedings{kavanagh_assessing_2023,
	title = {Assessing the {Effects} of {Lemmatisation} and {Spell} {Checking} on {Sentiment} {Analysis} of {Online} {Reviews}},
	url = {https://ieeexplore.ieee.org/document/10066648},
	doi = {10.1109/ICSC56153.2023.00046},
	abstract = {With many text preprocessing options, choosing the most efficient pipeline is important for accuracy and computational expense. Online text often contains non-standard English, spelling errors, colloquialisms, emojis, slang and other variations that affect current natural language processing tools, with no clear guidelines for preprocessing this type of text. In this work we analyse text preprocessing techniques using a dataset of online reviews scraped from iTunes and Google Play store. The objective is to measure the efficacy of different combinations of these techniques to maximise the amount of detected sentiment in a dataset of 438,157 reviews. Sentiment detection was performed by two state-of-the-art sentiment analysers (RoBERTa and VADER). Statistical analysis of the results suggest preprocessing strategies for maximising sentiment detected within mental health app reviews and similar text formats.},
	urldate = {2025-04-04},
	booktitle = {2023 {IEEE} 17th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Kavanagh, James and Greenhow, Keith and Jordanous, Anna},
	month = feb,
	year = {2023},
	note = {ISSN: 2325-6516},
	keywords = {Computational modeling, Current measurement, Language parsing and understanding, Mental health, Natural Language Processing, Pipelines, Semantics, Sentiment analysis, Statistical analysis, Text analysis, Web text analysis},
	pages = {235--238},
}

@misc{shah_neelshah18emot_2025,
	title = {{NeelShah18}/emot},
	copyright = {MIT},
	url = {https://github.com/NeelShah18/emot},
	abstract = {Open source Emoticons and Emoji detection library: emot},
	urldate = {2025-04-04},
	author = {Shah, Neel},
	month = jan,
	year = {2025},
	note = {original-date: 2017-06-18T12:53:35Z},
	keywords = {detection, emoji, emoticons, extraction, python},
}

@inproceedings{baziotis_datastories_2017,
	address = {Vancouver, Canada},
	title = {{DataStories} at {SemEval}-2017 {Task} 4: {Deep} {LSTM} with {Attention} for {Message}-level and {Topic}-based {Sentiment} {Analysis}},
	shorttitle = {{DataStories} at {SemEval}-2017 {Task} 4},
	url = {https://aclanthology.org/S17-2126/},
	doi = {10.18653/v1/S17-2126},
	abstract = {In this paper we present two deep-learning systems that competed at SemEval-2017 Task 4 “Sentiment Analysis in Twitter”. We participated in all subtasks for English tweets, involving message-level and topic-based sentiment polarity classification and quantification. We use Long Short-Term Memory (LSTM) networks augmented with two kinds of attention mechanisms, on top of word embeddings pre-trained on a big collection of Twitter messages. Also, we present a text processing tool suitable for social network messages, which performs tokenization, word normalization, segmentation and spell correction. Moreover, our approach uses no hand-crafted features or sentiment lexicons. We ranked 1st (tie) in Subtask A, and achieved very competitive results in the rest of the Subtasks. Both the word embeddings and our text processing tool are available to the research community.},
	urldate = {2025-04-04},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2017)},
	publisher = {Association for Computational Linguistics},
	author = {Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos},
	editor = {Bethard, Steven and Carpuat, Marine and Apidianaki, Marianna and Mohammad, Saif M. and Cer, Daniel and Jurgens, David},
	month = aug,
	year = {2017},
	pages = {747--754},
}

@inproceedings{mohammad_semeval-2016_2016,
	address = {San Diego, California},
	title = {{SemEval}-2016 {Task} 6: {Detecting} {Stance} in {Tweets}},
	shorttitle = {{SemEval}-2016 {Task} 6},
	url = {https://aclanthology.org/S16-1003/},
	doi = {10.18653/v1/S16-1003},
	urldate = {2025-04-03},
	booktitle = {Proceedings of the 10th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2016)},
	publisher = {Association for Computational Linguistics},
	author = {Mohammad, Saif and Kiritchenko, Svetlana and Sobhani, Parinaz and Zhu, Xiaodan and Cherry, Colin},
	editor = {Bethard, Steven and Carpuat, Marine and Cer, Daniel and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
	month = jun,
	year = {2016},
	pages = {31--41},
}

@article{wankhade_survey_2022,
	title = {A survey on sentiment analysis methods, applications, and challenges},
	volume = {55},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10144-1},
	doi = {10.1007/s10462-022-10144-1},
	abstract = {The rapid growth of Internet-based applications, such as social media platforms and blogs, has resulted in comments and reviews concerning day-to-day activities. Sentiment analysis is the process of gathering and analyzing people’s opinions, thoughts, and impressions regarding various topics, products, subjects, and services. People’s opinions can be beneficial to corporations, governments, and individuals for collecting information and making decisions based on opinion. However, the sentiment analysis and evaluation procedure face numerous challenges. These challenges create impediments to accurately interpreting sentiments and determining the appropriate sentiment polarity. Sentiment analysis identifies and extracts subjective information from the text using natural language processing and text mining. This article discusses a complete overview of the method for completing this task as well as the applications of sentiment analysis. Then, it evaluates, compares, and investigates the approaches used to gain a comprehensive understanding of their advantages and disadvantages. Finally, the challenges of sentiment analysis are examined in order to define future directions.},
	language = {en},
	number = {7},
	urldate = {2025-04-03},
	journal = {Artificial Intelligence Review},
	author = {Wankhade, Mayur and Rao, Annavarapu Chandra Sekhara and Kulkarni, Chaitanya},
	month = oct,
	year = {2022},
	keywords = {Artificial Intelligence, Machine learning, Sentiment analysis, Social media, Text analysis, Word embedding},
	pages = {5731--5780},
}

@incollection{li_model-free_2023,
	address = {Singapore},
	title = {Model-{Free} {Indirect} {RL}: {Temporal} {Difference}},
	isbn = {978-981-19778-4-8},
	shorttitle = {Model-{Free} {Indirect} {RL}},
	url = {https://doi.org/10.1007/978-981-19-7784-8\_4},
	abstract = {Unlike Monte Carlo (MC) methods, temporal difference (TD) methods learn the value function via bootstrapping, i.e., reusing historical value estimates to update the current value function. Therefore, TD learning methods can learn from incomplete episodes or continuing tasks in a step-by-step manner since it can update the value function based on its current estimate. As stated by Andrew Barto and Richard Sutton, if one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be the temporal difference. Typical TD learning algorithms include SARSA, Q-learning, and expected SARSA. SARSA is an on-policy TD algorithm whose action-value function is estimated by samples from the same policy, while Q-learning is an off-policy TD algorithm whose samples can come from arbitrary policies. Thus far, temporal difference learning has received extra attention in the interdisciplinary fields of neuroscience and psychology. A few physiological studies have found similarities to TD learning, for example, the firing rate of dopamine neurons in the brain appears to be proportional to a reward difference between the estimated reward and the actual reward. The larger the reward difference is, the more a new behavior is reinforced.},
	language = {en},
	urldate = {2025-03-30},
	booktitle = {Reinforcement {Learning} for {Sequential} {Decision} and {Optimal} {Control}},
	publisher = {Springer Nature},
	author = {Li, Shengbo Eben},
	editor = {Li, Shengbo Eben},
	year = {2023},
	doi = {10.1007/978-981-19-7784-8\_4},
	pages = {67--87},
}

@misc{chen_coordinated_2023,
	title = {Coordinated {Dynamic} {Bidding} in {Repeated} {Second}-{Price} {Auctions} with {Budgets}},
	url = {http://arxiv.org/abs/2306.07709},
	doi = {10.48550/arXiv.2306.07709},
	abstract = {In online ad markets, a rising number of advertisers are employing bidding agencies to participate in ad auctions. These agencies are specialized in designing online algorithms and bidding on behalf of their clients. Typically, an agency usually has information on multiple advertisers, so she can potentially coordinate bids to help her clients achieve higher utilities than those under independent bidding. In this paper, we study coordinated online bidding algorithms in repeated second-price auctions with budgets. We propose algorithms that guarantee every client a higher utility than the best she can get under independent bidding. We show that these algorithms achieve maximal coalition welfare and discuss bidders' incentives to misreport their budgets, in symmetric cases. Our proofs combine the techniques of online learning and equilibrium analysis, overcoming the difficulty of competing with a multi-dimensional benchmark. The performance of our algorithms is further evaluated by experiments on both synthetic and real data. To the best of our knowledge, we are the first to consider bidder coordination in online repeated auctions with constraints.},
	urldate = {2025-03-30},
	publisher = {arXiv},
	author = {Chen, Yurong and Wang, Qian and Duan, Zhijian and Sun, Haoran and Chen, Zhaohua and Yan, Xiang and Deng, Xiaotie},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07709 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Economics - Theoretical Economics},
}

@book{nisan_algorithmic_2007,
	address = {Cambridge},
	title = {Algorithmic {Game} {Theory}},
	isbn = {978-0-521-87282-9},
	url = {https://www.cambridge.org/core/books/algorithmic-game-theory/0092C07CA8B724E1B1BE2238DDD66B38},
	abstract = {In recent years game theory has had a substantial impact on computer science, especially on Internet- and e-commerce-related issues. Algorithmic Game Theory, first published in 2007, develops the central ideas and results of this exciting area in a clear and succinct manner. More than 40 of the top researchers in this field have written chapters that go from the foundations to the state of the art. Basic chapters on algorithmic methods for equilibria, mechanism design and combinatorial auctions are followed by chapters on important game theory applications such as incentives and pricing, cost sharing, information markets and cryptography and security. This definitive work will set the tone of research for the next few years and beyond. Students, researchers, and practitioners alike need to learn more about these fascinating theoretical developments and their widespread practical application.},
	urldate = {2025-03-29},
	publisher = {Cambridge University Press},
	editor = {Nisan, Noam and Roughgarden, Tim and Tardos, Eva and Vazirani, Vijay V.},
	year = {2007},
	doi = {10.1017/CBO9780511800481},
}

@article{gad_pygad_2024,
	title = {{PyGAD}: an intuitive genetic algorithm {Python} library},
	volume = {83},
	issn = {1573-7721},
	shorttitle = {{PyGAD}},
	url = {https://doi.org/10.1007/s11042-023-17167-y},
	doi = {10.1007/s11042-023-17167-y},
	abstract = {This paper introduces PyGAD, an open-source easy-to-use Python library for building the genetic algorithm (GA) and solving multi-objective optimization problems. PyGAD is designed as a general-purpose optimization library with the support of a wide range of parameters to give the user control over its life cycle. This includes, but not limited to, the population, fitness function, gene value space, gene data type, parent selection, crossover, and mutation. Its usage consists of 3 main steps: build the fitness function, create an instance of the pygad.GA class, and call the pygad.GA.run() method. The library supports training deep learning models created either with PyGAD itself or with frameworks such as Keras and PyTorch. Given its stable state, PyGAD is also in active development to respond to the user’s requested features and enhancements received on GitHub.},
	language = {en},
	number = {20},
	urldate = {2025-03-29},
	journal = {Multimedia Tools and Applications},
	author = {Gad, Ahmed Fawzy},
	month = jun,
	year = {2024},
	keywords = {Deep learning, Evolutionary algorithm, Genetic algorithm, Keras, NumPy, Optimization, PyTorch, Python},
	pages = {58029--58042},
}

@article{musiela_martingale_1998,
	title = {Martingale {Methods} in {Financial} {Modelling}},
	volume = {93},
	issn = {978-3-540-20966-9},
	doi = {10.1007/b137866},
	journal = {Journal of the American Statistical Association},
	author = {Musiela, Marek and Rutkowski, Marek},
	month = sep,
	year = {1998},
}

@article{chen_factor_2023,
	title = {A factor pricing model based on double moving average strategy},
	volume = {10},
	copyright = {2023 The Author(s)},
	issn = {2662-9992},
	url = {https://www.nature.com/articles/s41599-023-02362-x},
	doi = {10.1057/s41599-023-02362-x},
	abstract = {The heterogeneous market hypothesis states that there are different types of investors with different investment term-structures in the stock market. Based on the hypothesis, we analyze the trading strategies of moving averages with different term structures, and we find that the trading strategies formed by the 1-month short-term moving average with 3-, 6-, 9-, 12-, 18-, and 24-month long-term moving averages can gain significant excess returns in the Chinese market. Accordingly, this paper adopts the moving average factor formed by the method of Liu’s to expand the current four-factor pricing model of the Chinese market. The results of spanning regression show that the introduction of the short-term average with a period of 1-month and the long-term moving average with a period of 3- and 12-month can significantly improve the pricing power. The GRS test developed by Gibbons et al. (1989) also shows that the augmented six-factor model with moving average factors can pass the test at the significance level of 5\%, and the average absolute value of intercept terms decreases by about 50\% compared with the results of the four-factor model. At the same time, under different macro-state dependence, the overall performance of the augmented six-factor model with moving averages is still better than that of the four-factor pricing model proposed by Liu et al.’s. Our main contribution is to introduce double moving average factors to capture the behaviors of investors’ different term structures and add these factors to the most competitive asset pricing model for enhancing the pricing power. It is of great significance to supplement the pricing model in the Chinese market.},
	language = {en},
	number = {1},
	urldate = {2025-03-27},
	journal = {Humanities and Social Sciences Communications},
	author = {Chen, YuZhi and Fang, Yi and Li, XinYue and Wei, Jian},
	month = nov,
	year = {2023},
	note = {Publisher: Palgrave},
	keywords = {Business and management, Finance},
	pages = {1--13},
}

@misc{noauthor_auction_nodate,
	title = {Auction theory: {Auction} {Theory} in {Practice}: {Real} world {Examples} and {Case} {Studies}},
	shorttitle = {Auction theory},
	url = {https://fastercapital.com/content/Auction-theory--Auction-Theory-in-Practice--Real-world-Examples-and-Case-Studies.html},
	abstract = {At the heart of market economies lies the mechanism of auctions, a fundamental process for allocating resources and commodities based on competitive bidding. This method not only determines the value of an item through the collective assessment of its worth by interested parties but also ensures...},
	language = {en},
	urldate = {2025-03-27},
	journal = {FasterCapital},
}

@article{jank_automated_2011,
	title = {An {Automated} and {Data}-{Driven} {Bidding} {Strategy} for {Online} {Auctions}},
	volume = {23},
	issn = {1091-9856},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/ijoc.1100.0397},
	doi = {10.1287/ijoc.1100.0397},
	abstract = {The flexibility of time and location as well as the availability of an abundance of both old and new products makes online auctions an important part of people's daily shopping experience. Whereas many bidders rely on variants of the well-documented early or last-minute bidding strategies, neither strategy takes into account the aspect of auction competition: at any point in time, there are hundreds, even thousands, of the same or similar items up for sale, competing for the same bidder. In this paper, we propose a novel automated and data-driven bidding strategy. Our strategy consists of two main components. First, we develop a dynamic, forward-looking model for price in competing auctions. By incorporating dynamic features of the auction process and its competitive environment, our model is capable of accurately predicting an auction's price and outperforming model alternatives such as the generalized additive model, classification and regression trees, or Neural Networks. Then, using the idea of maximizing a bidder's surplus, we build a bidding framework around this model that selects the best auction to bid on and determines the best bid amount. The best auction is given by the one that yields the highest predicted surplus; the best bid amount is given by its predicted auction price. Our approach maximizes expected surplus and balances the probability of winning an auction with its average surplus. In simulations, we compare our automated strategy with early and last-minute bidding and find that our approach extracts 97\% and 15\% more expected surplus, respectively.},
	number = {2},
	urldate = {2025-03-27},
	journal = {INFORMS Journal on Computing},
	author = {Jank, Wolfgang and Zhang, Shu},
	month = may,
	year = {2011},
	note = {Publisher: INFORMS},
	keywords = {bidding, competition, dynamics, eBay, electronic commerce, forecasting, functional data, model selection, online auction, surplus},
	pages = {238--253},
}

@article{fefferman_existence_2006,
	title = {Existence and smoothness of the {Navier}-{Stokes} equation},
	abstract = {n X j=1 uj @ui @xj = ui @p @xi + fi(x,t) (x 2 Rn,t 0), (1) divu = n X i=1 @ui @xi = 0 (x 2 R n,t 0) (2)},
	journal = {The Millennium Prize Problems},
	author = {FEFFERMAN, CHARLES},
	month = jan,
	year = {2006},
}

@inproceedings{chan_updating_1982,
	address = {Heidelberg},
	title = {Updating {Formulae} and a {Pairwise} {Algorithm} for {Computing} {Sample} {Variances}},
	isbn = {978-3-642-51461-6},
	doi = {10.1007/978-3-642-51461-6_3},
	abstract = {A general formula is presented son. computing the sample valiance son a sample of size mtn given the means and valiance son two subsamples of sizes m and n. This formula is used in the construction of a pairwise algorithm son computing the valiance. Other applications are discussed as well including the use of updating formulae in a parallel computing environment. we present resume and rounding error analyze son several numerical schemes.},
	language = {en},
	booktitle = {{COMPSTAT} 1982 5th {Symposium} held at {Toulouse} 1982},
	publisher = {Physica-Verlag HD},
	author = {Chan, T. F. and Golub, G. H. and LeVeque, R. J.},
	editor = {Caussinus, H. and Ettinger, P. and Tomassone, R.},
	year = {1982},
	keywords = {Correct Digit, Double Precision, Error Analysis, Single Precision, Stanford Linear Accelerator},
	pages = {30--41},
}

@inproceedings{kjolstad_ghost_2010,
	address = {New York, NY, USA},
	series = {{ParaPLoP} '10},
	title = {Ghost {Cell} {Pattern}},
	isbn = {978-1-4503-0127-5},
	url = {https://doi.org/10.1145/1953611.1953615},
	doi = {10.1145/1953611.1953615},
	abstract = {Many problems consist of a structured grid of points that are updated repeatedly based on the values of a fixed set of neighboring points in the same grid. To parallelize these problems we can geometrically divide the grid into chunks that are processed by different processors. One challenge with this approach is that the update of points at the periphery of a chunk requires values from neighboring chunks. These are often located in remote memory belonging to different processes. The naive implementation results in a lot of time spent on communication leaving less time for useful computation. By using the Ghost Cell Pattern communication overhead can be reduced. This results in faster time to completion.},
	urldate = {2025-03-23},
	booktitle = {Proceedings of the 2010 {Workshop} on {Parallel} {Programming} {Patterns}},
	publisher = {Association for Computing Machinery},
	author = {Kjolstad, Fredrik Berg and Snir, Marc},
	month = mar,
	year = {2010},
	pages = {1--9},
}

@misc{noauthor_chorin_nodate,
	title = {Chorin method {\textbar} {MOOSE}},
	url = {https://mooseframework.inl.gov/modules/navier_stokes/inschorin.html},
	urldate = {2025-03-23},
}

@misc{barker_concentration_2022,
	title = {From concentration to quantitative regularity: a short survey of recent developments for the {Navier}-{Stokes} equations},
	shorttitle = {From concentration to quantitative regularity},
	url = {http://arxiv.org/abs/2211.16215},
	doi = {10.48550/arXiv.2211.16215},
	abstract = {In this short survey paper, we focus on some new developments in the study of the regularity or potential singularity formation for solutions of the 3D Navier-Stokes equations. Some of the motivating questions are: Are certain norms accumulating/concentrating on small scales near potential blow-up times? At what speed do certain scale-invariant norms blow-up? Can one prove explicit quantitative regularity estimates? Can one break the criticality barrier, even slightly? We emphasize that these questions are closely linked together. Many recent advances for the Navier-Stokes equations are directly inspired by results and methods from the field of nonlinear dispersive equations.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Barker, Tobias and Prange, Christophe},
	month = nov,
	year = {2022},
	note = {arXiv:2211.16215 [math]},
	keywords = {Mathematics - Analysis of PDEs},
}

@article{mattingly_recent_2003,
	title = {On recent progress for the stochastic {Navier} {Stokes} equations},
	issn = {2118-9366},
	url = {https://www.numdam.org/item/JEDP_2003____A11_0/?utm_source=chatgpt.com},
	doi = {10.5802/jedp.625},
	language = {en},
	urldate = {2025-03-23},
	journal = {Journées équations aux dérivées partielles},
	author = {Mattingly, Jonathan},
	year = {2003},
	pages = {1--52},
}

@inproceedings{hammami_overview_2017,
	title = {An {Overview} on {Loop} {Tiling} {Techniques} for {Code} {Generation}},
	url = {https://ieeexplore.ieee.org/abstract/document/8308298},
	doi = {10.1109/AICCSA.2017.168},
	abstract = {Loop tiling is a well-known compiler transformation for both sequential and parallel programs optimization. It focuses on the efficient execution of loop nests in order to generate high-performance codes running on modern architectures by increasing the amount of data reuse as well as data locality. Thus, by reordering loop iterations, accesses to the close data become within a relatively short timeframe. Our present work involves a thorough study on the major known tiling techniques. It permits first to uncover several tiling features, e.g. the shape which depends on the target program dependencies, the size where we distinguish between static and parametrized tiling, and the area which aims at tiling for parallelism and/or cache optimization. Then, we present the most recent code generators using the tiling technique in order to highlight the interest of loop tiling transformation for automatic parallelization in general and code generation in particular.},
	urldate = {2025-03-23},
	booktitle = {2017 {IEEE}/{ACS} 14th {International} {Conference} on {Computer} {Systems} and {Applications} ({AICCSA})},
	author = {Hammami, Emna and Slama, Yosr},
	month = oct,
	year = {2017},
	note = {ISSN: 2161-5330},
	keywords = {Diamond, Generators, Optimization, Parallel processing, Shape, Synchronization, Tools, code generation, code optimization, iteration space, loop, parallelization, shape, size, tiling},
	pages = {280--287},
}

@article{adhianto_performance_2007,
	series = {Performance {Modelling} and {Analysis} of {Communication} {Systems}},
	title = {Performance modeling of communication and computation in hybrid {MPI} and {OpenMP} applications},
	volume = {15},
	issn = {1569-190X},
	url = {https://www.sciencedirect.com/science/article/pii/S1569190X06001109},
	doi = {10.1016/j.simpat.2006.11.014},
	abstract = {Performance evaluation and modeling are crucial steps to enabling the optimization of parallel programs. Programs written using two programming models, such as MPI and OpenMP, require analysis to determine both performance efficiency and the most suitable numbers of processes and threads for their execution on a given platform. To study both of these problems, we propose the construction of a model that is based upon a small number of parameters, but is able to capture the complexity of the runtime system. We incorporate measurements of overheads introduced by each of the programming models, and thus need to model both the network and computational aspects of the system. We have combined two different techniques that includes static analysis, driven by the OpenUH compiler, to retrieve application signatures and a parallelization overhead measurement benchmark, realized by Sphinx and Perfsuite, to collect system profiles. Finally, we propose a performance evaluation measurement to identify communication and computation efficiency. In this paper, we describe our underlying framework, the performance model, and show how our tool can be applied to a sample code.},
	number = {4},
	urldate = {2025-03-23},
	journal = {Simulation Modelling Practice and Theory},
	author = {Adhianto, Laksono and Chapman, Barbara},
	month = apr,
	year = {2007},
	keywords = {Cluster, MPI, OpenMP, Performance modeling, SMP},
	pages = {481--491},
}

@misc{mark_bull_mpi,
	title = {{MPI} and {OpenMP}},
	url = {http://www.archer.ac.uk/training/course-material/2017/09/advmpi-camb/MPIandOpenMP.pdf},
	author = {Bull, Mark},
}

@article{bell_second-order_1989,
	title = {A second-order projection method for the incompressible navier-stokes equations},
	volume = {85},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/0021999189901514},
	doi = {10.1016/0021-9991(89)90151-4},
	abstract = {In this paper we describe a second-order projection method for the time-dependent, incompressible Navier-Stokes equations. As in the original projection method developed by Chorin, we first solve diffusion-convection equations to predict intermediate velocities which are then projected onto the space of divergence-free vector fields. By introducing more coupling between the diffusion-convection step and the projection step we obtain a temporal discretization that is second-order accurate. Our treatment of the diffusion-convection step uses a specialized higher order Godunov method for differencing the nonlinear convective terms that provides a robust treatment of these terms at high Reynolds number. The Godunov procedure is second-order accurate for smooth flow and remains stable for discontinuous initial data, even in the zero-viscosity limit. We approximate the projection directly using a Galerkin procedure that uses a local basis for discretely divergence-free vector fields. Numerical results are presented validating the convergence properties of the method. We also apply the method to doubly periodic shear-layers to assess the performance of the method on more difficult applications.},
	number = {2},
	urldate = {2025-03-23},
	journal = {Journal of Computational Physics},
	author = {Bell, John B and Colella, Phillip and Glaz, Harland M},
	month = dec,
	year = {1989},
	pages = {257--283},
}

@inproceedings{konstantinidis_accelerating_2012,
	address = {Berlin, Heidelberg},
	title = {Accelerating the {Red}/{Black} {SOR} {Method} {Using} {GPUs} with {CUDA}},
	isbn = {978-3-642-31464-3},
	doi = {10.1007/978-3-642-31464-3_60},
	abstract = {This work presents our strategy, applied optimizations and results in our effort to exploit the computational capabilities of GPUs under the CUDA environment in solving the Laplacian PDE. The parallelizable red/black SOR method was used. Additionally, a program for the CPU, featuring OpenMP, was developed as a performance reference. Significant performance improvements were achieved by using optimization methods which proved to have substantial speedup in performance. Eventually, a direct comparison of performance of both versions was realised. A 51x speedup was measured for the CUDA version over the CPU version, exceeding 134GB/sec bandwidth. Memory access patterns prove to be a critical factor in efficient program execution on GPUs and it is, therefore, appropriate to follow data reorganization in order to achieve the highest feasible memory throughput.},
	language = {en},
	booktitle = {Parallel {Processing} and {Applied} {Mathematics}},
	publisher = {Springer},
	author = {Konstantinidis, Elias and Cotronis, Yiannis},
	editor = {Wyrzykowski, Roman and Dongarra, Jack and Karczewski, Konrad and Waśniewski, Jerzy},
	year = {2012},
	keywords = {CUDA, GPU computing, PDEs, SOR, red/black},
	pages = {589--598},
}

@article{fefferman_existence_nodate,
	title = {{EXISTENCE} {AND} {SMOOTHNESS} {OF} {THE} {NAVIER}–{STOKES} {EQUATION}},
	language = {en},
	author = {Fefferman, Charles L},
}

@misc{noauthor_chorin_nodate-1,
	title = {Chorin method {\textbar} {MOOSE}},
	url = {https://mooseframework.inl.gov/modules/navier_stokes/inschorin.html},
	urldate = {2025-03-18},
}

@misc{noauthor_what_nodate,
	title = {What {Are} {Navier}-{Stokes} {Equations}? {\textbar} {SimWiki}},
	shorttitle = {What {Are} {Navier}-{Stokes} {Equations}?},
	url = {https://www.simscale.com/docs/simwiki/numerics-background/what-are-the-navier-stokes-equations/},
	abstract = {Navier-Stokes Equations represent conservation of mass, momentum, and energy to mathematically model fluid flow phenomena. Learn more.},
	language = {en-US},
	urldate = {2025-03-18},
	journal = {SimScale},
}

@misc{fearnley_cls_2017,
	title = {{CLS}: {New} {Problems} and {Completeness}},
	shorttitle = {{CLS}},
	url = {http://arxiv.org/abs/1702.06017},
	doi = {10.48550/arXiv.1702.06017},
	abstract = {The complexity class CLS was introduced by Daskalakis and Papadimitriou with the goal of capturing the complexity of some well-known problems in PPAD\${\textasciitilde}{\textbackslash}cap{\textasciitilde}\$PLS that have resisted, in some cases for decades, attempts to put them in polynomial time. No complete problem was known for CLS, and in previous work, the problems ContractionMap, i.e., the problem of finding an approximate fixpoint of a contraction map, and PLCP, i.e., the problem of solving a P-matrix Linear Complementarity Problem, were identified as prime candidates. First, we present a new CLS-complete problem MetaMetricContractionMap, which is closely related to the ContractionMap. Second, we introduce EndOfPotentialLine, which captures aspects of PPAD and PLS directly via a monotonic directed path, and show that EndOfPotentialLine is in CLS via a two-way reduction to EndOfMeteredLine. The latter was defined to keep track of how far a vertex is on the PPAD path via a restricted potential function. Third, we reduce PLCP to EndOfPotentialLine, thus making EndOfPotentialLine and EndOfMeteredLine at least as likely to be hard for CLS as PLCP. This last result leverages the monotonic structure of Lemke paths for PLCP problems, making EndOfPotentialLine a likely candidate to capture the exact complexity of PLCP; we note that the structure of Lemke-Howson paths for finding a Nash equilibrium in a two-player game very directly motivated the definition of the complexity class PPAD, which eventually ended up capturing this problem's complexity exactly.},
	urldate = {2025-03-08},
	publisher = {arXiv},
	author = {Fearnley, John and Gordon, Spencer and Mehta, Ruta and Savani, Rahul},
	month = apr,
	year = {2017},
	note = {arXiv:1702.06017 [cs]},
	keywords = {Computer Science - Computational Complexity},
}

@inproceedings{gonthier_four_2008,
	address = {Berlin, Heidelberg},
	title = {The {Four} {Colour} {Theorem}: {Engineering} of a {Formal} {Proof}},
	isbn = {978-3-540-87827-8},
	shorttitle = {The {Four} {Colour} {Theorem}},
	doi = {10.1007/978-3-540-87827-8_28},
	abstract = {The 150 year old Four Colour Theorem is the first famous result with a proof that requires large computer calculations. Such proofs are still controversial: It is thought that computer programs cannot be reviewed with mathematical rigor.},
	language = {en},
	booktitle = {Computer {Mathematics}},
	publisher = {Springer},
	author = {Gonthier, Georges},
	editor = {Kapur, Deepak},
	year = {2008},
	pages = {333--333},
}

@article{robertson_four-colour_1997,
	title = {The {Four}-{Colour} {Theorem}},
	volume = {70},
	issn = {0095-8956},
	url = {https://www.sciencedirect.com/science/article/pii/S0095895697917500},
	doi = {10.1006/jctb.1997.1750},
	abstract = {The four-colour theorem, that every loopless planar graph admits a vertex-colouring with at most four different colours, was proved in 1976 by Appel and Haken, using a computer. Here we give another proof, still using a computer, but simpler than Appel and Haken's in several respects.},
	number = {1},
	urldate = {2025-02-25},
	journal = {Journal of Combinatorial Theory, Series B},
	author = {Robertson, Neil and Sanders, Daniel and Seymour, Paul and Thomas, Robin},
	month = may,
	year = {1997},
	pages = {2--44},
}

@inproceedings{fearnley_complexity_2021,
	address = {New York, NY, USA},
	series = {{STOC} 2021},
	title = {The complexity of gradient descent: {CLS} = {PPAD} and {PLS}},
	isbn = {978-1-4503-8053-9},
	shorttitle = {The complexity of gradient descent},
	url = {https://doi.org/10.1145/3406325.3451052},
	doi = {10.1145/3406325.3451052},
	abstract = {We study search problems that can be solved by performing Gradient Descent on a bounded convex polytopal domain and show that this class is equal to the intersection of two well-known classes: PPAD and PLS. As our main underlying technical contribution, we show that computing a Karush-Kuhn-Tucker (KKT) point of a continuously differentiable function over the domain [0,1]2 is PPAD ∩ PLS-complete. This is the first natural problem to be shown complete for this class. Our results also imply that the class CLS (Continuous Local Search) - which was defined by Daskalakis and Papadimitriou as a more “natural” counterpart to PPAD ∩ PLS and contains many interesting problems - is itself equal to PPAD ∩ PLS.},
	urldate = {2025-02-24},
	booktitle = {Proceedings of the 53rd {Annual} {ACM} {SIGACT} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Fearnley, John and Goldberg, Paul W. and Hollender, Alexandros and Savani, Rahul},
	month = jun,
	year = {2021},
	pages = {46--59},
}

@misc{ikenmeyer_karchmer-wigderson_2022,
	title = {Karchmer-{Wigderson} {Games} for {Hazard}-free {Computation}},
	url = {http://arxiv.org/abs/2107.05128},
	doi = {10.48550/arXiv.2107.05128},
	abstract = {We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games. Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion. For the multiplexer function we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth \$2\$ that has optimal depth. We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound. We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Ikenmeyer, Christian and Komarath, Balagopal and Saurabh, Nitin},
	month = nov,
	year = {2022},
	note = {arXiv:2107.05128 [cs]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Discrete Mathematics},
}

@article{seiferas_techniques_1977,
	title = {Techniques for separating space complexity classes},
	volume = {14},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200007780041X},
	doi = {10.1016/S0022-0000(77)80041-X},
	abstract = {Diagonalization, cardinality, and recursive padding arguments are used to separate the Turing machine space complexity classes obtained by bounding space, number of worktape symbols, and number of worktape heads. Witness languages over a one-letter alphabet are constructed when possible.},
	number = {1},
	urldate = {2025-02-24},
	journal = {Journal of Computer and System Sciences},
	author = {Seiferas, Joel I.},
	month = feb,
	year = {1977},
	pages = {73--99},
}

@article{fenner_gap-definable_1994,
	title = {Gap-definable counting classes},
	volume = {48},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000005800248},
	doi = {10.1016/S0022-0000(05)80024-8},
	abstract = {The function class \#P lacks an important closure property: it is not closed under subtraction. To remedy this problem, we introduce the function class GapP as a natural alternative to \#P. GapP is the closure of \#P under subtraction and has all the other useful closure properties of \#P as well. We show that most previously studied counting classes, including PP, C=P, and ModkP, are “gap-definable,” i.e., definable using the values of GapP functions alone. We show that there is a smallest gap-definable class, SPP, which is still large enough to contain Few. We also show that SPP consists of exactly those languages low for GapP, and thus SPP languages are low for any gap-definable class. These results unify and improve earlier disparate results of J. Cai and L. Hemachandra (Math. Systems Theory 23, No. 2 (1990), 95–106) and J. Köbler et al. (J. Comput. System Sci. 44, No. 2 (1992), 272–286). We show further that any countable collection of languages is contained in a unique minimum gap-definable class, which implies that the gap-definable classes form a lattice under inclusion. Subtraction seems necessary for this result, since nothing similar is known for the \#P-definable classes.},
	number = {1},
	urldate = {2025-02-24},
	journal = {Journal of Computer and System Sciences},
	author = {Fenner, Stephen A. and Fortnow, Lance J. and Kurtz, Stuart A.},
	month = feb,
	year = {1994},
	pages = {116--148},
}

@inproceedings{toran_combinatorial_1989,
	address = {Berlin, Heidelberg},
	title = {A combinatorial technique for separating counting complexity classes},
	isbn = {978-3-540-46201-9},
	doi = {10.1007/BFb0035795},
	abstract = {We introduce a new combinatorial technique to obtain relativized separations of certain complexity classes related to the idea of counting, like PP, G (exact counting), and ⊕P (parity). To demonstrate its usefulness we present three relativizations separating NP from G, NP from ⊕P and ⊕P from PP. Other separations follow from these results, and as a consequence we obtain an oracle separating PP from PSPACE, thus solving an open problem proposed by Angluin in [An,80]. From the relativized separations we obtain absolute separations for counting complexity classes with log-time bounded computation time.},
	language = {en},
	booktitle = {Automata, {Languages} and {Programming}},
	publisher = {Springer},
	author = {Torán, Jacobo},
	editor = {Ausiello, Giorgio and Dezani-Ciancaglini, Mariangiola and Della Rocca, Simonetta Ronchi},
	year = {1989},
	pages = {733--744},
}

@inproceedings{toran_combinatorial_1989-1,
	address = {Berlin, Heidelberg},
	title = {A combinatorial technique for separating counting complexity classes},
	isbn = {978-3-540-46201-9},
	doi = {10.1007/BFb0035795},
	abstract = {We introduce a new combinatorial technique to obtain relativized separations of certain complexity classes related to the idea of counting, like PP, G (exact counting), and ⊕P (parity). To demonstrate its usefulness we present three relativizations separating NP from G, NP from ⊕P and ⊕P from PP. Other separations follow from these results, and as a consequence we obtain an oracle separating PP from PSPACE, thus solving an open problem proposed by Angluin in [An,80]. From the relativized separations we obtain absolute separations for counting complexity classes with log-time bounded computation time.},
	language = {en},
	booktitle = {Automata, {Languages} and {Programming}},
	publisher = {Springer},
	author = {Torán, Jacobo},
	editor = {Ausiello, Giorgio and Dezani-Ciancaglini, Mariangiola and Della Rocca, Simonetta Ronchi},
	year = {1989},
	pages = {733--744},
}

@misc{noauthor_shell_nodate,
	title = {Shell {Integration} - {Features}},
	url = {https://ghostty.org},
	abstract = {Some Ghostty features require integrating with your shell. Ghostty
can automatically inject shell integration for bash, zsh, fish, and
elvish.},
	language = {en},
	urldate = {2025-02-24},
	journal = {Ghostty},
}

@book{goldreich2008computational,
	title = {Computational complexity: a conceptual perspective},
	isbn = {978-1-139-47274-6},
	url = {https://books.google.co.uk/books?id=EuguvA-w5OEC},
	publisher = {Cambridge University Press},
	author = {Goldreich, O.},
	year = {2008},
}

@article{chan_computational_2024,
	title = {Computational complexity of counting coincidences},
	volume = {1015},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397524003931},
	doi = {10.1016/j.tcs.2024.114776},
	abstract = {Can you decide if there is a coincidence in the numbers counting two different combinatorial objects? For example, can you decide if two regions in R3 have the same number of domino tilings? There are two versions of the problem, with 2×1×1 and 2×2×1 boxes. We prove that in both cases the coincidence problem is not in the polynomial hierarchy unless the polynomial hierarchy collapses to a finite level. While the conclusions are the same, the proofs are notably different and generalize in different directions. We proceed to explore the coincidence problem for counting independent sets and matchings in graphs, matroid bases, order ideals and linear extensions in posets, permutation patterns, and the Kronecker coefficients. We also make a number of conjectures for counting other combinatorial objects such as plane triangulations, contingency tables, standard Young tableaux, reduced factorizations and the Littlewood–Richardson coefficients.},
	urldate = {2025-02-21},
	journal = {Theoretical Computer Science},
	author = {Chan, Swee Hong and Pak, Igor},
	month = nov,
	year = {2024},
	keywords = {\#P-completeness, Domino tiling, Graph matching, Kronecker coefficient, Linear extensions of posets, Matroid bases, Order ideals of posets, Permanent, Standard Young tableau},
	pages = {114776},
}

@misc{bandyopadhyay_counting_2005,
	title = {Counting without sampling. {New} algorithms for enumeration problems using statistical physics},
	url = {http://arxiv.org/abs/math/0510471},
	doi = {10.48550/arXiv.math/0510471},
	abstract = {We propose a new type of approximate counting algorithms for the problems of enumerating the number of independent sets and proper colorings in low degree graphs with large girth. Our algorithms are not based on a commonly used Markov chain technique, but rather are inspired by developments in statistical physics in connection with correlation decay properties of Gibbs measures and its implications to uniqueness of Gibbs measures on infinite trees, reconstruction problems and local weak convergence methods. On a negative side, our algorithms provide \${\textbackslash}epsilon\$-approximations only to the logarithms of the size of a feasible set (also known as free energy in statistical physics). But on the positive side, our approach provides deterministic as opposed to probabilistic guarantee on approximations. Moreover, for some regular graphs we obtain explicit values for the counting problem. For example, we show that every 4-regular \$n\$-node graph with large girth has approximately \$(1.494...){\textasciicircum}n\$ independent sets, and in every \$r\$-regular graph with \$n\$ nodes and large girth the number of \$q{\textbackslash}geq r+1\$-proper colorings is approximately \$[q(1-\{1{\textbackslash}over q\}){\textasciicircum}\{r{\textbackslash}over 2\}]{\textasciicircum}n\$, for large \$n\$. In statistical physics terminology, we compute explicitly the limit of the log-partition function. We extend our results to random regular graphs. Our explicit results would be hard to derive via the Markov chain method.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Bandyopadhyay, Antar and Gamarnik, David},
	month = oct,
	year = {2005},
	note = {arXiv:math/0510471},
	keywords = {Mathematics - Probability},
}

@inproceedings{dyer_relative_2000,
	address = {Berlin, Heidelberg},
	title = {On the {Relative} {Complexity} of {Approximate} {Counting} {Problems}},
	isbn = {978-3-540-44436-7},
	doi = {10.1007/3-540-44436-X_12},
	abstract = {Two natural classes of counting problems that are interreducible under approximation-preserving reductions are: (i) those that admit a particular kind of efficient approximation algorithm known as an “FPRAS,” and (ii) those that are complete for \#P with respect to approximation-preserving reducibility. We describe and investigate not only these two classes but also a third class, of intermediate complexity, that is not known to be identical to (i) or (ii). The third class can be characterised as the hardest problems in a logically defined subclass of \#P.},
	language = {en},
	booktitle = {Approximation {Algorithms} for {Combinatorial} {Optimization}},
	publisher = {Springer},
	author = {Dyer, Martin and Goldberg, Leslie Ann and Greenhill, Catherine and Jerrum, Mark},
	editor = {Jansen, Klaus and Khuller, Samir},
	year = {2000},
	keywords = {Conjunctive Normal Form, Counting Problem, Relation Symbol, Satisfying Assignment, Turing Machine},
	pages = {108--119},
}

@inproceedings{valiant_holographic_2004,
	title = {Holographic algorithms},
	url = {https://ieeexplore.ieee.org/document/1366250},
	doi = {10.1109/FOCS.2004.34},
	abstract = {We introduce a new notion of efficient reduction among computational problems. Classical reductions involve gadgets that map local solutions of one problem to local solutions of another in one-to-one, or possibly many-to-one or one-to-many, fashion. Our proposed reductions allow for gadgets with many-to-many correspondences. Their objective is to preserve the sum of the local solutions. Such reductions provide a method of translating a combinatorial problem to a family of finite systems of polynomial equations with integer coefficients such that the number of solutions of the combinatorial problem can be counted in polynomial time if some system in the family has a solution over the complex numbers. We can derive polynomial time algorithms in this way for ten problems for which only exponential time algorithms were known before. General questions about complexity classes are also formulated. If the method is applied to a \#P-complete problem then we obtain families of polynomial systems such that the solvability of any one member would imply P/sup \#P/ = NC2.},
	urldate = {2025-02-20},
	booktitle = {45th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Valiant, L.G.},
	month = oct,
	year = {2004},
	note = {ISSN: 0272-5428},
	keywords = {Contracts, Equations, Graph theory, Holography, Ice, Interference, National security, Physics, Polynomials, Research and development},
	pages = {306--315},
}

@article{toda_pp_1991,
	title = {{PP} is as {Hard} as the {Polynomial}-{Time} {Hierarchy}},
	volume = {20},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/10.1137/0220053},
	doi = {10.1137/0220053},
	abstract = {In this paper, it is shown that many natural counting classes, such as PP, \$C\_ =  P\$, and \$\{{\textbackslash}text\{MOD\}\}\_k \{{\textbackslash}text\{P\}\}\$, are at least as computationally hard as PH (the polynomial-time hierarchy) in the following sense: for each \$\{{\textbackslash}bf K\}\$ of the counting classes above, every set in \$\{{\textbackslash}bf K\}\$(PH) is polynomial-time randomized many-one reducible to a set in \$\{{\textbackslash}bf K\}\$ with two-sided exponentially small error probability. As a consequence of the result, it is seen that all the counting classes above are computationally harder than PH unless PH collapses to a finite level. Some other consequences are also shown.},
	number = {5},
	urldate = {2025-02-20},
	journal = {SIAM Journal on Computing},
	author = {Toda, Seinosuke},
	month = oct,
	year = {1991},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {865--877},
}

@article{merkle_hiding_1978,
	title = {Hiding information and signatures in trapdoor knapsacks},
	volume = {24},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/1055927},
	doi = {10.1109/TIT.1978.1055927},
	abstract = {The knapsack problem is an NP-complete combinatorial problem that is strongly believed to be computationally difficult to solve in general. Specific instances of this problem that appear very difficult to solve unless one possesses "trapdoor information" used in the design of the problem are demonstrated. Because only the designer can easily solve problems, others can send him information hidden in the solution to the problems without fear that an eavesdropper will be able to extract the information. This approach differs from usual cryptographic systems in that a secret key is not needed. Conversely, only the designer can generate signatures for messages, but anyone can easily check their authenticity.},
	number = {5},
	urldate = {2025-02-20},
	journal = {IEEE Transactions on Information Theory},
	author = {Merkle, R. and Hellman, M.},
	month = sep,
	year = {1978},
	note = {Conference Name: IEEE Transactions on Information Theory},
	pages = {525--530},
}

@article{berger_protein_1998,
	title = {Protein folding in the hydrophobic-hydrophilic ({HP}) model is {NP}-complete},
	volume = {5},
	issn = {1066-5277},
	doi = {10.1089/cmb.1998.5.27},
	abstract = {One of the simplest and most popular biophysical models of protein folding is the hydrophobic-hydrophilic (HP) model. The HP model abstracts the hydrophobic interaction in protein folding by labeling the amino acids as hydrophobic (H for nonpolar) or hydrophilic (P for polar). Chains of amino acids are configured as self-avoiding walks on the 3D cubic lattice, where an optimal conformation maximizes the number of adjacencies between H's. In this paper, the protein folding problem under the HP model on the cubic lattice is shown to be NP-complete. This means that the protein folding problem belongs to a large set of problems that are believed to be computationally intractable.},
	language = {eng},
	number = {1},
	journal = {Journal of Computational Biology: A Journal of Computational Molecular Cell Biology},
	author = {Berger, B. and Leighton, T.},
	year = {1998},
	pmid = {9541869},
	keywords = {Algorithms, Amino Acid Sequence, Amino Acids, Models, Chemical, Molecular Sequence Data, Protein Folding, Proteins},
	pages = {27--40},
}

@misc{pudlak_canonical_2019,
	title = {The canonical pairs of bounded depth {Frege} systems},
	url = {http://arxiv.org/abs/1912.03013},
	doi = {10.48550/arXiv.1912.03013},
	abstract = {The canonical pair of a proof system \$P\$ is the pair of disjoint NP sets where one set is the set of all satisfiable CNF formulas and the other is the set of CNF formulas that have \$P\$-proofs bounded by some polynomial. We give a combinatorial characterization of the canonical pairs of depth{\textasciitilde}\$d\$ Frege systems. Our characterization is based on certain games, introduced in this article, that are parametrized by a number{\textasciitilde}\$k\$, also called the depth. We show that the canonical pair of a depth{\textasciitilde}\$d\$ Frege system is polynomially equivalent to the pair \$(A\_\{d+2\},B\_\{d+2\})\$ where \$A\_\{d+2\}\$ (respectively, \$B\_\{d+1\}\$) are depth \{\$d+1\$\} games in which Player{\textasciitilde}I (Player II) has a positional winning strategy. Although this characterization is stated in terms of games, we will show that these combinatorial structures can be viewed as generalizations of monotone Boolean circuits. In particular, depth{\textasciitilde}1 games are essentially monotone Boolean circuits. Thus we get a generalization of the monotone feasible interpolation for Resolution, which is a property that enables one to reduce the task of proving lower bounds on the size of refutations to lower bounds on the size of monotone Boolean circuits. However, we do not have a method yet for proving lower bounds on the size of depth{\textasciitilde}\$d\$ games for \$d{\textgreater}1\$.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Pudlak, Pavel},
	month = dec,
	year = {2019},
	note = {arXiv:1912.03013 [math]},
	keywords = {Computer Science - Computational Complexity, Mathematics - Logic},
}

@inproceedings{carboni_oliveira_hardness_2018,
	title = {Hardness {Magnification} for {Natural} {Problems}},
	url = {https://ieeexplore.ieee.org/abstract/document/8555094},
	doi = {10.1109/FOCS.2018.00016},
	abstract = {We show that for several natural problems of interest, complexity lower bounds that are barely non-trivial imply super-polynomial or even exponential lower bounds in strong computational models. We term this phenomenon "hardness magnification". Our examples of hardness magnification include: 1. Let MCSP be the decision problem whose YES instances are truth tables of functions with circuit complexity at most s(n). We show that if MCSP[2{\textasciicircum}√n] cannot be solved on average with zero error by formulas of linear (or even sub-linear) size, then NP does not have polynomial-size formulas. In contrast, Hirahara and Santhanam (2017) recently showed that MCSP[2{\textasciicircum}√n] cannot be solved in the worst case by formulas of nearly quadratic size. 2. If there is a c {\textgreater} 0 such that for each positive integer d there is an ε {\textgreater} 0 such that the problem of checking if an n-vertex graph in the adjacency matrix representation has a vertex cover of size (log n){\textasciicircum}c cannot be solved by depth-d AC{\textasciicircum}0 circuits of size m{\textasciicircum}1+ε, where m = Θ(n{\textasciicircum}2), then NP does not have polynomial-size formulas. 3. Let (α, β)-MCSP[s] be the promise problem whose YES instances are truth tables of functions that are α-approximable by a circuit of size s(n), and whose NO instances are truth tables of functions that are not β-approximable by a circuit of size s(n). We show that for arbitrary 1/2 {\textless} β {\textless} α ≤ 1, if (α, β)-MCSP[2{\textasciicircum}√n] cannot be solved by randomized algorithms with random access to the input running in sublinear time, then NP is not contained in BPP. 4. If for each probabilistic quasi-linear time machine M using poly-logarithmic many random bits that is claimed to solve Satisfiability, there is a deterministic polynomial-time machine that on infinitely many input lengths n either identifies a satisfiable instance of bit-length n on which M does not accept with high probability or an unsatisfiable instance of bit-length n on which M does not reject with high probability, then NEXP is not contained in BPP. 5. Given functions s, c N → N where s {\textgreater} c, let MKtP[c, s] be the promise problem whose YES instances are strings of Kt complexity at most c(N) and NO instances are strings of Kt complexity greater than s(N). We show that if there is a δ {\textgreater} 0 such that for each ε {\textgreater} 0, MKtP[N{\textasciicircum}ε, N{\textasciicircum}ε + 5 log(N)] requires Boolean circuits of size N{\textasciicircum}1+δ, then EXP is not contained in SIZE (poly). For each of the cases of magnification above, we observe that standard hardness assumptions imply much stronger lower bounds for these problems than we require for magnification. We further explore magnification as an avenue to proving strong lower bounds, and argue that magnification circumvents the "natural proofs" barrier of Razborov and Rudich (1997). Examining some standard proof techniques, we find that they fall just short of proving lower bounds via magnification. As one of our main open problems, we ask whether there are other meta-mathematical barriers to proving lower bounds that rule out approaches combining magnification with known techniques.},
	urldate = {2025-02-19},
	booktitle = {2018 {IEEE} 59th {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS})},
	author = {Carboni Oliveira, Igor and Santhanam, Rahul},
	month = oct,
	year = {2018},
	note = {ISSN: 2575-8454},
	keywords = {Complexity theory, Computational modeling, Computer science, Cryptography, Integrated circuit modeling, Probabilistic logic, Standards, circuit complexity, computational complexity, hardness magnification, lower bounds, minimum circuit size problem, satisfiability, time bounded Kolmogorov complexity, vertex cover},
	pages = {65--76},
}

@misc{pich_localizability_2022,
	title = {Localizability of the approximation method},
	url = {http://arxiv.org/abs/2212.09285},
	doi = {10.48550/arXiv.2212.09285},
	abstract = {We use the approximation method of Razborov to analyze the locality barrier which arose from the investigation of the hardness magnification approach to complexity lower bounds. Adapting a limitation of the approximation method obtained by Razborov, we show that in many cases it is not possible to combine the approximation method with typical (localizable) hardness magnification theorems to derive strong circuit lower bounds. In particular, one cannot use the approximation method to derive an extremely strong constant-depth circuit lower bound and then magnify it to an \$NC{\textasciicircum}1\$ lower bound for an explicit function. To prove this we show that lower bounds obtained by the approximation method are in many cases localizable in the sense that they imply lower bounds for circuits which are allowed to use arbitrarily powerful oracles with small fan-in.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Pich, Jan},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09285 [cs]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Discrete Mathematics},
}

@book{arteche_proof_2024,
	title = {From proof complexity to circuit complexity via interactive protocols},
	volume = {297},
	isbn = {978-3-95977-322-5},
	url = {https://ora.ox.ac.uk/objects/uuid:d471a6fc-bed4-4f85-bf12-96abf752768c},
	abstract = {{\textless}p{\textgreater}Folklore in complexity theory suspects that circuit lower bounds against {\textless}strong{\textgreater}NC{\textless}/strong{\textgreater}$^{\textrm{1}}$ or {\textless}strong{\textgreater}P{\textless}/strong{\textgreater}/poly, currently out of reach, are a necessary step towards proving strong proof complexity lower bounds for systems like Frege or Extended Frege. Establishing such a connection formally, however, is already daunting, as it would imply the breakthrough separation {\textless}strong{\textgreater}NEXP{\textless}/strong{\textgreater} ⊈ {\textless}strong{\textgreater}P{\textless}/strong{\textgreater}/poly, as recently observed by Pich and Santhanam [Pich and Santhanam, 2023].{\textless}/p{\textgreater} {\textless}br{\textgreater} {\textless}p{\textgreater}We show such a connection conditionally for the Implicit Extended Frege proof system (iEF) introduced by Krajíček [Krajíček, 2004], capable of formalizing most of contemporary complexity theory. In particular, we show that if iEF proves efficiently the standard derandomization assumption that a concrete Boolean function is hard on average for subexponential-size circuits, then any superpolynomial lower bound on the length of iEF proofs implies \#{\textless}strong{\textgreater}P{\textless}/strong{\textgreater} ⊈ {\textless}strong{\textgreater}FP{\textless}/strong{\textgreater}/poly (which would in turn imply, for example, {\textless}strong{\textgreater}PSPACE{\textless}/strong{\textgreater} ⊈ {\textless}strong{\textgreater}P{\textless}/strong{\textgreater}/poly). Our proof exploits the formalization inside iEF of the soundness of the sum-check protocol of Lund, Fortnow, Karloff, and Nisan [Lund et al., 1992]. This has consequences for the self-provability of circuit upper bounds in iEF. Interestingly, further improving our result seems to require progress in constructing interactive proof systems with more efficient provers.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-19},
	publisher = {Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
	author = {Arteche, N. and Khaniki, E. and Pich, J. and Santhanam, R.},
	year = {2024},
	note = {ISSN: 1868-8969},
}

@book{daskalakis_complexity_2006,
	title = {The complexity of computing a {Nash} equilibrium},
	volume = {39},
	abstract = {How long does it take until economic agents converge to an equilibrium? By studying the complexity of the problem of computing a mixed Nash equilibrium in a game, we provide evidence that there are games in which convergence to such an equilibrium takes prohibitively long. Traditionally, computational problems fall into two classes: those that have a polynomial-time algorithm and those that are NP-hard. However, the concept of NP-hardness cannot be applied to the rare problems where "every instance has a solution"---for example, in the case of games Nash's theorem asserts that every game has a mixed equilibrium (now known as the Nash equilibrium, in honor of that result). We show that finding a Nash equilibrium is complete for a class of problems called PPAD, containing several other known hard problems; all problems in PPAD share the same style of proof that every instance has a solution.},
	publisher = {Association for Computing Machinery},
	author = {Daskalakis, Constantinos and Goldberg, Paul and Papadimitriou, Christos},
	month = jan,
	year = {2006},
	doi = {10.1145/1461928.1461951},
	note = {Journal Abbreviation: SIAM Journal on Computing
Pages: 78
Publication Title: SIAM Journal on Computing},
}

@incollection{mulmuley_geometric_2003,
	title = {Geometric {Complexity} {Theory}, {P} vs. {NP} and {Explicit} {Obstructions}},
	isbn = {978-81-85931-36-4},
	abstract = {Theory of computing has given rise to some fundamental mathematical problems, notably the P ≠ NP conjecture, and the related lower bound problems concerning formula or circuit size. We develop an approach to these problems through geometric invariant theory. The goal of this approach is to reduce the hard nonexistence problems under consideration to tractable existence problems. Accordingly, we reduce the arithmetic (characteristic 0) version of the P ≠ NP conjecture, and other related lower bound problems to proving existence of obstructions. These are representations in the homogeneous coordinate rings of orbit-closures in geometric invariant theory [MFK], of a class of points which are partially stable and whose stabilizers have special representation-theoretic properties. However, the Luna-Vust complexity [LV] of these orbit closures is quite high, in contrast with the well-understood homogeneous or almost-homogeneous-spaces, such as G/P [LLM], toric varieties [F3], and spherical embed-dings [BLV], whose Luna-Vust complexity is zero. We take a step towards explicit construction of obstructions by proving two results regarding these orbit closures. The first is a generalization of the Borel-Weil theorem for G/P to these orbit-closures. Second, we conjecture a nice representation-theoretic set of generators for their ideals, and prove a weaker version of the conjecture. Such a set of generators had earlier been given for the ideal of G/P by Lakshmibai, Seshadri, Littelmann [LS, Li3, LLM] and Kostant (cf. [PK]). Finally, using these results, we reduce, in essence, the arithmetic non-existence problems under consideration to fundmental existence and construction problems in representation theory and algebraic geometry that are conjectured to be in the complexity class P.},
	booktitle = {Advances in {Algebra} and {Geometry}: {University} of {Hyderabad} {Conference} 2001},
	author = {Mulmuley, Ketan and Sohoni, Milind},
	month = jan,
	year = {2003},
	doi = {10.1007/978-93-86279-12-5_20},
	pages = {239--261},
}

@article{ikenmeyer_complexity_2019,
	title = {On the complexity of hazard-free circuits},
	volume = {66},
	issn = {0004-5411, 1557-735X},
	url = {http://arxiv.org/abs/1711.01904},
	doi = {10.1145/3320123},
	abstract = {The problem of constructing hazard-free Boolean circuits dates back to the 1940s and is an important problem in circuit design. Our main lower-bound result unconditionally shows the existence of functions whose circuit complexity is polynomially bounded while every hazard-free implementation is provably of exponential size. Previous lower bounds on the hazard-free complexity were only valid for depth 2 circuits. The same proof method yields that every subcubic implementation of Boolean matrix multiplication must have hazards. These results follow from a crucial structural insight: Hazard-free complexity is a natural generalization of monotone complexity to all (not necessarily monotone) Boolean functions. Thus, we can apply known monotone complexity lower bounds to find lower bounds on the hazard-free complexity. We also lift these methods from the monotone setting to prove exponential hazard-free complexity lower bounds for non-monotone functions. As our main upper-bound result we show how to efficiently convert a Boolean circuit into a bounded-bit hazard-free circuit with only a polynomially large blow-up in the number of gates. Previously, the best known method yielded exponentially large circuits in the worst case, so our algorithm gives an exponential improvement. As a side result we establish the NP-completeness of several hazard detection problems.},
	number = {4},
	urldate = {2025-02-19},
	journal = {Journal of the ACM},
	author = {Ikenmeyer, Christian and Komarath, Balagopal and Lenzen, Christoph and Lysikov, Vladimir and Mokhov, Andrey and Sreenivasaiah, Karteek},
	month = aug,
	year = {2019},
	note = {arXiv:1711.01904 [cs]},
	keywords = {Computer Science - Computational Complexity},
	pages = {1--20},
}

@article{chen_settling_2009,
	title = {Settling the complexity of computing two-player {Nash} equilibria},
	volume = {56},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/1516512.1516516},
	doi = {10.1145/1516512.1516516},
	abstract = {We prove that Bimatrix, the problem of finding a Nash equilibrium in a two-player game, is complete for the complexity class PPAD (Polynomial Parity Argument, Directed version) introduced by Papadimitriou in 1991.Our result, building upon the work of Daskalakis et al. [2006a] on the complexity of four-player Nash equilibria, settles a long standing open problem in algorithmic game theory. It also serves as a starting point for a series of results concerning the complexity of two-player Nash equilibria. In particular, we prove the following theorems:—Bimatrix does not have a fully polynomial-time approximation scheme unless every problem in PPAD is solvable in polynomial time.—The smoothed complexity of the classic Lemke-Howson algorithm and, in fact, of any algorithm for Bimatrix is not polynomial unless every problem in PPAD is solvable in randomized polynomial time.Our results also have a complexity implication in mathematical economics:—Arrow-Debreu market equilibria are PPAD-hard to compute.},
	number = {3},
	urldate = {2025-02-19},
	journal = {J. ACM},
	author = {Chen, Xi and Deng, Xiaotie and Teng, Shang-Hua},
	month = may,
	year = {2009},
	pages = {14:1--14:57},
}

@misc{aaronson_why_2011,
	title = {Why {Philosophers} {Should} {Care} {About} {Computational} {Complexity}},
	url = {http://arxiv.org/abs/1108.1791},
	doi = {10.48550/arXiv.1108.1791},
	abstract = {One might think that, once we know something is computable, how efficiently it can be computed is a practical question with little further philosophical importance. In this essay, I offer a detailed case that one would be wrong. In particular, I argue that computational complexity theory -- the field that studies the resources (such as time, space, and randomness) needed to solve computational problems -- leads to new perspectives on the nature of mathematical knowledge, the strong AI debate, computationalism, the problem of logical omniscience, Hume's problem of induction, Goodman's grue riddle, the foundations of quantum mechanics, economic rationality, closed timelike curves, and several other topics of philosophical interest. I end by discussing aspects of complexity theory itself that could benefit from philosophical analysis.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Aaronson, Scott},
	month = aug,
	year = {2011},
	note = {arXiv:1108.1791 [cs]},
	keywords = {Computer Science - Computational Complexity, Quantum Physics},
}

@misc{goldberg_survey_2011,
	title = {A {Survey} of {PPAD}-{Completeness} for {Computing} {Nash} {Equilibria}},
	url = {http://arxiv.org/abs/1103.2709},
	doi = {10.48550/arXiv.1103.2709},
	abstract = {PPAD refers to a class of computational problems for which solutions are guaranteed to exist due to a specific combinatorial principle. The most well-known such problem is that of computing a Nash equilibrium of a game. Other examples include the search for market equilibria, and envy-free allocations in the context of cake-cutting. A problem is said to be complete for PPAD if it belongs to PPAD and can be shown to constitute one of the hardest computational challenges within that class. In this paper, I give a relatively informal overview of the proofs used in the PPAD-completeness results. The focus is on the mixed Nash equilibria guaranteed to exist by Nash's theorem. I also give an overview of some recent work that uses these ideas to show PSPACE-completeness for the computation of specific equilibria found by homotopy methods. I give a brief introduction to related problems of searching for market equilibria.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Goldberg, Paul W.},
	month = mar,
	year = {2011},
	note = {arXiv:1103.2709 [cs]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Computer Science and Game Theory},
}

@article{mulmuley_p_2011,
	title = {On {P} vs. {NP} and geometric complexity theory: {Dedicated} to {Sri} {Ramakrishna}},
	volume = {58},
	issn = {0004-5411},
	shorttitle = {On {P} vs. {NP} and geometric complexity theory},
	url = {https://dl.acm.org/doi/10.1145/1944345.1944346},
	doi = {10.1145/1944345.1944346},
	abstract = {This article gives an overview of the geometric complexity theory (GCT) approach towards the P vs. NP and related problems focusing on its main complexity theoretic results. These are: (1) two concrete lower bounds, which are currently the best known lower bounds in the context of the P vs. NC and permanent vs. determinant problems, (2) the Flip Theorem, which formalizes the self-referential paradox in the P vs. NP problem, and (3) the Decomposition Theorem, which decomposes the arithmetic P vs. NP and permanent vs. determinant problems into subproblems without self-referential difficulty, consisting of positivity hypotheses in algebraic geometry and representation theory and easier hardness hypotheses.},
	number = {2},
	urldate = {2025-02-19},
	journal = {J. ACM},
	author = {Mulmuley, Ketan D.},
	month = apr,
	year = {2011},
	pages = {5:1--5:26},
}

@inproceedings{aaronson_algebrization_2008,
	address = {New York, NY, USA},
	series = {{STOC} '08},
	title = {Algebrization: a new barrier in complexity theory},
	isbn = {978-1-60558-047-0},
	shorttitle = {Algebrization},
	url = {https://doi.org/10.1145/1374376.1374481},
	doi = {10.1145/1374376.1374481},
	abstract = {Any proof of P!=NP will have to overcome two barriers: relativization and natural proofs. Yet over the last decade, we have seen circuit lower bounds (for example, that PP does not have linear-size circuits) that overcome both barriers simultaneously. So the question arises of whether there is a third barrier to progress on the central questions in complexity theory.In this paper we present such a barrier, which we call algebraic relativization or algebrization. The idea is that, when we relativize some complexity class inclusion, we should give the simulating machine access not only to an oracle A, but also to a low-degree extension of A over a finite field or ring.We systematically go through basic results and open problems in complexity theory to delineate the power of the new algebrization barrier. First, we show that all known non-relativizing results based on arithmetization -- both inclusions such as IP=PSPACE and MIP=NEXP, and separations such as MAEXP not in P/poly -- do indeed algebrize. Second, we show that almost all of the major open problems -- including P versus NP, P versus RP, and NEXP versus P/poly -- will require non-algebrizing techniques. In some cases algebrization seems to explain exactly why progress stopped where it did: for example, why we have superlinear circuit lower bounds for PromiseMA but not for NP.Our second set of results follows from lower bounds in a new model of algebraic query complexity, which we introduce in this paper and which is interesting in its own right. Some of our lower bounds use direct combinatorial and algebraic arguments, while others stem from a surprising connection between our model and communication complexity. Using this connection, we are also able to give an MA-protocol for the Inner Product function with O(sqrt(n) log n) communication (essentially matching a lower bound of Klauck).},
	urldate = {2025-02-18},
	booktitle = {Proceedings of the fortieth annual {ACM} symposium on {Theory} of computing},
	publisher = {Association for Computing Machinery},
	author = {Aaronson, Scott and Wigderson, Avi},
	month = may,
	year = {2008},
	pages = {731--740},
}

@article{baker_relativizations_1975,
	title = {Relativizations of the \${\textbackslash}mathcal\{{P}\} = ?{\textbackslash}mathcal\{{NP}\}\$ {Question}},
	volume = {4},
	issn = {0097-5397},
	shorttitle = {Relativizations of the \${\textbackslash}mathcal\{{P}\} = ?},
	url = {https://epubs.siam.org/doi/10.1137/0204037},
	doi = {10.1137/0204037},
	abstract = {Let A be a language chosen randomly by tossing a fair coin for each string x to determine whether x belongs to A. With probability 1, each of the relativized classes LOGSPACEA, PA, NPA, PPA, and PSPACEA is properly contained in the next. Also, NPA≠co-NPA with probability 1. By contrast, with probability 1 the class PA coincides with the class BPPA of languages recognized by probabilistic oracle machines with error probability uniformly bounded below 12. NPA is shown, with probability 1, to contain a PA-immune set, i.e., a set having no infinite subset in PA. The relationship of PA-immunity to p-sparseness and NPA-completeness is briefly discussed: PA-immune sets in NPA can be sparse or moderately dense, but not co-sparse. Relativization with respect to a random length-preserving permutation π, instead of a random oracle A, yields analogous results and in addition the proper containment, with probability 1, of Pπ in NPπ∩co-NPπ, which we have been unable to decide for a simple random oracle. Most of these results are shown by straightforward counting arguments, applied to oracle-dependent languages designed not to be recognizable without a large number of oracle calls. It is conjectured that all pA-invariant statements that are true with probability 1 of subrecursive language classes uniformly relativized to a random oracle are also true in the unrelativized case.},
	number = {4},
	urldate = {2025-02-18},
	journal = {SIAM Journal on Computing},
	author = {Baker, Theodore and Gill, John and Solovay, Robert},
	month = dec,
	year = {1975},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {431--442},
}

@article{razborov_natural_1997,
	title = {Natural {Proofs}},
	volume = {55},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791494X},
	doi = {10.1006/jcss.1997.1494},
	abstract = {We introduce the notion ofnaturalproof. We argue that the known proofs of lower bounds on the complexity of explicit Boolean functions in nonmonotone models fall within our definition of natural. We show, based on a hardness assumption, that natural proofs can not prove superpolynomial lower bounds for general circuits. Without the hardness assumption, we are able to show that they can not prove exponential lower bounds (for general circuits) for the discrete logarithm problem. We show that the weaker class ofAC0-natural proofs which is sufficient to prove the parity lower bounds of Furst, Saxe, and Sipser, Yao, and Håstad is inherently incapable of proving the bounds of Razborov and Smolensky. We give some formal evidence that natural proofs are indeed natural by showing that every formal complexity measure, which can prove superpolynomial lower bounds for a single function, can do so for almost all functions, which is one of the two requirements of a natural proof in our sense.},
	number = {1},
	urldate = {2025-02-18},
	journal = {Journal of Computer and System Sciences},
	author = {Razborov, Alexander A and Rudich, Steven},
	month = aug,
	year = {1997},
	pages = {24--35},
}

@inproceedings{cook_complexity_1971,
	address = {New York, NY, USA},
	series = {{STOC} '71},
	title = {The complexity of theorem-proving procedures},
	isbn = {978-1-4503-7464-4},
	url = {https://dl.acm.org/doi/10.1145/800157.805047},
	doi = {10.1145/800157.805047},
	abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
	urldate = {2025-02-18},
	booktitle = {Proceedings of the third annual {ACM} symposium on {Theory} of computing},
	publisher = {Association for Computing Machinery},
	author = {Cook, Stephen A.},
	month = may,
	year = {1971},
	pages = {151--158},
}

@article{fortnow_status_2009,
	title = {The status of the {P} versus {NP} problem},
	volume = {52},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/1562164.1562186},
	doi = {10.1145/1562164.1562186},
	abstract = {It's one of the fundamental mathematical problems of our time, and its importance grows with the rise of powerful computers.},
	number = {9},
	urldate = {2025-02-18},
	journal = {Commun. ACM},
	author = {Fortnow, Lance},
	month = sep,
	year = {2009},
	pages = {78--86},
}

@misc{mulmuley_geometric_2009,
	title = {Geometric {Complexity} {Theory} {VI}: the flip via saturated and positive integer programming in representation theory and algebraic geometry},
	shorttitle = {Geometric {Complexity} {Theory} {VI}},
	url = {http://arxiv.org/abs/0704.0229},
	doi = {10.48550/arXiv.0704.0229},
	abstract = {This article belongs to a series on geometric complexity theory (GCT), an approach to the P vs. NP and related problems through algebraic geometry and representation theory. The basic principle behind this approach is called the flip. In essence, it reduces the negative hypothesis in complexity theory (the lower bound problems), such as the P vs. NP problem in characteristic zero, to the positive hypothesis in complexity theory (the upper bound problems): specifically, to showing that the problems of deciding nonvanishing of the fundamental structural constants in representation theory and algebraic geometry, such as the well known plethysm constants--or rather certain relaxed forms of these decision probelms--belong to the complexity class P. In this article, we suggest a plan for implementing the flip, i.e., for showing that these relaxed decision problems belong to P. This is based on the reduction of the preceding complexity-theoretic positive hypotheses to mathematical positivity hypotheses: specifically, to showing that there exist positive formulae--i.e. formulae with nonnegative coefficients--for the structural constants under consideration and certain functions associated with them. These turn out be intimately related to the similar positivity properties of the Kazhdan-Lusztig polynomials and the multiplicative structural constants of the canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum groups. The known proofs of these positivity properties depend on the Riemann hypothesis over finite fields and the related results. Thus the reduction here, in conjunction with the flip, in essence, says that the validity of the P vs. NP conjecture in characteristic zero is intimately linked to the Riemann hypothesis over finite fields and related problems.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Mulmuley, Ketan D.},
	month = jan,
	year = {2009},
	note = {arXiv:0704.0229 [cs]},
	keywords = {Computer Science - Computational Complexity},
}

@article{valiant_complexity_1979,
	title = {The complexity of computing the permanent},
	volume = {8},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/0304397579900446},
	doi = {10.1016/0304-3975(79)90044-6},
	abstract = {It is shown that the permanent function of (0, 1)-matrices is a complete problem for the class of counting problems associated with nondeterministic polynomial time computations. Related counting problems are also considered. The reductions used are characterized by their nontrivial use of arithmetic.},
	number = {2},
	urldate = {2025-02-18},
	journal = {Theoretical Computer Science},
	author = {Valiant, L. G.},
	month = jan,
	year = {1979},
	pages = {189--201},
}

@misc{noauthor_gladstonego_nodate,
	title = {{GladstoneGo} {Login}},
	url = {https://sportwarwick.gladstonego.cloud/identity/Account/ResetPassword?linkid=71df95e5-16e9-419b-b338-3&},
	urldate = {2025-02-18},
}

@article{papadimitriou_complexity_1994,
	title = {On the complexity of the parity argument and other inefficient proofs of existence},
	volume = {48},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000005800637},
	doi = {10.1016/S0022-0000(05)80063-7},
	abstract = {We define several new complexity classes of search problems, “between” the classes FP and FNP. These new classes are contained, along with factoring, and the class PLS, in the class TFNP of search problems in FNP that always have a witness. A problem in each of these new classes is defined in terms of an implicitly given, exponentially large graph. The existence of the solution sought is established via a simple graph-theoretic argument with an inefficiently constructive proof; for example, PLS can be thought of as corresponding to the lemma “every dag has a sink.” The new classes, are based on lemmata such as “every graph has an even number of odd-degree nodes.” They contain several important problems for which no polynomial time algorithm is presently known, including the computational versions of Sperner's lemma, Brouwer's fixpoint theorem, Chévalley's theorem, and the Borsuk-Ulam theorem, the linear complementarity problem for P-matrices, finding a mixed equilibrium in a non-zero sum game, finding a second Hamilton circuit in a Hamiltonian cubic graph, a second Hamiltonian decomposition in a quartic graph, and others. Some of these problems are shown to be complete.},
	number = {3},
	urldate = {2025-02-16},
	journal = {Journal of Computer and System Sciences},
	author = {Papadimitriou, Christos H.},
	month = jun,
	year = {1994},
	pages = {498--532},
}

@book{arora_computational_2009,
	address = {Cambridge},
	title = {Computational {Complexity}: {A} {Modern} {Approach}},
	isbn = {978-0-521-42426-4},
	shorttitle = {Computational {Complexity}},
	url = {https://www.cambridge.org/core/books/computational-complexity/3453CAFDEB0B4820B186FE69A64E1086},
	abstract = {This beginning graduate textbook describes both recent achievements and classical results of computational complexity theory. Requiring essentially no background apart from mathematical maturity, the book can be used as a reference for self-study for anyone interested in complexity, including physicists, mathematicians, and other scientists, as well as a textbook for a variety of courses and seminars. More than 300 exercises are included with a selected hint set. The book starts with a broad introduction to the field and progresses to advanced results. Contents include: definition of Turing machines and basic time and space complexity classes, probabilistic algorithms, interactive proofs, cryptography, quantum computation, lower bounds for concrete computational models (decision trees, communication complexity, constant depth, algebraic and monotone circuits, proof complexity), average-case complexity and hardness amplification, derandomization and pseudorandom constructions, and the PCP theorem.},
	urldate = {2025-02-16},
	publisher = {Cambridge University Press},
	author = {Arora, Sanjeev and Barak, Boaz},
	year = {2009},
	doi = {10.1017/CBO9780511804090},
}

@inproceedings{deligkas_pure-circuit_2022,
	title = {Pure-{Circuit}: {Strong} {Inapproximability} for {PPAD}},
	shorttitle = {Pure-{Circuit}},
	url = {https://ieeexplore.ieee.org/abstract/document/9996749},
	doi = {10.1109/FOCS54457.2022.00022},
	abstract = {The current state-of-the-art methods for showing inapproximability in PPAD arise from the ε-Generalized-Circuit (ε-GCIRCUIT) problem. Rubinstein (2018) showed that there exists a small unknown constant ε for which ε-GCIRCUIT is PPAD-hard, and subsequent work has shown hardness results for other problems in PPAD by using ε-GCIRCUIT as an intermediate problem.We introduce PURE-CIRCUIT, a new intermediate problem for PPAD, which can be thought of as ε-GCIRCUIT pushed to the limit as {\textbackslash}varepsilon{\textbackslash}rightarrow 1, and we show that the problem is PPAD-complete. We then prove that ε-GCIRCUIT is PPAD-hard for all {\textbackslash}varepsilon łt 0.1 by a reduction from PURE-CIRCUIT, and thus strengthen all prior work that has used GCIRCUIT as an intermediate problem from the existential-constant regime to the large-constant regime. We show that stronger inapproximability results can be derived by a direct reduction from PURE-CIRCUIT. In particular, we prove that finding an ε-well-supported Nash equilibrium in a polymatrix game is PPAD-hard for all {\textbackslash}varepsilon łt 1/3, and that this result is tight for two-action games.},
	urldate = {2025-02-16},
	booktitle = {2022 {IEEE} 63rd {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS})},
	author = {Deligkas, Argyrios and Fearnley, John and Hollender, Alexandros and Melissourgos, Themistoklis},
	month = oct,
	year = {2022},
	note = {ISSN: 2575-8454},
	keywords = {Computer science, Games, Nash equilibrium, PPAD, TFNP, approximation, generalized circuit, polymatrix games},
	pages = {159--170},
}

@misc{ikenmeyer_what_2022-1,
	title = {What is in \#{P} and what is not?},
	url = {http://arxiv.org/abs/2204.13149},
	doi = {10.48550/arXiv.2204.13149},
	abstract = {For several classical nonnegative integer functions, we investigate if they are members of the counting complexity class \#P or not. We prove \#P membership in surprising cases, and in other cases we prove non-membership, relying on standard complexity assumptions or on oracle separations. We initiate the study of the polynomial closure properties of \#P on affine varieties, i.e., if all problem instances satisfy algebraic constraints. This is directly linked to classical combinatorial proofs of algebraic identities and inequalities. We investigate \#TFNP and obtain oracle separations that prove the strict inclusion of \#P in all standard syntactic subclasses of \#TFNP-1.},
	urldate = {2025-02-16},
	publisher = {arXiv},
	author = {Ikenmeyer, Christian and Pak, Igor},
	month = apr,
	year = {2022},
	note = {arXiv:2204.13149 [cs]},
	keywords = {Computer Science - Computational Complexity, Mathematics - Combinatorics},
}

@article{deligkas_pure-circuit_2024,
	title = {Pure-{Circuit}: {Tight} {Inapproximability} for {PPAD}},
	volume = {71},
	issn = {0004-5411},
	shorttitle = {Pure-{Circuit}},
	url = {https://dl.acm.org/doi/10.1145/3678166},
	doi = {10.1145/3678166},
	abstract = {The current state-of-the-art methods for showing inapproximability in PPAD arise from the ɛ-Generalized-Circuit (ɛ-GCircuit) problem. Rubinstein (2018) showed that there exists a small unknown constant ɛ for which ɛ-GCircuit is PPAD-hard, and subsequent work has shown hardness results for other problems in PPAD by using ɛ-GCircuit as an intermediate problem.We introduce Pure-Circuit, a new intermediate problem for PPAD, which can be thought of as ɛ-GCircuit pushed to the limit as ɛ → 1, and we show that the problem is PPAD-complete. We then prove that ɛ-GCircuit is PPAD-hard for all ɛ \&lt; 1/10 by a reduction from Pure-Circuit, and thus strengthen all prior work that has used GCircuit as an intermediate problem from the existential-constant regime to the large-constant regime.We show that stronger inapproximability results can be derived by reducing directly from Pure-Circuit. In particular, we prove tight inapproximability results for computing approximate Nash equilibria and approximate well-supported Nash equilibria in graphical games, for finding approximate well-supported Nash equilibria in polymatrix games, and for finding approximate equilibria in threshold games.},
	number = {5},
	urldate = {2025-02-16},
	journal = {J. ACM},
	author = {Deligkas, Argyrios and Fearnley, John and Hollender, Alexandros and Melissourgos, Themistoklis},
	month = oct,
	year = {2024},
	pages = {31:1--31:48},
}

@misc{schryen_speedup_2023,
	title = {Speedup and efficiency of computational parallelization: {A} unifying approach and asymptotic analysis},
	shorttitle = {Speedup and efficiency of computational parallelization},
	url = {http://arxiv.org/abs/2212.11223},
	doi = {10.48550/arXiv.2212.11223},
	abstract = {In high performance computing environments, we observe an ongoing increase in the available numbers of cores. This development calls for re-emphasizing performance (scalability) analysis and speedup laws as suggested in the literature (e.g., Amdahl's law and Gustafson's law), with a focus on asymptotic performance. Understanding speedup and efficiency issues of algorithmic parallelism is useful for several purposes, including the optimization of system operations, temporal predictions on the execution of a program, and the analysis of asymptotic properties and the determination of speedup bounds. However, the literature is fragmented and shows a large diversity and heterogeneity of speedup models and laws. These phenomena make it challenging to obtain an overview of the models and their relationships, to identify the determinants of performance in a given algorithmic and computational context, and, finally, to determine the applicability of performance models and laws to a particular parallel computing setting. In this work, we provide a generic speedup (and thus also efficiency) model for homogeneous computing environments. Our approach generalizes many prominent models suggested in the literature and allows showing that they can be considered special cases of a unifying approach. The genericity of the unifying speedup model is achieved through parameterization. Considering combinations of parameter ranges, we identify six different asymptotic speedup cases and eight different asymptotic efficiency cases. Jointly applying these speedup and efficiency cases, we derive eleven scalability cases, from which we build a scalability typology. Researchers can draw upon our typology to classify their speedup model and to determine the asymptotic behavior when the number of parallel processing units increases. In addition, our results may be used to address various extensions of our setting.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Schryen, Guido},
	month = nov,
	year = {2023},
	note = {arXiv:2212.11223 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
}

@article{rodgers_improvements_1985,
	title = {Improvements in multiprocessor system design},
	volume = {13},
	issn = {0163-5964},
	url = {https://dl.acm.org/doi/10.1145/327070.327215},
	doi = {10.1145/327070.327215},
	number = {3},
	urldate = {2025-02-03},
	journal = {SIGARCH Comput. Archit. News},
	author = {Rodgers, David P.},
	month = jun,
	year = {1985},
	pages = {225--231},
}

@article{lorensen_marching_1987,
	title = {Marching cubes: {A} high resolution {3D} surface construction algorithm},
	volume = {21},
	issn = {0097-8930},
	shorttitle = {Marching cubes},
	url = {https://dl.acm.org/doi/10.1145/37402.37422},
	doi = {10.1145/37402.37422},
	abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
	number = {4},
	urldate = {2025-02-03},
	journal = {SIGGRAPH Comput. Graph.},
	author = {Lorensen, William E. and Cline, Harvey E.},
	month = aug,
	year = {1987},
	pages = {163--169},
}

@article{torrellas_false_1994,
	title = {False sharing and spatial locality in multiprocessor caches},
	volume = {43},
	issn = {1557-9956},
	url = {https://ieeexplore.ieee.org/abstract/document/286299},
	doi = {10.1109/12.286299},
	abstract = {The performance of the data cache in shared-memory multiprocessors has been shown to be different from that in uniprocessors. In particular, cache miss rates in multiprocessors do not show the sharp drop typical of uniprocessors when the size of the cache block increases. The resulting high cache miss rate is a cause of concern, since it can significantly limit the performance of multiprocessors. Some researchers have speculated that this effect is due to false sharing, the coherence transactions that result when different processors update different words of the same cache block in an interleaved fashion. While the analysis of six applications in the paper confirms that false sharing has a significant impact on the miss rate, the measurements also show that poor spatial locality among accesses to shared data has an even larger impact. To mitigate false sharing and to enhance spatial locality, we optimize the layout of shared data in cache blocks in a programmer-transparent manner. We show that this approach can reduce the number of misses on shared data by about 10\% on average.{\textless}{\textgreater}},
	number = {6},
	urldate = {2025-02-03},
	journal = {IEEE Transactions on Computers},
	author = {Torrellas, J. and Lam, H.S. and Hennessy, J.L.},
	month = jun,
	year = {1994},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Cache memory, Cache storage, Computer science education, Delay, Hardware, Interleaved codes, Large-scale systems, Optimizing compilers, Pensions, Programming profession},
	pages = {651--663},
}

@misc{l_2023,
	title = {Harnessing the power of {SIMD}: a spotlight on compiler limitations and memory alignment},
	url = {https://medium.com/@elinneon/harnessing-the-power-of-simd-a-spotlight-on-compiler-limitations-and-memory-alignment-51c60dfdde2e},
	author = {L, Ethan},
	month = oct,
	year = {2023},
}

@incollection{barry_chapter_2012,
	address = {Boston},
	title = {Chapter 11 - {Digital} {Signal} {Processing} {Using} {General}-{Purpose} {Processors}},
	isbn = {978-0-12-391490-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123914903000114},
	urldate = {2025-02-03},
	booktitle = {Modern {Embedded} {Computing}},
	publisher = {Morgan Kaufmann},
	author = {Barry, Peter and Crowley, Patrick},
	editor = {Barry, Peter and Crowley, Patrick},
	month = jan,
	year = {2012},
	doi = {10.1016/B978-0-12-391490-3.00011-4},
	pages = {317--346},
}

@misc{noauthor_best-practices_nodate,
	title = {Best-practices {\textbar} {Collapsing} {OpenMP} parallel loops},
	url = {https://co-design.pop-coe.eu/best-practices/openmp-nested-for-loops.html},
	urldate = {2025-02-03},
}

@misc{noauthor_art_nodate,
	title = {The {Art} of {HPC}},
	url = {https://theartofhpc.com/},
	urldate = {2025-02-03},
}

@article{blelloch1990prefix,
	title = {Prefix sums and their applications},
	author = {Blelloch, Guy E},
	year = {1990},
	note = {Publisher: School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA},
}

@phdthesis{beckingsale_towards_2015,
	title = {Towards {Scalable} {Adaptive} {Mesh} {Refinement} on {Future} {Parallel} {Architectures}},
	author = {Beckingsale, David},
	month = jan,
	year = {2015},
}

@inproceedings{conway_multiprocessor_1963,
	address = {New York, NY, USA},
	series = {{AFIPS} '63 ({Fall})},
	title = {A multiprocessor system design},
	isbn = {978-1-4503-7883-3},
	url = {https://dl.acm.org/doi/10.1145/1463822.1463838},
	doi = {10.1145/1463822.1463838},
	abstract = {Parallel processing is not so mysterious a concept as the dearth of algorithms which explicitly use it might suggest. As a rule of thumb, if N processes are performed and the outcome is independent of the order in which their steps are executed, provided that within each process the order of steps is preserved, then any or all of the processes can be performed simultaneously, if conflicts arising from multiple access to common storage can be resolved. All the elements of a matrix sum may be evaluated in parallel. The ith summand of all elements of a matrix product may be computed simultaneously. In an internal merge sort all strings in any pass may be created at the same time. All the coroutines of a separable program may be run concurrently.},
	urldate = {2025-02-03},
	booktitle = {Proceedings of the {November} 12-14, 1963, fall joint computer conference},
	publisher = {Association for Computing Machinery},
	author = {Conway, Melvin E.},
	month = nov,
	year = {1963},
	pages = {139--146},
}

@misc{noauthor_valgrind_nodate,
	title = {Valgrind},
	url = {https://valgrind.org/docs/manual/cl-manual.html},
	urldate = {2025-02-03},
}

@book{grossmann2007numerical,
	title = {Numerical treatment of partial differential equations},
	publisher = {Springer},
	author = {Grossmann, Christian},
	year = {2007},
}

@book{cannon_one-dimensional_1984,
	title = {The {One}-{Dimensional} {Heat} {Equation}},
	isbn = {978-0-521-30243-2},
	abstract = {This is a version of Gevrey's classical treatise on the heat equations. Included in this volume are discussions of initial and/or boundary value problems, numerical methods, free boundary problems and parameter determination problems. The material is presented as a monograph and/or information source book. After the first six chapters of standard classical material, each chapter is written as a self-contained unit except for an occasional reference to elementary definitions, theorems and lemmas in previous chapters.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Cannon, John Rozier},
	month = dec,
	year = {1984},
	note = {Google-Books-ID: XWSnBZxbz2oC},
	keywords = {Mathematics / Differential Equations / General, Mathematics / Differential Equations / Partial, Mathematics / General, Mathematics / Reference, Science / Mechanics / Thermodynamics},
}

@misc{noauthor_sign_nodate,
	title = {Sign in to {OneNote}},
	url = {https://www.onenote.com/hrd},
	urldate = {2025-01-21},
}

@inproceedings{takabi_brain_2016,
	title = {Brain {Computer} {Interface} ({BCI}) {Applications}: {Privacy} {Threats} and {Countermeasures}},
	shorttitle = {Brain {Computer} {Interface} ({BCI}) {Applications}},
	url = {https://ieeexplore.ieee.org/abstract/document/7809697},
	doi = {10.1109/CIC.2016.026},
	abstract = {In recent years, Brain-Computer Interfaces (BCIs) have gained popularity in non-medical domains such as the gaming, entertainment, personal health, and marketing industries. A growing number of companies offer various inexpensive consumer grade BCIs and some of these companies have recently introduced the concept of BCI "App stores" in order to facilitate the expansion of BCI applications and provide software development kits (SDKs) for other developers to create new applications for their devices. The BCI applications access to users' unique brainwave signals, which consequently allows them to make inferences about users' thoughts and mental processes. Since there are no specific standards that govern the development of BCI applications, its users are at the risk of privacy breaches. In this work, we perform first comprehensive analysis of BCI App stores including software development kits (SDKs), application programming interfaces (APIs), and BCI applications w.r.t privacy issues. The goal is to understand the way brainwave signals are handled by BCI applications and what threats to the privacy of users exist. Our findings show that most applications have unrestricted access to users' brainwave signals and can easily extract private information about their users without them even noticing. We discuss potential privacy threats posed by current practices used in BCI App stores and then describe some countermeasures that could be used to mitigate the privacy threats.},
	urldate = {2025-01-18},
	booktitle = {2016 {IEEE} 2nd {International} {Conference} on {Collaboration} and {Internet} {Computing} ({CIC})},
	author = {Takabi, Hassan and Bhalotiya, Anuj and Alohaly, Manar},
	month = nov,
	year = {2016},
	keywords = {Androids, Brain computer interface (BCI), Brainwave Signal, Companies, Electroencephalography, Electroencephalography (EEG), Headphones, Humanoid robots, Mobile Application, Privacy, Servers},
	pages = {102--111},
}

@incollection{silvers_fatal_2022,
	title = {A {Fatal} {Attraction} to {Normalizing}: {Treating} {Disabilities} as {Deviations} from “{Species}-{Typical}” {Functioning}},
	isbn = {978-1-00-328948-7},
	shorttitle = {A {Fatal} {Attraction} to {Normalizing}},
	abstract = {This chapter identifies and theorizes the sin of synecdoche: that of making a value judgment about a person on the basis of a single aspect of their identity. Asch and Wasserman are interested in how synecdoche factors into decisions to terminate pregnancies after the fetus is identified as having or being likely to develop some kind of disability. They argue that the sin of synecdoche involves a moral failing.},
	booktitle = {The {Disability} {Bioethics} {Reader}},
	publisher = {Routledge},
	author = {Silvers, Anita},
	year = {2022},
	note = {Num Pages: 13},
}

@misc{noauthor_robert_2001,
	title = {Robert {Byndom} v. {State} of {Arkansas}},
	url = {https://case-law.vlex.com/vid/byndom-v-state-00-890466866},
	abstract = {Robert Byndom v. State of Arkansas},
	language = {en},
	urldate = {2025-01-18},
	month = apr,
	year = {2001},
}

@misc{noauthor_byndom_nodate,
	title = {{BYNDOM} v. {STATE} (2001)},
	url = {https://caselaw.findlaw.com/court/ar-supreme-court/1337866.html},
	abstract = {Case opinion for AR Supreme Court BYNDOM v. STATE. Read the Court's full decision on FindLaw.},
	language = {en-US},
	urldate = {2025-01-18},
	journal = {Findlaw},
}

@misc{noauthor_byndom_nodate-1,
	title = {Byndom v {State}},
	url = {https://case-law.vlex.com/vid/byndom-v-state-00-890466866},
	abstract = {0: [object Object]. 1: [object Object]. 2: [object Object]. 3: [object Object]. 4: [object Object]},
	language = {en},
	urldate = {2025-01-18},
	journal = {vLex},
}

@article{dobkin_braincomputer_2007,
	title = {Brain–computer interface technology as a tool to augment plasticity and outcomes for neurological rehabilitation},
	volume = {579},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2151380/},
	doi = {10.1113/jphysiol.2006.123067},
	abstract = {Brain–computer interfaces (BCIs) are a rehabilitation tool for tetraplegic patients that aim to improve quality of life by augmenting communication, control of the environment, and self-care. The neurobiology of both rehabilitation and BCI control depends upon learning to modify the efficacy of spared neural ensembles that represent movement, sensation and cognition through progressive practice with feedback and reward. To serve patients, BCI systems must become safe, reliable, cosmetically acceptable, quickly mastered with minimal ongoing technical support, and highly accurate even in the face of mental distractions and the uncontrolled environment beyond a laboratory. BCI technologies may raise ethical concerns if their availability affects the decisions of patients who become locked-in with brain stem stroke or amyotrophic lateral sclerosis to be sustained with ventilator support. If BCI technology becomes flexible and affordable, volitional control of cortical signals could be employed for the rehabilitation of motor and cognitive impairments in hemiplegic or paraplegic patients by offering on-line feedback about cortical activity associated with mental practice, motor intention, and other neural recruitment strategies during progressive task-oriented practice. Clinical trials with measures of quality of life will be necessary to demonstrate the value of near-term and future BCI applications.},
	number = {Pt 3},
	urldate = {2025-01-18},
	journal = {The Journal of Physiology},
	author = {Dobkin, Bruce H},
	month = mar,
	year = {2007},
	pmid = {17095557},
	pmcid = {PMC2151380},
	pages = {637--642},
}

@article{soekadar_brain-machine_2014,
	title = {Brain-{Machine} {Interfaces} {In} {Neurorehabilitation} of {Stroke}.},
	volume = {83},
	doi = {10.1016/j.nbd.2014.11.025},
	abstract = {Stroke is among the leading causes of long-term disabilities leaving an increasing number of people with cognitive, affective and motor impairments depending on assistance in their daily life. While function after stroke can significantly improve in the first weeks and months, further recovery is often slow or non-existent in the more severe cases encompassing 30-50\% of all stroke victims. The neurobiological mechanisms underlying recovery in those patients are incompletely understood. However, recent studies demonstrated the brain's remarkable capacity for functional and structural plasticity and recovery even in severe chronic stroke. As all established rehabilitation strategies require some remaining motor function, there is currently no standardized and accepted treatment for patients with complete chronic muscle paralysis. The development of brain-machine interfaces (BMIs) that translate brain activity into control signals of computers or external devices provides two new strategies to overcome stroke-related motor paralysis. First, BMIs can establish continuous high-dimensional brain-control of robotic devices or functional electric stimulation (FES) to assist in daily life activities (assistive BMI). Second, BMIs could facilitate neuroplasticity, thus enhancing motor learning and motor recovery (rehabilitative BMI). Advances in sensor technology, development of non-invasive and implantable wireless BMI-systems and their combination with brain stimulation, along with evidence for BMI system's clinical efficacy suggest that BMI-related strategies will play an increasing role in neurorehabilitation of stroke.
Copyright © 2014. Published by Elsevier Inc.},
	journal = {Neurobiology of disease},
	author = {Soekadar, Surjo and Birbaumer, Niels and Slutzky, Marc and Cohen, Leonardo},
	month = dec,
	year = {2014},
}

@misc{neuralink_prime_2024,
	title = {{PRIME} {Study} {Progress} {Update} — {Second} {Participant}},
	url = {https://neuralink.com/blog/prime-study-progress-update-second-participant/},
	abstract = {Our technology is enabling our second PRIME Study participant to play video games and use CAD software. This post shares updates on his experience.},
	language = {en},
	urldate = {2025-01-18},
	journal = {Neuralink Blog},
	author = {Neuralink},
	month = aug,
	year = {2024},
}

@article{shih_brain-computer_2012,
	title = {Brain-{Computer} {Interfaces} in {Medicine}},
	volume = {87},
	issn = {0025-6196},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3497935/},
	doi = {10.1016/j.mayocp.2011.12.008},
	abstract = {Brain-computer interfaces (BCIs) acquire brain signals, analyze them, and translate them into commands that are relayed to output devices that carry out desired actions. BCIs do not use normal neuromuscular output pathways. The main goal of BCI is to replace or restore useful function to people disabled by neuromuscular disorders such as amyotrophic lateral sclerosis, cerebral palsy, stroke, or spinal cord injury. From initial demonstrations of electroencephalography-based spelling and single-neuron-based device control, researchers have gone on to use electroencephalographic, intracortical, electrocorticographic, and other brain signals for increasingly complex control of cursors, robotic arms, prostheses, wheelchairs, and other devices. Brain-computer interfaces may also prove useful for rehabilitation after stroke and for other disorders. In the future, they might augment the performance of surgeons or other medical professionals. Brain-computer interface technology is the focus of a rapidly growing research and development enterprise that is greatly exciting scientists, engineers, clinicians, and the public in general. Its future achievements will depend on advances in 3 crucial areas. Brain-computer interfaces need signal-acquisition hardware that is convenient, portable, safe, and able to function in all environments. Brain-computer interface systems need to be validated in long-term studies of real-world use by people with severe disabilities, and effective and viable models for their widespread dissemination must be implemented. Finally, the day-to-day and moment-to-moment reliability of BCI performance must be improved so that it approaches the reliability of natural muscle-based function.},
	number = {3},
	urldate = {2025-01-17},
	journal = {Mayo Clinic Proceedings},
	author = {Shih, Jerry J. and Krusienski, Dean J. and Wolpaw, Jonathan R.},
	month = mar,
	year = {2012},
	pmid = {22325364},
	pmcid = {PMC3497935},
	pages = {268--279},
}

@misc{noauthor_untitled_nodate,
	title = {Untitled presentation},
	url = {https://docs.google.com/presentation/d/13qeKBjEF4Rnlaoc5jcyWaXVKL4P0G1wnRApb9qawR5I/edit?ouid=110423806950173104606&usp=slides_home&ths=true&usp=embed_facebook},
	abstract = {DIGITAL TRANSFORMATION Strategy Plan 20XX COMPANY NAME},
	language = {en},
	urldate = {2025-01-16},
	journal = {Google Docs},
}

@article{kubler_brain-computer_2008,
	title = {Brain-computer interfaces and communication in paralysis: extinction of goal directed thinking in completely paralysed patients?},
	volume = {119},
	issn = {1388-2457},
	shorttitle = {Brain-computer interfaces and communication in paralysis},
	doi = {10.1016/j.clinph.2008.06.019},
	abstract = {OBJECTIVE: To investigate the relationship between physical impairment and brain-computer interface (BCI) performance.
METHOD: We present a meta-analysis of 29 patients with amyotrophic lateral sclerosis and six patients with other severe neurological diseases in different stages of physical impairment who were trained with a BCI. In most cases voluntary regulation of slow cortical potentials has been used as input signal for BCI-control. More recently sensorimotor rhythms and the P300 event-related brain potential were recorded.
RESULTS: A strong correlation has been found between physical impairment and BCI performance, indicating that performance worsens as impairment increases. Seven patients were in the complete locked-in state (CLIS) with no communication possible. After removal of these patients from the analysis, the relationship between physical impairment and BCI performance disappeared. The lack of a relation between physical impairment and BCI performance was confirmed when adding BCI data of patients from other BCI research groups.
CONCLUSIONS: Basic communication (yes/no) was not restored in any of the CLIS patients with a BCI. Whether locked-in patients can transfer learned brain control to the CLIS remains an open empirical question.
SIGNIFICANCE: Voluntary brain regulation for communication is possible in all stages of paralysis except the CLIS.},
	language = {eng},
	number = {11},
	journal = {Clinical Neurophysiology: Official Journal of the International Federation of Clinical Neurophysiology},
	author = {Kübler, A. and Birbaumer, N.},
	month = nov,
	year = {2008},
	pmid = {18824406},
	pmcid = {PMC2644824},
	keywords = {Adult, Aged, Brain Diseases, Communication Devices for People with Disabilities, Computer Systems, Electroencephalography, Event-Related Potentials, P300, Female, Goals, Humans, Male, Meta-Analysis as Topic, Middle Aged, Paralysis, Periodicity, Thinking, User-Computer Interface},
	pages = {2658--2666},
}

@article{hansson_implant_2005,
	title = {Implant ethics},
	volume = {31},
	issn = {0306-6800},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1734218/},
	doi = {10.1136/jme.2004.009803},
	abstract = {Implant ethics is defined here as the study of ethical aspects of the lasting introduction of technological devices into the human body. Whereas technological implants relieve us of some of the ethical problems connected with transplantation, other difficulties arise that are in need of careful analysis. A systematic approach to implant ethics is proposed. The major specific problems are identified as those concerning end of life issues (turning off devices), enhancement of human capabilities beyond normal levels, mental changes and personal identity, and cultural effects.},
	number = {9},
	urldate = {2025-01-16},
	journal = {Journal of Medical Ethics},
	author = {Hansson, S},
	month = sep,
	year = {2005},
	pmid = {16131553},
	pmcid = {PMC1734218},
	pages = {519--525},
}

@article{mikolajewska_ethical_2013,
	title = {Ethical considerations in the use of brain-computer interfaces},
	volume = {8},
	issn = {1644-3640},
	url = {https://doi.org/10.2478/s11536-013-0210-5},
	doi = {10.2478/s11536-013-0210-5},
	abstract = {Nervous system disorders are among the most severe disorders. Significant breakthroughs in contemporary clinical practice may provide brain-computer interfaces (BCIs) and neuroprostheses (NPs). The aim of this article is to investigate the extent to which the ethical considerations in the clinical application of brain-computer interfaces and associated threats are being identified. Ethical considerations and implications may significantly influence further development of BCIs and NPs. Moreover, there is significant public interest in supervising this development. Awareness of BCIs’ and NPs’ threats and limitations allow for wise planning and management in further clinical practice, especially in the area of long-term neurorehabilitation and care.},
	language = {en},
	number = {6},
	urldate = {2025-01-16},
	journal = {Central European Journal of Medicine},
	author = {Mikołajewska, Emilia and Mikołajewski, Dariusz},
	month = dec,
	year = {2013},
	keywords = {Assistive technology, Brain computer interface, Disabled people, Medical Ethics, Neuroprosthesis, Physiotherapy, Rehabilitation},
	pages = {720--724},
}

@article{vansteensel_towards_2023,
	title = {Towards clinical application of implantable brain–computer interfaces for people with late-stage {ALS}: medical and ethical considerations},
	volume = {270},
	issn = {1432-1459},
	shorttitle = {Towards clinical application of implantable brain–computer interfaces for people with late-stage {ALS}},
	url = {https://doi.org/10.1007/s00415-022-11464-6},
	doi = {10.1007/s00415-022-11464-6},
	abstract = {Individuals with amyotrophic lateral sclerosis (ALS) frequently develop speech and communication problems in the course of their disease. Currently available augmentative and alternative communication technologies do not present a solution for many people with advanced ALS, because these devices depend on residual and reliable motor activity. Brain–computer interfaces (BCIs) use neural signals for computer control and may allow people with late-stage ALS to communicate even when conventional technology falls short. Recent years have witnessed fast progression in the development and validation of implanted BCIs, which place neural signal recording electrodes in or on the cortex. Eventual widespread clinical application of implanted BCIs as an assistive communication technology for people with ALS will have significant consequences for their daily life, as well as for the clinical management of the disease, among others because of the potential interaction between the BCI and other procedures people with ALS undergo, such as tracheostomy. This article aims to facilitate responsible real-world implementation of implanted BCIs. We review the state of the art of research on implanted BCIs for communication, as well as the medical and ethical implications of the clinical application of this technology. We conclude that the contribution of all BCI stakeholders, including clinicians of the various ALS-related disciplines, will be needed to develop procedures for, and shape the process of, the responsible clinical application of implanted BCIs.},
	language = {en},
	number = {3},
	urldate = {2025-01-15},
	journal = {Journal of Neurology},
	author = {Vansteensel, Mariska J. and Klein, Eran and van Thiel, Ghislaine and Gaytant, Michael and Simmons, Zachary and Wolpaw, Jonathan R. and Vaughan, Theresa M.},
	month = mar,
	year = {2023},
	keywords = {Amyotrophic lateral sclerosis, Brain–computer interface, Clinical application, Ethics, Implant, Tracheostomy invasive ventilation},
	pages = {1323--1336},
}

@article{chandler_brain_2022,
	title = {Brain {Computer} {Interfaces} and {Communication} {Disabilities}: {Ethical}, {Legal}, and {Social} {Aspects} of {Decoding} {Speech} {From} the {Brain}},
	volume = {16},
	issn = {1662-5161},
	shorttitle = {Brain {Computer} {Interfaces} and {Communication} {Disabilities}},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.841035/full},
	doi = {10.3389/fnhum.2022.841035},
	abstract = {{\textless}p{\textgreater}A brain-computer interface technology that can decode the neural signals associated with attempted but unarticulated speech could offer a future efficient means of communication for people with severe motor impairments. Recent demonstrations have validated this approach. Here we assume that it will be possible in future to decode imagined (i.e., attempted but unarticulated) speech in people with severe motor impairments, and we consider the characteristics that could maximize the social utility of a BCI for communication. As a social interaction, communication involves the needs and goals of both speaker and listener, particularly in contexts that have significant potential consequences. We explore three high-consequence legal situations in which neurally-decoded speech could have implications: {\textless}italic{\textgreater}Testimony{\textless}/italic{\textgreater}, where decoded speech is used as evidence; {\textless}italic{\textgreater}Consent and Capacity{\textless}/italic{\textgreater}, where it may be used as a means of agency and participation such as consent to medical treatment; and {\textless}italic{\textgreater}Harm{\textless}/italic{\textgreater}, where such communications may be networked or may cause harm to others. We then illustrate how design choices might impact the social and legal acceptability of these technologies.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-01-15},
	journal = {Frontiers in Human Neuroscience},
	author = {Chandler, Jennifer A. and Van der Loos, Kiah I. and Boehnke, Susan and Beaudry, Jonas S. and Buchman, Daniel Z. and Illes, Judy},
	month = apr,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {AAC, Augmentative and alternative communication, BCI, Brain Computer Interface, Communication, Law, Neuroethics},
}

@article{burwell_ethical_2017,
	title = {Ethical aspects of brain computer interfaces: a scoping review},
	volume = {18},
	issn = {1472-6939},
	shorttitle = {Ethical aspects of brain computer interfaces},
	url = {https://doi.org/10.1186/s12910-017-0220-y},
	doi = {10.1186/s12910-017-0220-y},
	abstract = {Brain-Computer Interface (BCI) is a set of technologies that are of increasing interest to researchers. BCI has been proposed as assistive technology for individuals who are non-communicative or paralyzed, such as those with amyotrophic lateral sclerosis or spinal cord injury. The technology has also been suggested for enhancement and entertainment uses, and there are companies currently marketing BCI devices for those purposes (e.g., gaming) as well as health-related purposes (e.g., communication). The unprecedented direct connection created by BCI between human brains and computer hardware raises various ethical, social, and legal challenges that merit further examination and discussion.},
	language = {en},
	number = {1},
	urldate = {2025-01-15},
	journal = {BMC Medical Ethics},
	author = {Burwell, Sasha and Sample, Matthew and Racine, Eric},
	month = nov,
	year = {2017},
	keywords = {Brain-computer interface, Brain-machine interface, Ethics of technology, Medical Ethics, Scoping review},
	pages = {60},
}

@incollection{abecassis_chapter_2019,
	title = {Chapter 20 - {Brain}-{Computer} {Interface} ({BCI})},
	isbn = {978-0-323-48569-2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323485692000203},
	abstract = {Brain-computer interface encompasses a number of precise technologies aimed at restoring function to the central and peripheral nervous systems via capturing raw neural signals from various cortical regions and modulating the signal into a clinically meaningful output. Here we will describe the evolution of the different approaches used from a signal-acquisition perspective, however, in a historical context. Key examples are highlighted, sampling from successful implementation in an array of disease states, including stroke, neurodegeneration, spinal cord injury, and more.},
	urldate = {2025-01-14},
	booktitle = {Functional {Neurosurgery} and {Neuromodulation}},
	publisher = {Elsevier},
	author = {Abecassis, Isaac Josh and Ko, Andrew L.},
	editor = {Raslan, Ahmed M. and Burchiel, Kim J.},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-323-48569-2.00020-3},
	keywords = {Brain-computer interface, Brain-machine interface, Closed loop, Neural engineering},
	pages = {143--152},
}

@article{bin_Karim_Mixed_Naive_Bayes_2019,
	title = {Mixed naive bayes},
	journal = {https://github.com/remykarem/mixed-naive-bayes},
	author = {bin Karim, Raimi},
	month = oct,
	year = {2019},
}

@inproceedings{john_estimating_1995,
	address = {San Francisco, CA, USA},
	series = {{UAI}'95},
	title = {Estimating continuous distributions in {Bayesian} classifiers},
	isbn = {978-1-55860-385-1},
	abstract = {When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Eleventh} conference on {Uncertainty} in artificial intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {John, George H. and Langley, Pat},
	month = aug,
	year = {1995},
	pages = {338--345},
}

@misc{morris-knower_libguides_nodate,
	title = {{LibGuides}: {Measuring} your research impact: {G}-{Index}},
	copyright = {Copyright Cornell University 2025},
	shorttitle = {{LibGuides}},
	url = {https://guides.library.cornell.edu/impact/author-impact-g},
	abstract = {This guide provides an introduction to the various metrics used to measure researcher and journal impact.},
	language = {en},
	urldate = {2025-01-08},
	author = {Morris-Knower, Jim},
}

@misc{noauthor_technology_nodate,
	title = {Technology {\textbar} 2024 {Stack} {Overflow} {Developer} {Survey}},
	url = {https://survey.stackoverflow.co/2024/technology/#most-popular-technologies},
	language = {en},
	urldate = {2025-01-08},
}

@article{groote_factors_2023,
	title = {Factors {Affecting} {Publication} {Impact} and {Citation} {Trends} {Over} {Time}},
	volume = {18},
	copyright = {Copyright (c) 2023 Sandra L. De Groote, Jung Mi Scoulas, Paula R. Dempsey, Felicia Barrett},
	issn = {1715-720X},
	url = {https://journals.library.ualberta.ca/eblip/index.php/EBLIP/article/view/30206},
	doi = {10.18438/eblip30206},
	abstract = {Objective – The researchers investigated whether faculty use of the references in articles had a relationship with the later impact of the publication (measured by citation counts). The paper also reported on additional factors that may influence the later impact of publications.
Methods – This researchers analyzed data for articles published by faculty at a large public university from 1995 to 2015. Data were obtained from the Scopus abstract and citation database and analyzed using SPSS27 to conduct Pearson’s correlations and regression analysis.
Results – The number of references included in publications and the number of citations articles received each year following publication have increased over time. Publications received a greater number of citations annually in their 6th to 10th years, compared to the first 5. The number of references included in an article had a weak correlation with the number of citations an article received. Grant funded articles included more references and later received more citations than non-grant funded articles. Several variables, including number of references used in an article, the number of co-authors, and whether the article was grant funded, were shown to correlate with the later impact of a publication.
Conclusion – Based on the results, researchers should seek out grant funding and generously incorporate literature into their co-authored publications to increase their publications' potential for future impact. These factors may influence article quality, resulting in more citations over time. Further research is needed to better understand their influence and the influence of other factors.},
	language = {en},
	number = {2},
	urldate = {2025-01-08},
	journal = {Evidence Based Library and Information Practice},
	author = {Groote, Sandra L. De and Scoulas, Jung Mi and Dempsey, Paula R. and Barrett, Felicia},
	month = jun,
	year = {2023},
	note = {Number: 2},
	pages = {2--16},
}

@article{poirrier_robust_2021,
	title = {Robust h-index},
	volume = {126},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-020-03857-z},
	doi = {10.1007/s11192-020-03857-z},
	abstract = {The h-index is the most used measurement of impact for researchers. Sites such as Web of Science, Google Scholar, Microsoft Academic, and Scopus leverage it to show and compare the impact of authors. The h-index can be described in simple terms: it is the highest h for which an authors has h papers with the number of cites more or equal than h. Unfortunately, some researchers, in order to increase their productivity artificially, manipulate their h-index using different techniques such as self-citation. Even though it is relatively simple to discard self-citations, every day appears more sophisticated methods to artificially increase this index. One of these methods is collaborative citations, in which a researcher A cites indiscriminately another researcher B, with whom it has a previous collaboration, increasing her/his h-index. This work presents a new robust generalization of the h-index called rh-index that minimizes the impact of new collaborative citations, maintaining the importance of their citations previous to their collaborative work. To demonstrate the usefulness of the proposed index, we analyze its effect over 600 Chilean researchers. Our results show that, while some of the most cited researchers were barely affected, demonstrating their robustness, another group of authors show a substantial reduction in comparison to their original h-index.},
	language = {en},
	number = {3},
	urldate = {2025-01-08},
	journal = {Scientometrics},
	author = {Poirrier, Maurice and Moreno, Sebastián and Huerta-Cánepa, Gonzalo},
	month = mar,
	year = {2021},
	keywords = {Collaborative citation, Robust h-index, Self-citation, h-Index, h-index manipulation},
	pages = {1969--1981},
}

@misc{sadek_svd_2012,
	title = {{SVD} {Based} {Image} {Processing} {Applications}: {State} of {The} {Art}, {Contributions} and {Research} {Challenges}},
	shorttitle = {{SVD} {Based} {Image} {Processing} {Applications}},
	url = {http://arxiv.org/abs/1211.7102},
	doi = {10.48550/arXiv.1211.7102},
	abstract = {Singular Value Decomposition (SVD) has recently emerged as a new paradigm for processing different types of images. SVD is an attractive algebraic transform for image processing applications. The paper proposes an experimental survey for the SVD as an efficient transform in image processing applications. Despite the well-known fact that SVD offers attractive properties in imaging, the exploring of using its properties in various image applications is currently at its infancy. Since the SVD has many attractive properties have not been utilized, this paper contributes in using these generous properties in newly image applications and gives a highly recommendation for more research challenges. In this paper, the SVD properties for images are experimentally presented to be utilized in developing new SVD-based image processing applications. The paper offers survey on the developed SVD based image applications. The paper also proposes some new contributions that were originated from SVD properties analysis in different image processing. The aim of this paper is to provide a better understanding of the SVD in image processing and identify important various applications and open research directions in this increasingly important area; SVD based image processing in the future research.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Sadek, Rowayda A.},
	month = nov,
	year = {2012},
	note = {arXiv:1211.7102 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@article{chicco_advantages_2020,
	title = {The advantages of the {Matthews} correlation coefficient ({MCC}) over {F1} score and accuracy in binary classification evaluation},
	volume = {21},
	issn = {1471-2164},
	url = {https://doi.org/10.1186/s12864-019-6413-7},
	doi = {10.1186/s12864-019-6413-7},
	abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.},
	number = {1},
	urldate = {2025-01-07},
	journal = {BMC Genomics},
	author = {Chicco, Davide and Jurman, Giuseppe},
	month = jan,
	year = {2020},
	keywords = {Accuracy, Binary classification, Biostatistics, Confusion matrices, Dataset imbalance, F1 score, Genomics, Machine learning, Matthews correlation coefficient},
	pages = {6},
}

@misc{noauthor_semantic_nodate,
	title = {Semantic {Scholar} {\textbar} {Frequently} {Asked} {Questions}},
	url = {https://www.semanticscholar.org/faq},
	abstract = {Answers to commonly asked questions about Semantic Scholar.},
	language = {en},
	urldate = {2025-01-07},
}

@article{schober_correlation_2018,
	title = {Correlation {Coefficients}: {Appropriate} {Use} and {Interpretation}},
	volume = {126},
	issn = {0003-2999},
	shorttitle = {Correlation {Coefficients}},
	url = {https://journals.lww.com/anesthesia-analgesia/fulltext/2018/05000/correlation_coefficients__appropriate_use_and.50.aspx},
	doi = {10.1213/ANE.0000000000002864},
	abstract = {Correlation in the broadest sense is a measure of an association between variables. In correlated data, the change in the magnitude of 1 variable is associated with a change in the magnitude of another variable, either in the same (positive correlation) or in the opposite (negative correlation) direction. Most often, the term correlation is used in the context of a linear relationship between 2 continuous variables and expressed as Pearson product-moment correlation. The Pearson correlation coefficient is typically used for jointly normally distributed data (data that follow a bivariate normal distribution). For nonnormally distributed continuous data, for ordinal data, or for data with relevant outliers, a Spearman rank correlation can be used as a measure of a monotonic association. Both correlation coefficients are scaled such that they range from –1 to +1, where 0 indicates that there is no linear or monotonic association, and the relationship gets stronger and ultimately approaches a straight line (Pearson correlation) or a constantly increasing or decreasing curve (Spearman correlation) as the coefficient approaches an absolute value of 1. Hypothesis tests and confidence intervals can be used to address the statistical significance of the results and to estimate the strength of the relationship in the population from which the data were sampled. The aim of this tutorial is to guide researchers and clinicians in the appropriate use and interpretation of correlation coefficients.},
	language = {en-US},
	number = {5},
	urldate = {2025-01-07},
	journal = {Anesthesia \& Analgesia},
	author = {Schober, Patrick and Boer, Christa and Schwarte, Lothar A.},
	month = may,
	year = {2018},
	pages = {1763},
}

@misc{hassan_abir0sjr-journal-ranking_2024,
	title = {abir0/{SJR}-{Journal}-{Ranking}},
	copyright = {MIT},
	url = {https://github.com/abir0/SJR-Journal-Ranking},
	abstract = {A web scraping and visualization project on SJR and WoS journal indexes.},
	urldate = {2025-01-07},
	author = {Hassan, Abir},
	month = may,
	year = {2024},
	note = {original-date: 2023-06-25T14:26:54Z},
	keywords = {data-visualization, python, selenium, tableau, web-scraping},
}

@inproceedings{Wahle2022c,
	address = {Marseille, France},
	title = {D3: a massive dataset of scholarly metadata for analyzing the state of computer science research},
	booktitle = {Proceedings of the 13th language resources and evaluation conference},
	publisher = {European Language Resources Association},
	author = {Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif M. and Gipp, Bela},
	month = jul,
	year = {2022},
}

@inproceedings{lo_s2orc_2020,
	address = {Online},
	title = {{S2ORC}: {The} {Semantic} {Scholar} {Open} {Research} {Corpus}},
	shorttitle = {{S2ORC}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.447},
	doi = {10.18653/v1/2020.acl-main.447},
	language = {en},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Daniel},
	year = {2020},
	pages = {4969--4983},
}

@inproceedings{ammar_construction_2018,
	address = {New Orleans - Louisiana},
	title = {Construction of the {Literature} {Graph} in {Semantic} {Scholar}},
	url = {http://aclweb.org/anthology/N18-3011},
	doi = {10.18653/v1/N18-3011},
	language = {en},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 3 ({Industry} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ammar, Waleed and Groeneveld, Dirk and Bhagavatula, Chandra and Beltagy, Iz and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Elgohary, Ahmed and Feldman, Sergey and Ha, Vu and Kinney, Rodney and Kohlmeier, Sebastian and Lo, Kyle and Murray, Tyler and Ooi, Hsu-Han and Peters, Matthew and Power, Joanna and Skjonsberg, Sam and Wang, Lucy and Willhelm, Chris and Yuan, Zheng and Zuylen, Madeleine and {Oren}},
	year = {2018},
	pages = {84--91},
}

@misc{craft-morgan_research_nodate,
	title = {Research {Guides}: {Research} {Impact}: {Research} {Metrics}},
	copyright = {Copyright Ohio State University 2025},
	shorttitle = {Research {Guides}},
	url = {https://guides.osu.edu/c.php?g=608754&p=4233651},
	abstract = {Tracking and Enhancing the Impact of your Research},
	language = {en},
	urldate = {2025-01-05},
	author = {Craft-Morgan, Sheila},
}

@misc{noauthor_notitle_nodate,
	url = {https://www.atlantis-press.com/journals/jegh/metrics/cit_half_life},
	urldate = {2025-01-05},
}

@book{kumar_research_2015,
	title = {Research evaluation metrics},
	isbn = {978-92-3-100082-9},
	abstract = {Traducción parcial de la Introducción: "En la actualidad, la evaluación de la investigaciones es una cuestión que se está replanteando en todo el mundo. En algunos casos, los trabajos de investigación están generando resultados muy buenos, en la mayoría de los casos los resultados son mediocres, y en algunos casos negativos. Por todo esto, la evaluación de los resultados de la investigación se convierte en una condición sine qua non. Cuando el número de investigadores eran menos, eran los propios colegas de profesión quienes evaluaban la investigación. Con el paso del tiempo, el número de investigadores aumentó, las áreas de investigación proliferaron, los resultados de la investigación se multiplicaron. La tendencia continuó y después de la Segunda Guerra Mundial, la investigación comenzó a crecer exponencialmente. Hoy en día, incluso en una estimación moderada hay alrededor de más de un millón de investigadores y producen más de dos millón de trabajos de investigación y otros documentos por año. En este contexto, la evaluación de la investigación es una cuestión de primera importancia. Para cualquier promoción, acreditación, premio y beca puede haber decenas o cientos de nominados. De entre éstos, seleccionar el mejor candidato es una cuestión difícil de determinar. Las evaluaciones inter pares en muchos casos están demostrando ser subjetivas. En 1963 se crea Science Citation Index (SCI) que cubre la literatura científica desde 1961. Unos años después, Eugene Garfield, fundador del SCI, preparó una lista de los 50 autores científicos más citados basándose en las citas que recibía el trabajo de un autor por parte de los trabajos de otros colegas de investigación. El documento titulado "¿Pueden predecirse los ganadores del Premio Nobel? 'Fue publicado en 1968 (Garfield y Malin, 1968). En el siguiente año es decir, 1969, dos científicos que figuran en la lista, por ejemplo, Derek HR Barton y Murray Gell-Mann recibieron el codiciado premio. Esto reivindicó la utilidad del análisis de citas. Cada año, varios científicos pertenecientes al campo de la Física, Química, Fisiología y Medicina reciben el Premio Nobel. De esta manera el análisis de citas se convirtió en una herramienta útil. Sin embargo, el análisis de citas siempre tuvo críticas y múltiples fallas. Incluso Garfield comentó - "El Uso del análisis de citas de los trabajos de evaluación es una tarea difícil. Existen muchas posibilidades de error '(Garfiled, 1983). Para la evaluación de la investigación, se necesitaban algunos otros indicadores. El análisis de citas, junto con la revisión por pares garantiza el mejor juicio en innumerables casos. Pero se necesita algo que sea más exacto. La llegada de la World Wide Web (WWW) brindó la oportunidad; pues un buen número de indicadores se están generando a partir de los datos disponibles en la WWW". (Trad. Julio Alonso Arévalo. Univ. Salamanca).},
	language = {en},
	publisher = {UNESCO Publishing},
	author = {Kumar, Anup, Das},
	month = apr,
	year = {2015},
	note = {Google-Books-ID: MZlYCgAAQBAJ},
}

@misc{craft-morgan_research_nodate-1,
	title = {Research {Guides}: {Research} {Impact}: {Introduction}},
	copyright = {Copyright Ohio State University 2025},
	shorttitle = {Research {Guides}},
	url = {https://guides.osu.edu/c.php?g=608754&p=4224917},
	abstract = {Tracking and Enhancing the Impact of your Research},
	language = {en},
	urldate = {2025-01-05},
	author = {Craft-Morgan, Sheila},
}

@misc{noauthor_number_nodate,
	title = {Number of patent applications worldwide 2022},
	url = {https://www.statista.com/statistics/257610/number-of-patent-applications-worldwide/},
	abstract = {As of 2022, the number of patent applications worldwide amounted to about 3.45 million.},
	language = {en},
	urldate = {2024-12-30},
	journal = {Statista},
}

@article{fire_over-optimization_2019,
	title = {Over-optimization of academic publishing metrics: {Observing} {Goodhart}'s {Law} in action},
	volume = {8},
	shorttitle = {Over-optimization of academic publishing metrics},
	doi = {10.1093/gigascience/giz053},
	abstract = {Background
The academic publishing world is changing significantly, with ever-growing numbers of publications each year and shifting publishing patterns. However, the metrics used to measure academic success, such as the number of publications, citation number, and impact factor, have not changed for decades. Moreover, recent studies indicate that these metrics have become targets and follow Goodhart’s Law, according to which, “when a measure becomes a target, it ceases to be a good measure.”

Results
In this study, we analyzed {\textgreater}120 million papers to examine how the academic publishing world has evolved over the last century, with a deeper look into the specific field of biology. Our study shows that the validity of citation-based measures is being compromised and their usefulness is lessening. In particular, the number of publications has ceased to be a good metric as a result of longer author lists, shorter papers, and surging publication numbers. Citation-based metrics, such citation number and h-index, are likewise affected by the flood of papers, self-citations, and lengthy reference lists. Measures such as a journal’s impact factor have also ceased to be good metrics due to the soaring numbers of papers that are published in top journals, particularly from the same pool of authors. Moreover, by analyzing properties of {\textgreater}2,600 research fields, we observed that citation-based metrics are not beneficial for comparing researchers in different fields, or even in the same department.

Conclusions
Academic publishing has changed considerably; now we need to reconsider how we measure success.},
	journal = {GigaScience},
	author = {Fire, Michael and Guestrin, Carlos},
	month = jun,
	year = {2019},
}

@incollection{shaver_science_2018,
	address = {Cham},
	title = {Science {Today}},
	isbn = {978-3-319-91812-9},
	url = {https://doi.org/10.1007/978-3-319-91812-9_4},
	abstract = {90\% of all the scientists who have ever lived are alive today. By contrast, less than 7\% of all the people who have ever lived are alive today. It has been estimated that there were a few hundred scientists in the mid-1700s. If the number of scientists had increased at the same rate as the overall population, the number of scientists today would be a few thousand. Instead, according to UNESCO, there are about eight million researchers in the world today. The increase in the number of scientists over the last couple of hundred years is thousands of times the increase in the overall population. The growth rate in the number of scientists since the mid-1700s has been about 4\% per year, corresponding to a doubling time of about 18 years and far faster than the approximately 0.8\% per year growth rate for the overall population over that period. Currently in China the number of researchers is increasing at the furious rate of 6.6\% per year, while its overall population is growing at just 0.6 \% per year. There is no question that the number of scientists has increased dramatically over the last few hundred years (see Fig. 4.1).},
	language = {en},
	urldate = {2024-12-30},
	booktitle = {The {Rise} of {Science}: {From} {Prehistory} to the {Far} {Future}},
	publisher = {Springer International Publishing},
	author = {Shaver, Peter},
	editor = {Shaver, Peter},
	year = {2018},
	doi = {10.1007/978-3-319-91812-9_4},
	pages = {129--209},
}

@misc{ji_mipre_2022,
	title = {{MIP}*={RE}},
	url = {http://arxiv.org/abs/2001.04383},
	doi = {10.48550/arXiv.2001.04383},
	abstract = {We show that the class MIP* of languages that can be decided by a classical verifier interacting with multiple all-powerful quantum provers sharing entanglement is equal to the class RE of recursively enumerable languages. Our proof builds upon the quantum low-degree test of (Natarajan and Vidick, FOCS 2018) and the classical low-individual degree test of (Ji, et al., 2020) by integrating recent developments from (Natarajan and Wright, FOCS 2019) and combining them with the recursive compression framework of (Fitzsimons et al., STOC 2019). An immediate byproduct of our result is that there is an efficient reduction from the Halting Problem to the problem of deciding whether a two-player nonlocal game has entangled value \$1\$ or at most \$1/2\$. Using a known connection, undecidability of the entangled value implies a negative answer to Tsirelson's problem: we show, by providing an explicit example, that the closure \$C\_\{qa\}\$ of the set of quantum tensor product correlations is strictly included in the set \$C\_\{qc\}\$ of quantum commuting correlations. Following work of (Fritz, Rev. Math. Phys. 2012) and (Junge et al., J. Math. Phys. 2011) our results provide a refutation of Connes' embedding conjecture from the theory of von Neumann algebras.},
	urldate = {2024-12-21},
	publisher = {arXiv},
	author = {Ji, Zhengfeng and Natarajan, Anand and Vidick, Thomas and Wright, John and Yuen, Henry},
	month = nov,
	year = {2022},
	note = {arXiv:2001.04383 [quant-ph]},
	keywords = {Computer Science - Computational Complexity, Mathematics - Operator Algebras, Quantum Physics},
}

@article{aas_explaining_2021,
	title = {Explaining individual predictions when features are dependent: {More} accurate approximations to {Shapley} values},
	volume = {298},
	issn = {0004-3702},
	shorttitle = {Explaining individual predictions when features are dependent},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221000539},
	doi = {10.1016/j.artint.2021.103502},
	abstract = {Explaining complex or seemingly simple machine learning models is an important practical problem. We want to explain individual predictions from such models by learning simple, interpretable explanations. Shapley value is a game theoretic concept that can be used for this purpose. The Shapley value framework has a series of desirable theoretical properties, and can in principle handle any predictive model. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes that the features are independent. Since Shapley values currently suffer from inclusion of unrealistic data instances when features are correlated, the explanations may be very misleading. This is the case even if a simple linear model is used for predictions. In this paper, we extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with various degrees of feature dependence, where our method gives more accurate approximations to the true Shapley values.},
	urldate = {2024-11-26},
	journal = {Artificial Intelligence},
	author = {Aas, Kjersti and Jullum, Martin and Løland, Anders},
	month = sep,
	year = {2021},
	keywords = {Dependence, Feature attribution, Kernel SHAP, Shapley values},
	pages = {103502},
}

@misc{zhang_interpretable_2018,
	title = {Interpretable {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1710.00935},
	doi = {10.48550/arXiv.1710.00935},
	abstract = {This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
	month = feb,
	year = {2018},
	note = {arXiv:1710.00935},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ngiam_multimodal_2011,
	title = {Multimodal deep learning},
	url = {http://ai.stanford.edu/~ang/papers/icml11-MultimodalDeepLearning.pdf},
	urldate = {2024-11-26},
	booktitle = {Proceedings of the 28th international conference on machine learning ({ICML}-11)},
	author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y.},
	year = {2011},
	pages = {689--696},
}

@book{ngiam_multimodal_2011-1,
	title = {Multimodal {Deep} {Learning}},
	abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning. 1.},
	author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew},
	month = jan,
	year = {2011},
	note = {Journal Abbreviation: Proceedings of the 28th International Conference on Machine Learning, ICML 2011
Pages: 696
Publication Title: Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
}

@article{ngiam_multimodal_nodate,
	title = {Multimodal {Deep} {Learning}},
	abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classiﬁer is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classiﬁcation, demonstrating best published visual speech classiﬁcation on AVLetters and eﬀective shared representation learning.},
	language = {en},
	author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
}

@article{jiang_autosurv_2024,
	title = {Autosurv: interpretable deep learning framework for cancer survival analysis incorporating clinical and multi-omics data},
	volume = {8},
	copyright = {2024 The Author(s)},
	issn = {2397-768X},
	shorttitle = {Autosurv},
	url = {https://www.nature.com/articles/s41698-023-00494-6},
	doi = {10.1038/s41698-023-00494-6},
	abstract = {Accurate prognosis for cancer patients can provide critical information for optimizing treatment plans and improving life quality. Combining omics data and demographic/clinical information can offer a more comprehensive view of cancer prognosis than using omics or clinical data alone and can also reveal the underlying disease mechanisms at the molecular level. In this study, we developed and validated a deep learning framework to extract information from high-dimensional gene expression and miRNA expression data and conduct prognosis prediction for breast cancer and ovarian-cancer patients using multiple independent multi-omics datasets. Our model achieved significantly better prognosis prediction than the current machine learning and deep learning approaches in various settings. Moreover, an interpretation method was applied to tackle the “black-box” nature of deep neural networks and we identified features (i.e., genes, miRNA, demographic/clinical variables) that were important to distinguish predicted high- and low-risk patients. The significance of the identified features was partially supported by previous studies.},
	language = {en},
	number = {1},
	urldate = {2024-11-26},
	journal = {npj Precision Oncology},
	author = {Jiang, Lindong and Xu, Chao and Bai, Yuntong and Liu, Anqi and Gong, Yun and Wang, Yu-Ping and Deng, Hong-Wen},
	month = jan,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer, Computational biology and bioinformatics},
	pages = {1--16},
}

@inproceedings{tadesse_cardiovascular_2019,
	title = {Cardiovascular disease diagnosis using cross-domain transfer learning},
	url = {https://ieeexplore.ieee.org/abstract/document/8857737},
	doi = {10.1109/EMBC.2019.8857737},
	abstract = {While cardiovascular diseases (CVDs) are commonly diagnosed by cardiologists via inspecting electrocardiogram (ECG) waveforms, these decisions can be supported by a data-driven approach, which may automate this process. An automatic diagnostic approach often employs hand-crafted features extracted from ECG waveforms. These features, however, do not generalise well, challenged by variation in acquisition settings such as sampling rate and mounting points. Existing deep learning (DL) approaches, on the other hand, extract features from ECG automatically but require construction of dedicated networks that require huge data and computational resource if trained from scratch. Here we propose an end-to-end trainable cross-domain transfer learning for CVD classification from ECG waveforms, by utilising existing vision-based CNN frameworks as feature extractors, followed by ECG feature learning layers. Because these frameworks are designed for image inputs, we employ a stacked spectrogram representation of multi-lead ECG waveforms as a preprocessing step. We also proposed a fusion of multiple ECG leads, using plausible stacking arrangements of the spectrograms, to encode their spatial relations. The proposed approach is validated on multiple ECG datasets and competitive performance is achieved.},
	urldate = {2024-11-26},
	booktitle = {2019 41st {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Tadesse, Girmaw Abebe and Zhu, Tingting and Liu, Yong and Zhou, Yingling and Chen, Jiyan and Tian, Maoyi and Clifton, David},
	month = jul,
	year = {2019},
	note = {ISSN: 1558-4615},
	keywords = {Cardiovascular Disease, Deep Learning, Electrocardiography, Feature extraction, Health Informatics, Microsoft Windows, Spectrogram, Stacking, Support vector machines, Training, Transfer Learning},
	pages = {4262--4265},
}

@inproceedings{oussidi_deep_2018,
	title = {Deep generative models: {Survey}},
	shorttitle = {Deep generative models},
	url = {https://ieeexplore.ieee.org/abstract/document/8354080},
	doi = {10.1109/ISACV.2018.8354080},
	abstract = {Generative models have found their way to the forefront of deep learning the last decade and so far, it seems that the hype will not fade away any time soon. In this paper, we give an overview of the most important building blocks of most recent revolutionary deep generative models such as RBM, DBM, DBN, VAE and GAN. We will also take a look at three of state-of-the-art generative models, namely PixelRNN, DRAW and NADE. We will delve into their unique architectures, the learning procedures and their potential and limitations. We will also review some of the known issues that arise when trying to design and train deep generative architectures using shallow ones and how different models deal with these issues. This paper is not meant to be a comprehensive study of these models, but rather a starting point for those who bear an interest in the field.},
	urldate = {2024-11-26},
	booktitle = {2018 {International} {Conference} on {Intelligent} {Systems} and {Computer} {Vision} ({ISCV})},
	author = {Oussidi, Achraf and Elhassouny, Azeddine},
	month = apr,
	year = {2018},
	keywords = {Architecture, Data models, Decoding, Neural networks, Neurons, Probabilistic logic, Training},
	pages = {1--8},
}

@article{guo_deep_2020,
	title = {Deep {Group}-{Shuffling} {Dual} {Random} {Walks} {With} {Label} {Smoothing} for {Person} {Reidentification}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9015960},
	doi = {10.1109/ACCESS.2020.2976849},
	abstract = {Person reidentification (ReID) is a challenging task of finding a target pedestrian in a gallery set collected from multiple nonoverlapping camera views. Recently, state-of-the-art ReID performance has been achieved via an end-to-end trainable deep neural network framework, which integrates convolution feature extraction, similarity learning and reranking into a joint optimization framework. In such a framework, the similarity is learned via an embedding network, the reranking is conducted with a random walk, and the whole framework is optimized with a cross-entropy-based verification loss. Unfortunately, the embedding net is difficult to train well because their two-dimensional outputs mutually interfere each other when using the conventional random walk. In addition, the supervision information has not been fully exploited during the training phase due to the binary nature of the verification loss. In this paper, we propose a novel approach, called group-shuffling dual random walks with label smoothing (GSDRWLS), in which random walks are performed separately on two channels-one for positive verification and one for negative verification-and the binary verification labels are properly modified with an adaptive label smoothing technique before feeding into the verification loss in order to train the overall network effectively and to avoid the overfitting problem. Extensive experiments conducted on three large benchmark datasets, including CUHK03, Market-1501 and DukeMTMC, confirm the superior performance of our proposal.},
	urldate = {2024-11-26},
	journal = {IEEE Access},
	author = {Guo, Ruopei and Lin, Chaoqun and Li, Chun-Guang and Lin, Jiaru},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Feature extraction, Neural networks, Person reidentification, Probes, Proposals, Smoothing methods, Task analysis, Training, deep neural network, dual random walks, label smoothing},
	pages = {40018--40028},
}

@article{krittanawong_artificial_2022,
	title = {Artificial {Intelligence} and {Cardiovascular} {Genetics}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1729},
	url = {https://www.mdpi.com/2075-1729/12/2/279},
	doi = {10.3390/life12020279},
	abstract = {Polygenic diseases, which are genetic disorders caused by the combined action of multiple genes, pose unique and significant challenges for the diagnosis and management of affected patients. A major goal of cardiovascular medicine has been to understand how genetic variation leads to the clinical heterogeneity seen in polygenic cardiovascular diseases (CVDs). Recent advances and emerging technologies in artificial intelligence (AI), coupled with the ever-increasing availability of next generation sequencing (NGS) technologies, now provide researchers with unprecedented possibilities for dynamic and complex biological genomic analyses. Combining these technologies may lead to a deeper understanding of heterogeneous polygenic CVDs, better prognostic guidance, and, ultimately, greater personalized medicine. Advances will likely be achieved through increasingly frequent and robust genomic characterization of patients, as well the integration of genomic data with other clinical data, such as cardiac imaging, coronary angiography, and clinical biomarkers. This review discusses the current opportunities and limitations of genomics; provides a brief overview of AI; and identifies the current applications, limitations, and future directions of AI in genomics.},
	language = {en},
	number = {2},
	urldate = {2024-11-26},
	journal = {Life},
	author = {Krittanawong, Chayakrit and Johnson, Kipp W. and Choi, Edward and Kaplin, Scott and Venner, Eric and Murugan, Mullai and Wang, Zhen and Glicksberg, Benjamin S. and Amos, Christopher I. and Schatz, Michael C. and Tang, W. H. Wilson},
	month = feb,
	year = {2022},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI, artificial intelligence, cardiology, cardiovascular disease, deep learning, genetics, genomics, machine learning},
	pages = {279},
}

@book{altman1988roughly,
	series = {Technical report},
	title = {Roughly sorting: {Sequential} and parallel approach},
	url = {https://books.google.co.uk/books?id=0JwWHAAACAAJ},
	publisher = {University of Kentucky, Department of Computer Science},
	author = {Altman, T. and Igarashi, Y.},
	year = {1988},
}

@article{lindsey_transformative_2015,
	title = {Transformative {Impact} of {Proteomics} on {Cardiovascular} {Health} and {Disease}: {A} {Scientific} {Statement} {From} the {American} {Heart} {Association}},
	volume = {132},
	issn = {1524-4539},
	shorttitle = {Transformative {Impact} of {Proteomics} on {Cardiovascular} {Health} and {Disease}},
	doi = {10.1161/CIR.0000000000000226},
	abstract = {The year 2014 marked the 20th anniversary of the coining of the term proteomics. The purpose of this scientific statement is to summarize advances over this period that have catalyzed our capacity to address the experimental, translational, and clinical implications of proteomics as applied to cardiovascular health and disease and to evaluate the current status of the field. Key successes that have energized the field are delineated; opportunities for proteomics to drive basic science research, facilitate clinical translation, and establish diagnostic and therapeutic healthcare algorithms are discussed; and challenges that remain to be solved before proteomic technologies can be readily translated from scientific discoveries to meaningful advances in cardiovascular care are addressed. Proteomics is the result of disruptive technologies, namely, mass spectrometry and database searching, which drove protein analysis from 1 protein at a time to protein mixture analyses that enable large-scale analysis of proteins and facilitate paradigm shifts in biological concepts that address important clinical questions. Over the past 20 years, the field of proteomics has matured, yet it is still developing rapidly. The scope of this statement will extend beyond the reaches of a typical review article and offer guidance on the use of next-generation proteomics for future scientific discovery in the basic research laboratory and clinical settings.},
	language = {eng},
	number = {9},
	journal = {Circulation},
	author = {Lindsey, Merry L. and Mayr, Manuel and Gomes, Aldrin V. and Delles, Christian and Arrell, D. Kent and Murphy, Anne M. and Lange, Richard A. and Costello, Catherine E. and Jin, Yu-Fang and Laskowitz, Daniel T. and Sam, Flora and Terzic, Andre and Van Eyk, Jennifer and Srinivas, Pothur R. and {American Heart Association Council on Functional Genomics and Translational Biology, Council on Cardiovascular Disease in the Young, Council on Clinical Cardiology, Council on Cardiovascular and Stroke Nursing, Council on Hypertension, and Stroke Council}},
	month = sep,
	year = {2015},
	pmid = {26195497},
	keywords = {AHA Scientific Statements, American Heart Association, Cardiovascular Diseases, Cardiovascular System, Health Status, Humans, Proteomics, United States, biomarkers, mass spectrometry, proteome, systems biology, translational research},
	pages = {852--872},
}

@article{zhang_deep_2019,
	title = {Deep {Learning} for {Diagnosis} of {Chronic} {Myocardial} {Infarction} on                    {Nonenhanced} {Cardiac} {Cine} {MRI}},
	volume = {291},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/10.1148/radiol.2019182304},
	doi = {10.1148/radiol.2019182304},
	abstract = {Background Renal impairment is common in patients with coronary artery disease and,                            if severe, late gadolinium enhancement (LGE) imaging for myocardial                            infarction (MI) evaluation cannot be performed.Purpose To develop a fully automatic framework for chronic MI delineation via                            deep learning on non–contrast material–enhanced cardiac                            cine MRI.Materials and Methods In this retrospective single-center study, a deep learning model was                            developed to extract motion features from the left ventricle and                            delineate MI regions on nonenhanced cardiac cine MRI collected between                            October 2015 and March 2017. Patients with chronic MI, as well as                            healthy control patients, had both nonenhanced cardiac cine (25 phases                            per cardiac cycle) and LGE MRI examinations. Eighty percent of MRI                            examinations were used for the training data set and 20\% for the                            independent testing data set. Chronic MI regions on LGE MRI were defined                            as ground truth. Diagnostic performance was assessed by analysis of the                            area under the receiver operating characteristic curve (AUC). MI area                            and MI area percentage from nonenhanced cardiac cine and LGE MRI were                            compared by using the Pearson correlation, paired t                            test, and Bland-Altman analysis.Results Study participants included 212 patients with chronic MI (men, 171; age,                            57.2 years ± 12.5) and 87 healthy control patients (men, 42; age,                            43.3 years ± 15.5). Using the full cardiac cine MRI, the                            per-segment sensitivity and specificity for detecting chronic MI in the                            independent test set was 89.8\% and 99.1\%, respectively, with an AUC of                            0.94. There were no differences between nonenhanced cardiac cine and LGE                            MRI analyses in number of MI segments (114 vs 127, respectively;                                P = .38), per-patient MI area (6.2                                cm2 ± 2.8 vs 5.5 cm2 ± 2.3,                            respectively; P = .27; correlation coefficient,                                r = 0.88), and MI area percentage (21.5\%                            ± 17.3 vs 18.5\% ± 15.4; P = .17;                            correlation coefficient, r = 0.89).Conclusion The proposed deep learning framework on nonenhanced cardiac cine MRI                            enables the confirmation (presence), detection (position), and                            delineation (transmurality and size) of chronic myocardial infarction.                            However, future larger-scale multicenter studies are required for a full                            validation. Published under a CC BY 4.0 license. Online supplemental material is available for this                                    article. See also the editorial by Leiner in this issue.},
	number = {3},
	urldate = {2024-11-22},
	journal = {Radiology},
	author = {Zhang, Nan and Yang, Guang and Gao, Zhifan and Xu, Chenchu and Zhang, Yanping and Shi, Rui and Keegan, Jennifer and Xu, Lei and Zhang, Heye and Fan, Zhanming and Firmin, David},
	month = jun,
	year = {2019},
	note = {Publisher: Radiological Society of North America},
	pages = {606--617},
}

@article{virani_heart_2021,
	title = {Heart {Disease} and {Stroke} {Statistics}—2021 {Update}},
	volume = {143},
	url = {https://www.ahajournals.org/doi/10.1161/CIR.0000000000000950},
	doi = {10.1161/CIR.0000000000000950},
	abstract = {Background:

The American Heart Association, in conjunction with the National Institutes of Health, annually reports the most up-to-date statistics related to heart disease, stroke, and cardiovascular risk factors, including core health behaviors (smoking, physical activity, diet, and weight) and health factors (cholesterol, blood pressure, and glucose control) that contribute to cardiovascular health. The Statistical Update presents the latest data on a range of major clinical heart and circulatory disease conditions (including stroke, congenital heart disease, rhythm disorders, subclinical atherosclerosis, coronary heart disease, heart failure, valvular disease, venous disease, and peripheral artery disease) and the associated outcomes (including quality of care, procedures, and economic costs).
Methods:

The American Heart Association, through its Statistics Committee, continuously monitors and evaluates sources of data on heart disease and stroke in the United States to provide the most current information available in the annual Statistical Update. The 2021 Statistical Update is the product of a full year’s worth of effort by dedicated volunteer clinicians and scientists, committed government professionals, and American Heart Association staff members. This year’s edition includes data on the monitoring and benefits of cardiovascular health in the population, an enhanced focus on social determinants of health, adverse pregnancy outcomes, vascular contributions to brain health, the global burden of cardiovascular disease, and further evidence-based approaches to changing behaviors related to cardiovascular disease.
Results:

Each of the 27 chapters in the Statistical Update focuses on a different topic related to heart disease and stroke statistics.
Conclusions:

The Statistical Update represents a critical resource for the lay public, policy makers, media professionals, clinicians, health care administrators, researchers, health advocates, and others seeking the best available data on these factors and conditions.},
	number = {8},
	urldate = {2024-11-22},
	journal = {Circulation},
	author = {Virani, Salim S. and Alonso, Alvaro and Aparicio, Hugo J. and Benjamin, Emelia J. and Bittencourt, Marcio S. and Callaway, Clifton W. and Carson, April P. and Chamberlain, Alanna M. and Cheng, Susan and Delling, Francesca N. and Elkind, Mitchell S.V. and Evenson, Kelly R. and Ferguson, Jane F. and Gupta, Deepak K. and Khan, Sadiya S. and Kissela, Brett M. and Knutson, Kristen L. and Lee, Chong D. and Lewis, Tené T. and Liu, Junxiu and Loop, Matthew Shane and Lutsey, Pamela L. and Ma, Jun and Mackey, Jason and Martin, Seth S. and Matchar, David B. and Mussolino, Michael E. and Navaneethan, Sankar D. and Perak, Amanda Marma and Roth, Gregory A. and Samad, Zainab and Satou, Gary M. and Schroeder, Emily B. and Shah, Svati H. and Shay, Christina M. and Stokes, Andrew and VanWagner, Lisa B. and Wang, Nae-Yuh and Tsao, Connie W. and {On behalf of the American Heart Association Council on Epidemiology and Prevention Statistics Committee and Stroke Statistics Subcommittee}},
	month = feb,
	year = {2021},
	note = {Publisher: American Heart Association},
	pages = {e254--e743},
}

@article{avard_non-contrast_2022,
	title = {Non-contrast {Cine} {Cardiac} {Magnetic} {Resonance} image radiomics features and machine learning algorithms for myocardial infarction detection},
	volume = {141},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482521009392},
	doi = {10.1016/j.compbiomed.2021.105145},
	abstract = {Objective
Robust differentiation between infarcted and normal tissue is important for clinical diagnosis and precision medicine. The aim of this work is to investigate the radiomic features and to develop a machine learning algorithm for the differentiation of myocardial infarction (MI) and viable tissues/normal cases in the left ventricular myocardium on non-contrast Cine Cardiac Magnetic Resonance (Cine-CMR) images.
Methods
Seventy-two patients (52 with MI and 20 healthy control patients) were enrolled in this study. MR imaging was performed on a 1.5 T MRI using the following parameters: TR = 43.35 ms, TE = 1.22 ms, flip angle = 65°, temporal resolution of 30–40 ms. N4 bias field correction algorithm was applied to correct the inhomogeneity of images. All images were segmented and verified simultaneously by two cardiac imaging experts in consensus. Subsequently, features extraction was performed within the whole left ventricular myocardium (3D volume) in end-diastolic volume phase. Re-sampling to 1 × 1 × 1 mm3 voxels was performed for MR images. All intensities within the VOI of MR images were discretized to 64 bins. Radiomic features were normalized to obtain Z-scores, followed by Student's t-test statistical analysis for comparison. A p-value {\textless} 0.05 was used as a threshold for statistically significant differences and false discovery rate (FDR) correction performed to report q-value (FDR adjusted p-value). The extracted features were ranked using the MSVM-RFE algorithm, then Spearman correlation between features was performed to eliminate highly correlated features (R2 {\textgreater} 0.80). Ten different machine learning algorithms were used for classification and different metrics used for evaluation and various parameters used for models' evaluation.
Results
In univariate analysis, the highest area under the curve (AUC) of receiver operating characteristic (ROC) value was achieved for the Maximum 2D diameter slice (M2DS) shape feature (AUC = 0.88, q-value = 1.02E-7), while the average of univariate AUCs was 0.62 ± 0.08. In multivariate analysis, Logistic Regression (AUC = 0.93 ± 0.03, Accuracy = 0.86 ± 0.05, Recall = 0.87 ± 0.1, Precision = 0.93 ± 0.03 and F1 Score = 0.90 ± 0.04) and SVM (AUC = 0.92 ± 0.05, Accuracy = 0.85 ± 0.04, Recall = 0.92 ± 0.01, Precision = 0.88 ± 0.04 and F1 Score = 0.90 ± 0.02) yielded optimal performance as the best machine learning algorithm for this radiomics analysis.
Conclusion
This study demonstrated that using radiomics analysis on non-contrast Cine-CMR images enables to accurately detect MI, which could potentially be used as an alternative diagnostic method for Late Gadolinium Enhancement Cardiac Magnetic Resonance (LGE-CMR).},
	urldate = {2024-11-22},
	journal = {Computers in Biology and Medicine},
	author = {Avard, Elham and Shiri, Isaac and Hajianfar, Ghasem and Abdollahi, Hamid and Kalantari, Kiara Rezaei and Houshmand, Golnaz and Kasani, Kianosh and Bitarafan-rajabi, Ahmad and Deevband, Mohammad Reza and Oveisi, Mehrdad and Zaidi, Habib},
	month = feb,
	year = {2022},
	keywords = {Cine-CMR, Machine learning, Myocardial infarction, Radiomics},
	pages = {105145},
}

@article{liu_machine_2021,
	title = {Machine learning-based long-term outcome prediction in patients undergoing percutaneous coronary intervention},
	volume = {11},
	issn = {2223-3660, 2223-3652},
	url = {https://cdt.amegroups.org/article/view/69917},
	doi = {10.21037/cdt-21-37},
	abstract = {Machine learning-based long-term outcome prediction in patients undergoing percutaneous coronary intervention},
	language = {en},
	number = {3},
	urldate = {2024-11-22},
	journal = {Cardiovascular Diagnosis and Therapy},
	author = {Liu, Shangyu and Yang, Shengwen and Xing, Anlu and Zheng, Lihui and Shen, Lishui and Tu, Bin and Yao, Yan},
	month = jun,
	year = {2021},
	note = {Number: 3
Publisher: AME publishing company},
	pages = {73643--73743},
}

@article{broers_usefulness_2020,
	title = {Usefulness of a {Lifestyle} {Intervention} in {Patients} {With} {Cardiovascular} {Disease}},
	volume = {125},
	issn = {0002-9149},
	url = {https://www.sciencedirect.com/science/article/pii/S0002914919312287},
	doi = {10.1016/j.amjcard.2019.10.041},
	abstract = {The importance of modifying lifestyle factors in order to improve prognosis in cardiac patients is well-known. Current study aims to evaluate the effects of a lifestyle intervention on changes in lifestyle- and health data derived from wearable devices. Cardiac patients from Spain (n = 34) and The Netherlands (n = 36) were included in the current analysis. Data were collected for 210 days, using the Fitbit activity tracker, Beddit sleep tracker, Moves app (GPS tracker), and the Careportal home monitoring system. Locally Weighted Error Sum of Squares regression assessed trajectories of outcome variables. Linear Mixed Effects regression analysis was used to find relevant predictors of improvement deterioration of outcome measures. Analysis showed that Number of Steps and Activity Level significantly changed over time (F = 58.21, p {\textless} 0.001; F = 6.33, p = 0.01). No significant changes were observed on blood pressure, weight, and sleep efficiency. Secondary analysis revealed that being male was associated with higher activity levels (F = 12.53, p {\textless} 0.001) and higher number of steps (F = 8.44, p {\textless} 0.01). Secondary analysis revealed demographic (gender, nationality, marital status), clinical (co-morbidities, heart failure), and psychological (anxiety, depression) profiles that were associated with lifestyle measures. In conclusion results showed that physical activity increased over time and that certain subgroups of patients were more likely to have a better lifestyle behaviors based on their demographic, clinical, and psychological profile. This advocates a personalized approach in future studies in order to change lifestyle in cardiac patients.},
	number = {3},
	urldate = {2024-11-22},
	journal = {The American Journal of Cardiology},
	author = {Broers, Eva R. and Gavidia, Giovana and Wetzels, Mart and Ribas, Vicent and Ayoola, Idowu and Piera-Jimenez, Jordi and Widdershoven, Jos W. M. G. and Habibović, Mirela},
	month = feb,
	year = {2020},
	pages = {370--375},
}

@misc{noauthor_broers_nodate,
	title = {Broers: {Usefulness} of a lifestyle intervention in... - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar_lookup?journal=Am.%20J.%20Cardiol.&title=Usefulness%20of%20a%20Lifestyle%20Intervention%20in%20Patients%20with%20Cardiovascular%20Disease&author=E.R.%20Broers&author=G.%20Gavidia&author=M.%20Wetzels&author=V.%20Ribas&author=I.%20Ayoola&volume=125&publication_year=2019&pages=370-375&pmid=31761149&doi=10.1016/j.amjcard.2019.10.041&},
	urldate = {2024-11-22},
}

@article{bertsimas_personalized_2020,
	title = {Personalized treatment for coronary artery disease patients: a machine learning approach},
	volume = {23},
	issn = {1572-9389},
	shorttitle = {Personalized treatment for coronary artery disease patients},
	url = {https://doi.org/10.1007/s10729-020-09522-4},
	doi = {10.1007/s10729-020-09522-4},
	abstract = {Current clinical practice guidelines for managing Coronary Artery Disease (CAD) account for general cardiovascular risk factors. However, they do not present a framework that considers personalized patient-specific characteristics. Using the electronic health records of 21,460 patients, we created data-driven models for personalized CAD management that significantly improve health outcomes relative to the standard of care. We develop binary classifiers to detect whether a patient will experience an adverse event due to CAD within a 10-year time frame. Combining the patients’ medical history and clinical examination results, we achieve 81.5\% AUC. For each treatment, we also create a series of regression models that are based on different supervised machine learning algorithms. We are able to estimate with average R2 = 0.801 the outcome of interest; the time from diagnosis to a potential adverse event (TAE). Leveraging combinations of these models, we present ML4CAD, a novel personalized prescriptive algorithm. Considering the recommendations of multiple predictive models at once, the goal of ML4CAD is to identify for every patient the therapy with the best expected TAE using a voting mechanism. We evaluate its performance by measuring the prescription effectiveness and robustness under alternative ground truths. We show that our methodology improves the expected TAE upon the current baseline by 24.11\%, increasing it from 4.56 to 5.66 years. The algorithm performs particularly well for the male (24.3\% improvement) and Hispanic (58.41\% improvement) subpopulations. Finally, we create an interactive interface, providing physicians with an intuitive, accurate, readily implementable, and effective tool.},
	language = {en},
	number = {4},
	urldate = {2024-11-22},
	journal = {Health Care Management Science},
	author = {Bertsimas, Dimitris and Orfanoudaki, Agni and Weiner, Rory B.},
	month = dec,
	year = {2020},
	keywords = {Coronary artery disease, Machine learning, Personalization, Precision medicine, Prescriptions},
	pages = {482--506},
}

@article{hoogeveen_improved_2020,
	title = {Improved cardiovascular risk prediction using targeted plasma proteomics in primary prevention},
	volume = {41},
	issn = {0195-668X},
	url = {https://doi.org/10.1093/eurheartj/ehaa648},
	doi = {10.1093/eurheartj/ehaa648},
	abstract = {In the era of personalized medicine, it is of utmost importance to be able to identify subjects at the highest cardiovascular (CV) risk. To date, single biomarkers have failed to markedly improve the estimation of CV risk. Using novel technology, simultaneous assessment of large numbers of biomarkers may hold promise to improve prediction. In the present study, we compared a protein-based risk model with a model using traditional risk factors in predicting CV events in the primary prevention setting of the European Prospective Investigation (EPIC)-Norfolk study, followed by validation in the Progressione della Lesione Intimale Carotidea (PLIC) cohort.Using the proximity extension assay, 368 proteins were measured in a nested case–control sample of 822 individuals from the EPIC-Norfolk prospective cohort study and 702 individuals from the PLIC cohort. Using tree-based ensemble and boosting methods, we constructed a protein-based prediction model, an optimized clinical risk model, and a model combining both. In the derivation cohort (EPIC-Norfolk), we defined a panel of 50 proteins, which outperformed the clinical risk model in the prediction of myocardial infarction [area under the curve (AUC) 0.754 vs. 0.730; P \&lt; 0.001] during a median follow-up of 20 years. The clinically more relevant prediction of events occurring within 3 years showed an AUC of 0.732 using the clinical risk model and an AUC of 0.803 for the protein model (P \&lt; 0.001). The predictive value of the protein panel was confirmed to be superior to the clinical risk model in the validation cohort (AUC 0.705 vs. 0.609; P \&lt; 0.001).In a primary prevention setting, a proteome-based model outperforms a model comprising clinical risk factors in predicting the risk of CV events. Validation in a large prospective primary prevention cohort is required to address the value for future clinical implementation in CV prevention.},
	number = {41},
	urldate = {2024-11-22},
	journal = {European Heart Journal},
	author = {Hoogeveen, Renate M and Pereira, João P Belo and Nurmohamed, Nick S and Zampoleri, Veronica and Bom, Michiel J and Baragetti, Andrea and Boekholdt, S Matthijs and Knaapen, Paul and Khaw, Kay-Tee and Wareham, Nicholas J and Groen, Albert K and Catapano, Alberico L and Koenig, Wolfgang and Levin, Evgeni and Stroes, Erik S G},
	month = nov,
	year = {2020},
	pages = {3998--4007},
}

@article{sengupta_cognitive_2016,
	title = {Cognitive {Machine}-{Learning} {Algorithm} for {Cardiac} {Imaging}},
	volume = {9},
	url = {https://www.ahajournals.org/doi/full/10.1161/CIRCIMAGING.115.004330},
	doi = {10.1161/CIRCIMAGING.115.004330},
	abstract = {Background—Associating a patient’s profile with the memories of prototypical patients built through previous repeat clinical experience is a key process in clinical judgment. We hypothesized that a similar process using a cognitive computing tool would be well suited for learning and recalling multidimensional attributes of speckle tracking echocardiography data sets derived from patients with known constrictive pericarditis and restrictive cardiomyopathy.Methods and Results—Clinical and echocardiographic data of 50 patients with constrictive pericarditis and 44 with restrictive cardiomyopathy were used for developing an associative memory classifier–based machine-learning algorithm. The speckle tracking echocardiography data were normalized in reference to 47 controls with no structural heart disease, and the diagnostic area under the receiver operating characteristic curve of the associative memory classifier was evaluated for differentiating constrictive pericarditis from restrictive cardiomyopathy. Using only speckle tracking echocardiography variables, associative memory classifier achieved a diagnostic area under the curve of 89.2\%, which improved to 96.2\% with addition of 4 echocardiographic variables. In comparison, the area under the curve of early diastolic mitral annular velocity and left ventricular longitudinal strain were 82.1\% and 63.7\%, respectively. Furthermore, the associative memory classifier demonstrated greater accuracy and shorter learning curves than other machine-learning approaches, with accuracy asymptotically approaching 90\% after a training fraction of 0.3 and remaining flat at higher training fractions.Conclusions—This study demonstrates feasibility of a cognitive machine-learning approach for learning and recalling patterns observed during echocardiographic evaluations. Incorporation of machine-learning algorithms in cardiac imaging may aid standardized assessments and support the quality of interpretations, particularly for novice readers with limited experience.},
	number = {6},
	urldate = {2024-11-22},
	journal = {Circulation: Cardiovascular Imaging},
	author = {Sengupta, Partho P. and Huang, Yen-Min and Bansal, Manish and Ashrafi, Ali and Fisher, Matt and Shameer, Khader and Gall, Walt and Dudley, Joel T.},
	month = jun,
	year = {2016},
	note = {Publisher: American Heart Association},
	pages = {e004330},
}

@article{banerjee_completely_2021,
	title = {A completely automated pipeline for {3D} reconstruction of human heart from {2D} cine magnetic resonance slices},
	volume = {379},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2020.0257},
	doi = {10.1098/rsta.2020.0257},
	abstract = {Cardiac magnetic resonance (CMR) imaging is a valuable modality in the diagnosis and characterization of cardiovascular diseases, since it can identify abnormalities in structure and function of the myocardium non-invasively and without the need for ionizing radiation. However, in clinical practice, it is commonly acquired as a collection of separated and independent 2D image planes, which limits its accuracy in 3D analysis. This paper presents a completely automated pipeline for generating patient-specific 3D biventricular heart models from cine magnetic resonance (MR) slices. Our pipeline automatically selects the relevant cine MR images, segments them using a deep learning-based method to extract the heart contours, and aligns the contours in 3D space correcting possible misalignments due to breathing or subject motion first using the intensity and contours information from the cine data and next with the help of a statistical shape model. Finally, the sparse 3D representation of the contours is used to generate a smooth 3D biventricular mesh. The computational pipeline is applied and evaluated in a CMR dataset of 20 healthy subjects. Our results show an average reduction of misalignment artefacts from 1.82 ± 1.60 mm to 0.72 ± 0.73 mm over 20 subjects, in terms of distance from the final reconstructed mesh. The high-resolution 3D biventricular meshes obtained with our computational pipeline are used for simulations of electrical activation patterns, showing agreement with non-invasive electrocardiographic imaging. The automatic methodologies presented here for patient-specific MR imaging-based 3D biventricular representations contribute to the efficient realization of precision medicine, enabling the enhanced interpretability of clinical data, the digital twin vision through patient-specific image-based modelling and simulation, and augmented reality applications.
This article is part of the theme issue ‘Advanced computation in cardiovascular physiology: new challenges and opportunities’.},
	number = {2212},
	urldate = {2024-11-22},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Banerjee, Abhirup and Camps, Julià and Zacur, Ernesto and Andrews, Christopher M. and Rudy, Yoram and Choudhury, Robin P. and Rodriguez, Blanca and Grau, Vicente},
	month = oct,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {ECGI, cardiac mesh reconstruction, cine MRI, electrophysiological simulation, misalignment correction},
	pages = {20200257},
}

@article{mohsen_artificial_2023,
	title = {Artificial {Intelligence}-{Based} {Methods} for {Precision} {Cardiovascular} {Medicine}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-4426},
	url = {https://www.mdpi.com/2075-4426/13/8/1268},
	doi = {10.3390/jpm13081268},
	abstract = {Precision medicine has the potential to revolutionize the way cardiovascular diseases are diagnosed, predicted, and treated by tailoring treatment strategies to the individual characteristics of each patient. Artificial intelligence (AI) has recently emerged as a promising tool for improving the accuracy and efficiency of precision cardiovascular medicine. In this scoping review, we aimed to identify and summarize the current state of the literature on the use of AI in precision cardiovascular medicine. A comprehensive search of electronic databases, including Scopes, Google Scholar, and PubMed, was conducted to identify relevant studies. After applying inclusion and exclusion criteria, a total of 28 studies were included in the review. We found that AI is being increasingly applied in various areas of cardiovascular medicine, including the diagnosis, prognosis of cardiovascular diseases, risk prediction and stratification, and treatment planning. As a result, most of these studies focused on prediction (50\%), followed by diagnosis (21\%), phenotyping (14\%), and risk stratification (14\%). A variety of machine learning models were utilized in these studies, with logistic regression being the most used (36\%), followed by random forest (32\%), support vector machine (25\%), and deep learning models such as neural networks (18\%). Other models, such as hierarchical clustering (11\%), Cox regression (11\%), and natural language processing (4\%), were also utilized. The data sources used in these studies included electronic health records (79\%), imaging data (43\%), and omics data (4\%). We found that AI is being increasingly applied in various areas of cardiovascular medicine, including the diagnosis, prognosis of cardiovascular diseases, risk prediction and stratification, and treatment planning. The results of the review showed that AI has the potential to improve the performance of cardiovascular disease diagnosis and prognosis, as well as to identify individuals at high risk of developing cardiovascular diseases. However, further research is needed to fully evaluate the clinical utility and effectiveness of AI-based approaches in precision cardiovascular medicine. Overall, our review provided a comprehensive overview of the current state of knowledge in the field of AI-based methods for precision cardiovascular medicine and offered new insights for researchers interested in this research area.},
	language = {en},
	number = {8},
	urldate = {2024-11-22},
	journal = {Journal of Personalized Medicine},
	author = {Mohsen, Farida and Al-Saadi, Balqees and Abdi, Nima and Khan, Sulaiman and Shah, Zubair},
	month = aug,
	year = {2023},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, cardiovascular diseases, machine learning, precision medicine},
	pages = {1268},
}

@article{krittanawong_artificial_2017,
	title = {Artificial {Intelligence} in {Precision} {Cardiovascular} {Medicine}},
	volume = {69},
	issn = {0735-1097},
	url = {https://www.sciencedirect.com/science/article/pii/S0735109717368456},
	doi = {10.1016/j.jacc.2017.03.571},
	abstract = {Artificial intelligence (AI) is a field of computer science that aims to mimic human thought processes, learning capacity, and knowledge storage. AI techniques have been applied in cardiovascular medicine to explore novel genotypes and phenotypes in existing diseases, improve the quality of patient care, enable cost-effectiveness, and reduce readmission and mortality rates. Over the past decade, several machine-learning techniques have been used for cardiovascular disease diagnosis and prediction. Each problem requires some degree of understanding of the problem, in terms of cardiovascular medicine and statistics, to apply the optimal machine-learning algorithm. In the near future, AI will result in a paradigm shift toward precision cardiovascular medicine. The potential of AI in cardiovascular medicine is tremendous; however, ignorance of the challenges may overshadow its potential clinical impact. This paper gives a glimpse of AI’s application in cardiovascular clinical care and discusses its potential role in facilitating precision cardiovascular medicine.},
	number = {21},
	urldate = {2024-11-22},
	journal = {Journal of the American College of Cardiology},
	author = {Krittanawong, Chayakrit and Zhang, HongJu and Wang, Zhen and Aydar, Mehmet and Kitai, Takeshi},
	month = may,
	year = {2017},
	keywords = {big data, cognitive computing, deep learning, machine learning},
	pages = {2657--2664},
}

@article{gruson_collaborative_2020,
	title = {Collaborative {AI} and {Laboratory} {Medicine} integration in precision cardiovascular medicine},
	volume = {509},
	issn = {0009-8981},
	url = {https://www.sciencedirect.com/science/article/pii/S0009898120302655},
	doi = {10.1016/j.cca.2020.06.001},
	abstract = {Artificial Intelligence (AI) is a broad term that combines computation with sophisticated mathematical models and in turn allows the development of complex algorithms which are capable to simulate human intelligence such as problem solving and learning. It is devised to promote a significant paradigm shift in the most diverse areas of medical knowledge. On the other hand, Cardiology is a vast field dealing with diseases relating to the heart, the circulatory system, and includes coronary heart disease, cerebrovascular disease, rheumatic heart disease and other conditions. AI has emerged as a promising tool in cardiovascular medicine which is aimed in augmenting the effectiveness of the cardiologist and to extend better quality to patients. It has the ability to support decision‑making and improve diagnostic and prognostic performance. Attempt has also been made to explore novel genotypes and phenotypes in existing cardiovascular diseases, improve the quality of patient care, to enablecost-effectiveness with reducereadmissionand mortality rates. Our review addresses the integration of AI and laboratory medicine as an accelerator of personalization care associated with the precision and the need of value creation services in cardiovascular medicine.},
	urldate = {2024-11-22},
	journal = {Clinica Chimica Acta},
	author = {Gruson, Damien and Bernardini, Sergio and Dabla, Pradeep Kumar and Gouget, Bernard and Stankovic, Sanja},
	month = oct,
	year = {2020},
	keywords = {Artificial intelligence, Biomarkers, Cardiology, Data, Laboratory, Machine learning, Personalized},
	pages = {67--71},
}

@incollection{kattan_expert_2001,
	address = {Oxford},
	title = {Expert {Systems} in {Medicine}},
	isbn = {978-0-08-043076-8},
	url = {https://www.sciencedirect.com/science/article/pii/B0080430767005568},
	abstract = {Medical expert systems are designed to improve patient care by optimizing medical decision making. The distinguishing feature of medical expert systems is that they make recommendations based on input data; they are differentiated from decision support systems in that the latter are designed to help clinicians make decisions rather than actually make the recommendation, which is what an expert system does. This recommendation is essentially a prediction (of diagnosis or prognosis) or prescription (i.e., a treatment recommendation). The key issue in measuring the progress made thus far in medical expert systems is that of efficacy evaluation. For diagnostic or prognostic problems, the question is whether these systems have been able to predict more accurately than human experts. The literature suggests that expert systems are at least as accurate as human experts. For systems that make treatment recommendations, have medical expert systems truly improved patient outcomes? Again, it appears that validated expert systems make recommendations that are at least as good as those made by human experts, but the literature base is smaller. When trying to answer either of these questions, numerous issues need to be considered.},
	urldate = {2024-11-22},
	booktitle = {International {Encyclopedia} of the {Social} \& {Behavioral} {Sciences}},
	publisher = {Pergamon},
	author = {Kattan, M. W.},
	editor = {Smelser, Neil J. and Baltes, Paul B.},
	month = jan,
	year = {2001},
	doi = {10.1016/B0-08-043076-7/00556-8},
	pages = {5135--5139},
}
