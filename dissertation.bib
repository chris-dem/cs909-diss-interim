@misc{_ShellIntegrationFeatures_,
  title = {Shell {{Integration}} - {{Features}}},
  journal = {Ghostty},
  url = {https://ghostty.org},
  urldate = {2025-02-24},
  abstract = {Some Ghostty features require integrating with your shell. Ghostty can automatically inject shell integration for bash, zsh, fish, and elvish.},
  langid = {english},
  file = {/Users/christosdemetriou/Zotero/storage/KIBINHFH/shell-integration.html}
}

@inproceedings{aaronson_AlgebrizationNewBarrier_2008,
  title = {Algebrization: A New Barrier in Complexity Theory},
  shorttitle = {Algebrization},
  booktitle = {Proceedings of the Fortieth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Aaronson, Scott and Wigderson, Avi},
  year = {2008},
  month = may,
  series = {{{STOC}} '08},
  pages = {731--740},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1374376.1374481},
  url = {https://doi.org/10.1145/1374376.1374481},
  urldate = {2025-02-18},
  abstract = {Any proof of P!=NP will have to overcome two barriers: relativization and natural proofs. Yet over the last decade, we have seen circuit lower bounds (for example, that PP does not have linear-size circuits) that overcome both barriers simultaneously. So the question arises of whether there is a third barrier to progress on the central questions in complexity theory.In this paper we present such a barrier, which we call algebraic relativization or algebrization. The idea is that, when we relativize some complexity class inclusion, we should give the simulating machine access not only to an oracle A, but also to a low-degree extension of A over a finite field or ring.We systematically go through basic results and open problems in complexity theory to delineate the power of the new algebrization barrier. First, we show that all known non-relativizing results based on arithmetization -- both inclusions such as IP=PSPACE and MIP=NEXP, and separations such as MAEXP not in P/poly -- do indeed algebrize. Second, we show that almost all of the major open problems -- including P versus NP, P versus RP, and NEXP versus P/poly -- will require non-algebrizing techniques. In some cases algebrization seems to explain exactly why progress stopped where it did: for example, why we have superlinear circuit lower bounds for PromiseMA but not for NP.Our second set of results follows from lower bounds in a new model of algebraic query complexity, which we introduce in this paper and which is interesting in its own right. Some of our lower bounds use direct combinatorial and algebraic arguments, while others stem from a surprising connection between our model and communication complexity. Using this connection, we are also able to give an MA-protocol for the Inner Product function with O(sqrt(n) log n) communication (essentially matching a lower bound of Klauck).},
  isbn = {978-1-60558-047-0}
}

@misc{aaronson_WhyPhilosophersShould_2011,
  title = {Why {{Philosophers Should Care About Computational Complexity}}},
  author = {Aaronson, Scott},
  year = {2011},
  month = aug,
  number = {arXiv:1108.1791},
  eprint = {1108.1791},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1108.1791},
  url = {http://arxiv.org/abs/1108.1791},
  urldate = {2025-02-19},
  abstract = {One might think that, once we know something is computable, how efficiently it can be computed is a practical question with little further philosophical importance. In this essay, I offer a detailed case that one would be wrong. In particular, I argue that computational complexity theory -- the field that studies the resources (such as time, space, and randomness) needed to solve computational problems -- leads to new perspectives on the nature of mathematical knowledge, the strong AI debate, computationalism, the problem of logical omniscience, Hume's problem of induction, Goodman's grue riddle, the foundations of quantum mechanics, economic rationality, closed timelike curves, and several other topics of philosophical interest. I end by discussing aspects of complexity theory itself that could benefit from philosophical analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Quantum Physics},
  file = {/Users/christosdemetriou/Zotero/storage/LRPA8JTA/Aaronson - 2011 - Why Philosophers Should Care About Computational Complexity.pdf;/Users/christosdemetriou/Zotero/storage/BZ82AU7B/1108.html}
}

@book{arora_ComputationalComplexityModern_2009,
  title = {Computational {{Complexity}}: {{A Modern Approach}}},
  shorttitle = {Computational {{Complexity}}},
  author = {Arora, Sanjeev and Barak, Boaz},
  year = {2009},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511804090},
  url = {https://www.cambridge.org/core/books/computational-complexity/3453CAFDEB0B4820B186FE69A64E1086},
  urldate = {2025-02-16},
  abstract = {This beginning graduate textbook describes both recent achievements and classical results of computational complexity theory. Requiring essentially no background apart from mathematical maturity, the book can be used as a reference for self-study for anyone interested in complexity, including physicists, mathematicians, and other scientists, as well as a textbook for a variety of courses and seminars. More than 300 exercises are included with a selected hint set. The book starts with a broad introduction to the field and progresses to advanced results. Contents include: definition of Turing machines and basic time and space complexity classes, probabilistic algorithms, interactive proofs, cryptography, quantum computation, lower bounds for concrete computational models (decision trees, communication complexity, constant depth, algebraic and monotone circuits, proof complexity), average-case complexity and hardness amplification, derandomization and pseudorandom constructions, and the PCP theorem.},
  isbn = {978-0-521-42426-4},
  file = {/Users/christosdemetriou/Zotero/storage/7WJ96KSJ/3453CAFDEB0B4820B186FE69A64E1086.html}
}

@book{arteche_ProofComplexityCircuit_2024,
  title = {From Proof Complexity to Circuit Complexity via Interactive Protocols},
  author = {Arteche, N. and Khaniki, E. and Pich, J. and Santhanam, R.},
  year = {2024},
  volume = {297},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  issn = {1868-8969},
  url = {https://ora.ox.ac.uk/objects/uuid:d471a6fc-bed4-4f85-bf12-96abf752768c},
  urldate = {2025-02-19},
  abstract = {{$<$}p{$>$}Folklore in complexity theory suspects that circuit lower bounds against {$<$}strong{$>$}NC{$<$}/strong{$>$}\textsuperscript{1} or {$<$}strong{$>$}P{$<$}/strong{$>$}/poly, currently out of reach, are a necessary step towards proving strong proof complexity lower bounds for systems like Frege or Extended Frege. Establishing such a connection formally, however, is already daunting, as it would imply the breakthrough separation {$<$}strong{$>$}NEXP{$<$}/strong{$>$} {$\not\subseteq$} {$<$}strong{$>$}P{$<$}/strong{$>$}/poly, as recently observed by Pich and Santhanam [Pich and Santhanam, 2023].{$<$}/p{$>$} {$<$}br{$>$} {$<$}p{$>$}We show such a connection conditionally for the Implicit Extended Frege proof system (iEF) introduced by Kraj{\'i}{\v c}ek [Kraj{\'i}{\v c}ek, 2004], capable of formalizing most of contemporary complexity theory. In particular, we show that if iEF proves efficiently the standard derandomization assumption that a concrete Boolean function is hard on average for subexponential-size circuits, then any superpolynomial lower bound on the length of iEF proofs implies \#{$<$}strong{$>$}P{$<$}/strong{$>$} {$\not\subseteq$} {$<$}strong{$>$}FP{$<$}/strong{$>$}/poly (which would in turn imply, for example, {$<$}strong{$>$}PSPACE{$<$}/strong{$>$} {$\not\subseteq$} {$<$}strong{$>$}P{$<$}/strong{$>$}/poly). Our proof exploits the formalization inside iEF of the soundness of the sum-check protocol of Lund, Fortnow, Karloff, and Nisan [Lund et al., 1992]. This has consequences for the self-provability of circuit upper bounds in iEF. Interestingly, further improving our result seems to require progress in constructing interactive proof systems with more efficient provers.{$<$}/p{$>$}},
  isbn = {978-3-95977-322-5},
  langid = {english},
  file = {/Users/christosdemetriou/Zotero/storage/6YHF5QXS/Arteche et al. - 2024 - From proof complexity to circuit complexity via interactive protocols.pdf}
}

@article{baker_Relativizations$mathcalPMathcalNP$_1975,
  title = {Relativizations of the \${\textbackslash}mathcal\{\vphantom\}{{P}}\vphantom\{\} = ?{\textbackslash}mathcal\{\vphantom\}{{NP}}\vphantom\{\}\$ {{Question}}},
  shorttitle = {Relativizations of the \${\textbackslash}mathcal\{\vphantom\}{{P}}\vphantom\{\} = ?},
  author = {Baker, Theodore and Gill, John and Solovay, Robert},
  year = {1975},
  month = dec,
  journal = {SIAM Journal on Computing},
  volume = {4},
  number = {4},
  pages = {431--442},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/0204037},
  url = {https://epubs.siam.org/doi/10.1137/0204037},
  urldate = {2025-02-18},
  abstract = {Let A be a language chosen randomly by tossing a fair coin for each string x to determine whether x belongs to A. With probability 1, each of the relativized classes LOGSPACEA, PA, NPA, PPA, and PSPACEA is properly contained in the next. Also, NPA{$\neq$}co-NPA with probability 1. By contrast, with probability 1 the class PA coincides with the class BPPA of languages recognized by probabilistic oracle machines with error probability uniformly bounded below 12. NPA is shown, with probability 1, to contain a PA-immune set, i.e., a set having no infinite subset in PA. The relationship of PA-immunity to p-sparseness and NPA-completeness is briefly discussed: PA-immune sets in NPA can be sparse or moderately dense, but not co-sparse. Relativization with respect to a random length-preserving permutation {$\pi$}, instead of a random oracle A, yields analogous results and in addition the proper containment, with probability 1, of P{$\pi$} in NP{$\pi\cap$}co-NP{$\pi$}, which we have been unable to decide for a simple random oracle. Most of these results are shown by straightforward counting arguments, applied to oracle-dependent languages designed not to be recognizable without a large number of oracle calls. It is conjectured that all pA-invariant statements that are true with probability 1 of subrecursive language classes uniformly relativized to a random oracle are also true in the unrelativized case.}
}

@article{berger_ProteinFoldingHydrophobichydrophilic_1998,
  title = {Protein Folding in the Hydrophobic-Hydrophilic ({{HP}}) Model Is {{NP-complete}}},
  author = {Berger, B. and Leighton, T.},
  year = {1998},
  journal = {Journal of Computational Biology: A Journal of Computational Molecular Cell Biology},
  volume = {5},
  number = {1},
  pages = {27--40},
  issn = {1066-5277},
  doi = {10.1089/cmb.1998.5.27},
  abstract = {One of the simplest and most popular biophysical models of protein folding is the hydrophobic-hydrophilic (HP) model. The HP model abstracts the hydrophobic interaction in protein folding by labeling the amino acids as hydrophobic (H for nonpolar) or hydrophilic (P for polar). Chains of amino acids are configured as self-avoiding walks on the 3D cubic lattice, where an optimal conformation maximizes the number of adjacencies between H's. In this paper, the protein folding problem under the HP model on the cubic lattice is shown to be NP-complete. This means that the protein folding problem belongs to a large set of problems that are believed to be computationally intractable.},
  langid = {english},
  pmid = {9541869},
  keywords = {Algorithms,Amino Acid Sequence,Amino Acids,Models Chemical,Molecular Sequence Data,Protein Folding,Proteins}
}

@misc{bevyengine,
  title = {Bevy Engine},
  author = {Contributors, Bevy},
  year = {2023},
  month = mar,
  url = {https://github.com/bevyengine/bevy/releases/tag/v0.10.0},
  abstract = {A refreshingly simple data-driven game engine built in Rust},
  langid = {english}
}

@article{bronislaw_TheoremeFunctionsDensembles_1928,
  title = {Un Theoreme Sur Les Functions d'ensembles},
  author = {Bronis{\l}aw, Knaster},
  year = {1928},
  volume = {6},
  pages = {133--134}
}

@article{bund_SmallHazardFreeTransducers_2025,
  title = {Small {{Hazard-Free Transducers}}},
  author = {Bund, Johannes and Lenzen, Christoph and Medina, Moti},
  year = {2025},
  month = may,
  journal = {IEEE Transactions on Computers},
  volume = {74},
  number = {5},
  pages = {1549--1564},
  issn = {1557-9956},
  doi = {10.1109/TC.2025.3533096},
  url = {https://ieeexplore.ieee.org/abstract/document/10856331},
  urldate = {2025-07-07},
  abstract = {In digital circuits, hazardous input signals are a result of spurious operation of bistable elements. For example, the problem occurs in circuits with asynchronous inputs or clock domain crossings. Marino (TC'81) showed that hazards in bistable elements are inevitable. Hazard-free circuits compute the ``most stable'' output possible on hazardous inputs, under the constraint that it returns the same output as the circuit on stable inputs. Ikenmeyer et al. (JACM'19) proved an unconditional exponential separation between the hazard-free complexity and (standard) circuit complexity of explicit functions. Despite that, asymptotically optimal hazard-free sorting circuit are possible (Bund et al., TC'19). This raises the question: Which classes of functions permit efficient hazard-free circuits? We prove that circuit implementations of transducers with small state space are such a class. A transducer is a finite state machine that transcribes, symbol by symbol, an input string of length n into an output string of length n. We present a construction that transforms any function arising from a transducer into an efficient circuit that computes the hazard-free extension of the function. For transducers with constant state space, the circuit has asymptotically optimal size, with small constants if the state space is small.},
  keywords = {Boolean functions,Circuits,Computational Complexity,Computers,Encoding,Fault-Tolerant Circuits,Hazards,Logic,Logic gates,Parallel Prefix Architecture,Standards,Symbols,Transducers},
  file = {/Users/christosdemetriou/Zotero/storage/7LW7M45W/Bund et al. - 2025 - Small Hazard-Free Transducers.pdf;/Users/christosdemetriou/Zotero/storage/QGH7DZG9/10856331.html}
}

@inproceedings{carbonioliveira_HardnessMagnificationNatural_2018,
  title = {Hardness {{Magnification}} for {{Natural Problems}}},
  booktitle = {2018 {{IEEE}} 59th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  author = {Carboni Oliveira, Igor and Santhanam, Rahul},
  year = {2018},
  month = oct,
  pages = {65--76},
  issn = {2575-8454},
  doi = {10.1109/FOCS.2018.00016},
  url = {https://ieeexplore.ieee.org/abstract/document/8555094},
  urldate = {2025-02-19},
  abstract = {We show that for several natural problems of interest, complexity lower bounds that are barely non-trivial imply super-polynomial or even exponential lower bounds in strong computational models. We term this phenomenon "hardness magnification". Our examples of hardness magnification include: 1. Let MCSP be the decision problem whose YES instances are truth tables of functions with circuit complexity at most s(n). We show that if MCSP[2{\textasciicircum}{\textsurd}n] cannot be solved on average with zero error by formulas of linear (or even sub-linear) size, then NP does not have polynomial-size formulas. In contrast, Hirahara and Santhanam (2017) recently showed that MCSP[2{\textasciicircum}{\textsurd}n] cannot be solved in the worst case by formulas of nearly quadratic size. 2. If there is a c {$>$} 0 such that for each positive integer d there is an {$\varepsilon$} {$>$} 0 such that the problem of checking if an n-vertex graph in the adjacency matrix representation has a vertex cover of size (log n){\textasciicircum}c cannot be solved by depth-d AC{\textasciicircum}0 circuits of size m{\textasciicircum}1+{$\varepsilon$}, where m = {$\Theta$}(n{\textasciicircum}2), then NP does not have polynomial-size formulas. 3. Let ({$\alpha$}, {$\beta$})-MCSP[s] be the promise problem whose YES instances are truth tables of functions that are {$\alpha$}-approximable by a circuit of size s(n), and whose NO instances are truth tables of functions that are not {$\beta$}-approximable by a circuit of size s(n). We show that for arbitrary 1/2 {$<$} {$\beta$} {$<$} {$\alpha$} {$\leq$} 1, if ({$\alpha$}, {$\beta$})-MCSP[2{\textasciicircum}{\textsurd}n] cannot be solved by randomized algorithms with random access to the input running in sublinear time, then NP is not contained in BPP. 4. If for each probabilistic quasi-linear time machine M using poly-logarithmic many random bits that is claimed to solve Satisfiability, there is a deterministic polynomial-time machine that on infinitely many input lengths n either identifies a satisfiable instance of bit-length n on which M does not accept with high probability or an unsatisfiable instance of bit-length n on which M does not reject with high probability, then NEXP is not contained in BPP. 5. Given functions s, c N {$\rightarrow$} N where s {$>$} c, let MKtP[c, s] be the promise problem whose YES instances are strings of Kt complexity at most c(N) and NO instances are strings of Kt complexity greater than s(N). We show that if there is a {$\delta$} {$>$} 0 such that for each {$\varepsilon$} {$>$} 0, MKtP[N{\textasciicircum}{$\varepsilon$}, N{\textasciicircum}{$\varepsilon$} + 5 log(N)] requires Boolean circuits of size N{\textasciicircum}1+{$\delta$}, then EXP is not contained in SIZE (poly). For each of the cases of magnification above, we observe that standard hardness assumptions imply much stronger lower bounds for these problems than we require for magnification. We further explore magnification as an avenue to proving strong lower bounds, and argue that magnification circumvents the "natural proofs" barrier of Razborov and Rudich (1997). Examining some standard proof techniques, we find that they fall just short of proving lower bounds via magnification. As one of our main open problems, we ask whether there are other meta-mathematical barriers to proving lower bounds that rule out approaches combining magnification with known techniques.},
  keywords = {circuit complexity,Complexity theory,computational complexity,Computational modeling,Computer science,Cryptography,hardness magnification,Integrated circuit modeling,lower bounds,minimum circuit size problem,Probabilistic logic,satisfiability,Standards,time bounded Kolmogorov complexity,vertex cover},
  file = {/Users/christosdemetriou/Zotero/storage/JZJZVT2Z/Carboni Oliveira and Santhanam - 2018 - Hardness Magnification for Natural Problems.pdf;/Users/christosdemetriou/Zotero/storage/NT3EATXS/8555094.html}
}

@article{chan_ComputationalComplexityCounting_2024,
  title = {Computational Complexity of Counting Coincidences},
  author = {Chan, Swee Hong and Pak, Igor},
  year = {2024},
  month = nov,
  journal = {Theoretical Computer Science},
  volume = {1015},
  pages = {114776},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2024.114776},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397524003931},
  urldate = {2025-02-21},
  abstract = {Can you decide if there is a coincidence in the numbers counting two different combinatorial objects? For example, can you decide if two regions in R3 have the same number of domino tilings? There are two versions of the problem, with 2{\texttimes}1{\texttimes}1 and 2{\texttimes}2{\texttimes}1 boxes. We prove that in both cases the coincidence problem is not in the polynomial hierarchy unless the polynomial hierarchy collapses to a finite level. While the conclusions are the same, the proofs are notably different and generalize in different directions. We proceed to explore the coincidence problem for counting independent sets and matchings in graphs, matroid bases, order ideals and linear extensions in posets, permutation patterns, and the Kronecker coefficients. We also make a number of conjectures for counting other combinatorial objects such as plane triangulations, contingency tables, standard Young tableaux, reduced factorizations and the Littlewood--Richardson coefficients.},
  keywords = {Domino tiling,Graph matching,Kronecker coefficient,Linear extensions of posets,Matroid bases,Order ideals of posets,P-completeness,Permanent,Standard Young tableau},
  file = {/Users/christosdemetriou/Zotero/storage/CXHYKTAP/S0304397524003931.html}
}

@article{chen_Complexity2DDiscrete_2009,
  title = {On the Complexity of {{2D}} Discrete Fixed Point Problem},
  author = {Chen, Xi and Deng, Xiaotie},
  year = {2009},
  month = oct,
  journal = {Theoretical Computer Science},
  series = {Automata, {{Languages}} and {{Programming}} ({{ICALP}} 2006)},
  volume = {410},
  number = {44},
  pages = {4448--4456},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2009.07.052},
  url = {https://www.sciencedirect.com/science/article/pii/S030439750900499X},
  urldate = {2025-04-25},
  abstract = {We study a computational complexity version of the 2D Sperner problem, which states that any three coloring of vertices of a triangulated triangle, satisfying some boundary conditions, will have a trichromatic triangle. In introducing a complexity class PPAD, Papadimitriou [C.H. Papadimitriou, On graph-theoretic lemmata and complexity classes, in: Proceedings of the 31st Annual Symposium on Foundations of Computer Science, 1990, 794--801] proved that its 3D analogue is PPAD-complete about fifteen years ago. The complexity of 2D-SPERNER itself has remained open since then. We settle this open problem with a PPAD-completeness proof. The result also allows us to derive the computational complexity characterization of a discrete version of the 2D Brouwer fixed point problem, improving a recent result of Daskalakis, Goldberg and Papadimitriou [C. Daskalakis, P.W. Goldberg, C.H. Papadimitriou, The complexity of computing a Nash equilibrium, in: Proceedings of the 38th Annual ACM Symposium on Theory of Computing (STOC), 2006]. Those hardness results for the simplest version of those problems provide very useful tools to the study of other important problems in the PPAD class.},
  file = {/Users/christosdemetriou/Zotero/storage/PBYPLKYI/S030439750900499X.html}
}

@article{chen_SettlingComplexityComputing_2009,
  title = {Settling the Complexity of Computing Two-Player {{Nash}} Equilibria},
  author = {Chen, Xi and Deng, Xiaotie and Teng, Shang-Hua},
  year = {2009},
  month = may,
  journal = {J. ACM},
  volume = {56},
  number = {3},
  pages = {14:1--14:57},
  issn = {0004-5411},
  doi = {10.1145/1516512.1516516},
  url = {https://doi.org/10.1145/1516512.1516516},
  urldate = {2025-02-19},
  abstract = {We prove that Bimatrix, the problem of finding a Nash equilibrium in a two-player game, is complete for the complexity class PPAD (Polynomial Parity Argument, Directed version) introduced by Papadimitriou in 1991.Our result, building upon the work of Daskalakis et al. [2006a] on the complexity of four-player Nash equilibria, settles a long standing open problem in algorithmic game theory. It also serves as a starting point for a series of results concerning the complexity of two-player Nash equilibria. In particular, we prove the following theorems:---Bimatrix does not have a fully polynomial-time approximation scheme unless every problem in PPAD is solvable in polynomial time.---The smoothed complexity of the classic Lemke-Howson algorithm and, in fact, of any algorithm for Bimatrix is not polynomial unless every problem in PPAD is solvable in randomized polynomial time.Our results also have a complexity implication in mathematical economics:---Arrow-Debreu market equilibria are PPAD-hard to compute.},
  file = {/Users/christosdemetriou/Zotero/storage/IIYLBZD5/Chen et al. - 2009 - Settling the complexity of computing two-player Nash equilibria.pdf}
}

@inproceedings{cook_ComplexityTheoremprovingProcedures_1971,
  title = {The Complexity of Theorem-Proving Procedures},
  booktitle = {Proceedings of the Third Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Cook, Stephen A.},
  year = {1971},
  month = may,
  series = {{{STOC}} '71},
  pages = {151--158},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/800157.805047},
  url = {https://dl.acm.org/doi/10.1145/800157.805047},
  urldate = {2025-02-18},
  abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be ``reduced'' to the problem of determining whether a given propositional formula is a tautology. Here ``reduced'' means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
  isbn = {978-1-4503-7464-4},
  file = {/Users/christosdemetriou/Zotero/storage/TCSEGHBW/Cook - 1971 - The complexity of theorem-proving procedures.pdf}
}

@book{daskalakis_ComplexityComputingNash_2006,
  title = {The Complexity of Computing a {{Nash}} Equilibrium},
  author = {Daskalakis, Constantinos and Goldberg, Paul and Papadimitriou, Christos},
  year = {2006},
  month = jan,
  journal = {SIAM Journal on Computing},
  volume = {39},
  pages = {78},
  publisher = {Association for Computing Machinery},
  doi = {10.1145/1461928.1461951},
  abstract = {How long does it take until economic agents converge to an equilibrium? By studying the complexity of the problem of computing a mixed Nash equilibrium in a game, we provide evidence that there are games in which convergence to such an equilibrium takes prohibitively long. Traditionally, computational problems fall into two classes: those that have a polynomial-time algorithm and those that are NP-hard. However, the concept of NP-hardness cannot be applied to the rare problems where "every instance has a solution"---for example, in the case of games Nash's theorem asserts that every game has a mixed equilibrium (now known as the Nash equilibrium, in honor of that result). We show that finding a Nash equilibrium is complete for a class of problems called PPAD, containing several other known hard problems; all problems in PPAD share the same style of proof that every instance has a solution.},
  file = {/Users/christosdemetriou/Zotero/storage/MRG4SPFP/Daskalakis et al. - 2006 - The complexity of computing a Nash equilibrium.pdf}
}

@inproceedings{daskalakis_ComplexityConstrainedMinmax_2021,
  title = {The Complexity of Constrained Min-Max Optimization},
  booktitle = {Proceedings of the 53rd {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  year = {2021},
  month = jun,
  series = {{{STOC}} 2021},
  pages = {1466--1478},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3406325.3451125},
  url = {https://dl.acm.org/doi/10.1145/3406325.3451125},
  urldate = {2025-04-25},
  abstract = {Despite its important applications in Machine Learning, min-max optimization of objective functions that are nonconvex-nonconcave remains elusive. Not only are there no known first-order methods converging to even approximate local min-max equilibria (a.k.a.\&nbsp;approximate saddle points), but the computational complexity of identifying them is also poorly understood. In this paper, we provide a characterization of the computational complexity as well as of the limitations of first-order methods in this problem. Specifically, we show that in linearly constrained min-max optimization problems with nonconvex-nonconcave objectives an approximate local min-max equilibrium of large enough approximation is guaranteed to exist, but computing such a point is PPAD-complete. The same is true of computing an approximate fixed point of the (Projected) Gradient Descent/Ascent update dynamics, which is computationally equivalent to computing approximate local min-max equilibria. An important byproduct of our proof is to establish an unconditional hardness result in the\&nbsp;Nemirovsky-Yudin 1983 oracle optimization model, where we are given oracle access to the values of some function f : P {$\rightarrow$} [-1, 1] and its gradient ∇ f, where P {$\subseteq$} [0, 1]d is a known convex polytope. We show that any algorithm that uses such first-order oracle access to f and finds an {$\varepsilon$}-approximate local min-max equilibrium needs to make a number of oracle queries that is exponential in at least one of 1/{$\varepsilon$}, L, G, or d, where L and G are respectively the smoothness and Lipschitzness of f. This comes in sharp contrast to minimization problems, where finding approximate local minima in the same setting can be done with Projected Gradient Descent using O(L/{$\varepsilon$}) many queries. Our result is the first to show an exponential separation between these two fundamental optimization problems in the oracle model.},
  isbn = {978-1-4503-8053-9},
  file = {/Users/christosdemetriou/Zotero/storage/M88PTMKX/Daskalakis et al. - 2021 - The complexity of constrained min-max optimization.pdf}
}

@inproceedings{deligkas_ConstantInapproximabilityPPA_2022,
  title = {Constant Inapproximability for {{PPA}}},
  booktitle = {Proceedings of the 54th {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Deligkas, Argyrios and Fearnley, John and Hollender, Alexandros and Melissourgos, Themistoklis},
  year = {2022},
  month = jun,
  series = {{{STOC}} 2022},
  pages = {1010--1023},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3519935.3520079},
  url = {https://dl.acm.org/doi/10.1145/3519935.3520079},
  urldate = {2025-07-09},
  abstract = {In the {$\varepsilon$}-Consensus-Halving problem, we are given n probability measures v1, {\dots}, vn on the interval R = [0,1], and the goal is to partition R into two parts R+ and R- using at most n cuts, so that {\textbar}vi(R+) - vi(R-){\textbar} {$\leq$} {$\varepsilon$} for all i. This fundamental fair division problem was the first natural problem shown to be complete for the class PPA, and all subsequent PPA-completeness results for other natural problems have been obtained by reducing from it. We show that {$\varepsilon$}-Consensus-Halving is PPA-complete even when the parameter {$\varepsilon$} is a constant. In fact, we prove that this holds for any constant {$\varepsilon$} \&lt; 1/5. As a result, we obtain constant inapproximability results for all known natural PPA-complete problems, including Necklace-Splitting, the Discrete-Ham-Sandwich problem, two variants of the pizza sharing problem, and for finding fair independent sets in cycles and paths.},
  isbn = {978-1-4503-9264-8},
  file = {/Users/christosdemetriou/Zotero/storage/W6TZQSNU/Deligkas et al. - 2022 - Constant inapproximability for PPA.pdf}
}

@inproceedings{deligkas_PureCircuitStrongInapproximability_2022,
  title = {Pure-{{Circuit}}: {{Strong Inapproximability}} for {{PPAD}}},
  shorttitle = {Pure-{{Circuit}}},
  booktitle = {2022 {{IEEE}} 63rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  author = {Deligkas, Argyrios and Fearnley, John and Hollender, Alexandros and Melissourgos, Themistoklis},
  year = {2022},
  month = oct,
  pages = {159--170},
  issn = {2575-8454},
  doi = {10.1109/FOCS54457.2022.00022},
  url = {https://ieeexplore.ieee.org/abstract/document/9996749},
  urldate = {2025-02-16},
  abstract = {The current state-of-the-art methods for showing inapproximability in PPAD arise from the {$\varepsilon$}-Generalized-Circuit ({$\varepsilon$}-GCIRCUIT) problem. Rubinstein (2018) showed that there exists a small unknown constant {$\varepsilon$} for which {$\varepsilon$}-GCIRCUIT is PPAD-hard, and subsequent work has shown hardness results for other problems in PPAD by using {$\varepsilon$}-GCIRCUIT as an intermediate problem.We introduce PURE-CIRCUIT, a new intermediate problem for PPAD, which can be thought of as {$\varepsilon$}-GCIRCUIT pushed to the limit as {\textbackslash}varepsilon{\textbackslash}rightarrow 1, and we show that the problem is PPAD-complete. We then prove that {$\varepsilon$}-GCIRCUIT is PPAD-hard for all {\textbackslash}varepsilon {\l}t 0.1 by a reduction from PURE-CIRCUIT, and thus strengthen all prior work that has used GCIRCUIT as an intermediate problem from the existential-constant regime to the large-constant regime. We show that stronger inapproximability results can be derived by a direct reduction from PURE-CIRCUIT. In particular, we prove that finding an {$\varepsilon$}-well-supported Nash equilibrium in a polymatrix game is PPAD-hard for all {\textbackslash}varepsilon {\l}t 1/3, and that this result is tight for two-action games.},
  keywords = {approximation,Computer science,Games,generalized circuit,Nash equilibrium,polymatrix games,PPAD,TFNP},
  file = {/Users/christosdemetriou/Zotero/storage/VQYJ78YU/Deligkas et al. - 2022 - Pure-Circuit Strong Inapproximability for PPAD.pdf;/Users/christosdemetriou/Zotero/storage/5YHQMGWF/9996749.html}
}

@article{deligkas_PureCircuitTightInapproximability_2024,
  title = {Pure-{{Circuit}}: {{Tight Inapproximability}} for {{PPAD}}},
  shorttitle = {Pure-{{Circuit}}},
  author = {Deligkas, Argyrios and Fearnley, John and Hollender, Alexandros and Melissourgos, Themistoklis},
  year = {2024},
  month = oct,
  journal = {J. ACM},
  volume = {71},
  number = {5},
  pages = {31:1--31:48},
  issn = {0004-5411},
  doi = {10.1145/3678166},
  url = {https://dl.acm.org/doi/10.1145/3678166},
  urldate = {2025-02-16},
  abstract = {The current state-of-the-art methods for showing inapproximability in PPAD arise from the {$\varepsilon$}-Generalized-Circuit ({$\varepsilon$}-GCircuit) problem. Rubinstein (2018) showed that there exists a small unknown constant {$\varepsilon$} for which {$\varepsilon$}-GCircuit is PPAD-hard, and subsequent work has shown hardness results for other problems in PPAD by using {$\varepsilon$}-GCircuit as an intermediate problem.We introduce Pure-Circuit, a new intermediate problem for PPAD, which can be thought of as {$\varepsilon$}-GCircuit pushed to the limit as {$\varepsilon$} {$\rightarrow$} 1, and we show that the problem is PPAD-complete. We then prove that {$\varepsilon$}-GCircuit is PPAD-hard for all {$\varepsilon$} \&lt; 1/10 by a reduction from Pure-Circuit, and thus strengthen all prior work that has used GCircuit as an intermediate problem from the existential-constant regime to the large-constant regime.We show that stronger inapproximability results can be derived by reducing directly from Pure-Circuit. In particular, we prove tight inapproximability results for computing approximate Nash equilibria and approximate well-supported Nash equilibria in graphical games, for finding approximate well-supported Nash equilibria in polymatrix games, and for finding approximate equilibria in threshold games.},
  file = {/Users/christosdemetriou/Zotero/storage/D7GDB7U9/Deligkas et al. - 2024 - Pure-Circuit Tight Inapproximability for PPAD.pdf}
}

@article{eichelberger_HazardDetectionCombinational_1965,
  title = {Hazard {{Detection}} in {{Combinational}} and {{Sequential Switching Circuits}}},
  author = {Eichelberger, E. B.},
  year = {1965},
  month = mar,
  journal = {IBM Journal of Research and Development},
  volume = {9},
  number = {2},
  pages = {90--99},
  issn = {0018-8646},
  doi = {10.1147/rd.92.0090},
  url = {https://ieeexplore.ieee.org/abstract/document/5392161},
  urldate = {2025-07-05},
  abstract = {This paper is concerned with a unified approach to the detection of hazards in both combinational and sequential circuits through the use of ternary algebra. First, hazards in a combinational network resulting from the simultaneous changing of two or more inputs are discussed. A technique is described that will detect hazards resulting from both single- and multiple-input changes. The various types of hazards connected with gate-type sequential circuits are also discussed, and a general technique is described that will detect any type of hazard or race condition that could result in an incorrect terminal state. This technique could be easily implemented in a computer program which would be capable of detecting hazards in circuits containing hundreds of logic blocks.},
  file = {/Users/christosdemetriou/Zotero/storage/HEKIAHNS/5392161.html}
}

@misc{fearnley_CLSNewProblems_2017,
  title = {{{CLS}}: {{New Problems}} and {{Completeness}}},
  shorttitle = {{{CLS}}},
  author = {Fearnley, John and Gordon, Spencer and Mehta, Ruta and Savani, Rahul},
  year = {2017},
  month = apr,
  number = {arXiv:1702.06017},
  eprint = {1702.06017},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.06017},
  url = {http://arxiv.org/abs/1702.06017},
  urldate = {2025-03-08},
  abstract = {The complexity class CLS was introduced by Daskalakis and Papadimitriou with the goal of capturing the complexity of some well-known problems in PPAD\${\textasciitilde}{\textbackslash}cap{\textasciitilde}\$PLS that have resisted, in some cases for decades, attempts to put them in polynomial time. No complete problem was known for CLS, and in previous work, the problems ContractionMap, i.e., the problem of finding an approximate fixpoint of a contraction map, and PLCP, i.e., the problem of solving a P-matrix Linear Complementarity Problem, were identified as prime candidates. First, we present a new CLS-complete problem MetaMetricContractionMap, which is closely related to the ContractionMap. Second, we introduce EndOfPotentialLine, which captures aspects of PPAD and PLS directly via a monotonic directed path, and show that EndOfPotentialLine is in CLS via a two-way reduction to EndOfMeteredLine. The latter was defined to keep track of how far a vertex is on the PPAD path via a restricted potential function. Third, we reduce PLCP to EndOfPotentialLine, thus making EndOfPotentialLine and EndOfMeteredLine at least as likely to be hard for CLS as PLCP. This last result leverages the monotonic structure of Lemke paths for PLCP problems, making EndOfPotentialLine a likely candidate to capture the exact complexity of PLCP; we note that the structure of Lemke-Howson paths for finding a Nash equilibrium in a two-player game very directly motivated the definition of the complexity class PPAD, which eventually ended up capturing this problem's complexity exactly.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/christosdemetriou/Zotero/storage/THBBJPH7/Fearnley et al. - 2017 - CLS New Problems and Completeness.pdf;/Users/christosdemetriou/Zotero/storage/HQ5IVYM5/1702.html}
}

@inproceedings{fearnley_ComplexityGradientDescent_2021,
  title = {The Complexity of Gradient Descent: {{CLS}} = {{PPAD}} and {{PLS}}},
  shorttitle = {The Complexity of Gradient Descent},
  booktitle = {Proceedings of the 53rd {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Fearnley, John and Goldberg, Paul W. and Hollender, Alexandros and Savani, Rahul},
  year = {2021},
  month = jun,
  series = {{{STOC}} 2021},
  pages = {46--59},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3406325.3451052},
  url = {https://doi.org/10.1145/3406325.3451052},
  urldate = {2025-02-24},
  abstract = {We study search problems that can be solved by performing Gradient Descent on a bounded convex polytopal domain and show that this class is equal to the intersection of two well-known classes: PPAD and PLS. As our main underlying technical contribution, we show that computing a Karush-Kuhn-Tucker (KKT) point of a continuously differentiable function over the domain [0,1]2 is PPAD {$\cap$} PLS-complete. This is the first natural problem to be shown complete for this class. Our results also imply that the class CLS (Continuous Local Search) - which was defined by Daskalakis and Papadimitriou as a more ``natural'' counterpart to PPAD {$\cap$} PLS and contains many interesting problems - is itself equal to PPAD {$\cap$} PLS.},
  isbn = {978-1-4503-8053-9},
  file = {/Users/christosdemetriou/Zotero/storage/T2B38K7U/Fearnley et al. - 2021 - The complexity of gradient descent CLS = PPAD ∩ PLS.pdf}
}

@article{fearnley_FasterAlgorithmFinding_2022,
  title = {A {{Faster Algorithm}}~for {{Finding Tarski Fixed Points}}},
  author = {Fearnley, John and P{\'a}lv{\"o}lgyi, D{\"o}m{\"o}t{\"o}r and Savani, Rahul},
  year = {2022},
  month = oct,
  journal = {ACM Trans. Algorithms},
  volume = {18},
  number = {3},
  pages = {23:1--23:23},
  issn = {1549-6325},
  doi = {10.1145/3524044},
  url = {https://doi.org/10.1145/3524044},
  urldate = {2025-07-07},
  abstract = {Dang et\&nbsp;al. have given an algorithm that can find a Tarski fixed point in a k-dimensional lattice of width n using O(log k n) queries\&nbsp;[2]. Multiple authors have conjectured that this algorithm is optimal\&nbsp;[2, 7], and indeed this has been proven for two-dimensional instances\&nbsp;[7]. We show that these conjectures are false in dimension three or higher by giving an O(log2 n) query algorithm for the three-dimensional Tarski problem. We also give a new decomposition theorem for k-dimensional Tarski problems which, in combination with our new algorithm for three dimensions, gives an O(log2 {$\lceil$}k/3{$\rceil$} n) query algorithm for the k-dimensional problem.},
  file = {/Users/christosdemetriou/Zotero/storage/M34PN8WW/Fearnley et al. - 2022 - A Faster Algorithm for Finding Tarski Fixed Points.pdf}
}

@article{fearnley_UniqueEndPotential_2020,
  title = {Unique End of Potential Line},
  author = {Fearnley, John and Gordon, Spencer and Mehta, Ruta and Savani, Rahul},
  year = {2020},
  month = dec,
  journal = {Journal of Computer and System Sciences},
  volume = {114},
  pages = {1--35},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2020.05.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0022000020300520},
  urldate = {2025-07-04},
  abstract = {The complexity class CLS was proposed by Daskalakis and Papadimitriou in 2011 to understand   the complexity of important NP search problems that admit both path following and potential optimizing algorithms. Here we identify a subclass of CLS -- called UniqueEOPL -- that applies a more specific combinatorial principle that guarantees unique solutions. We show that UniqueEOPL contains several important problems such as the P-matrix Linear Complementarity Problem, finding fixed points of Contraction Maps, and solving Unique Sink Orientations (USOs). We identify a problem -- closely related to solving contraction maps and USOs -- that is complete for UniqueEOPL.},
  keywords = {Continuous local search,Contraction map,P-matrix Linear Complementarity Problem,TFNP,Total search problems,Unique sink orientation},
  file = {/Users/christosdemetriou/Zotero/storage/KZJ78DYD/Fearnley et al. - 2020 - Unique end of potential line.pdf;/Users/christosdemetriou/Zotero/storage/T3QYZMNB/S0022000020300520.html}
}

@article{fenner_GapdefinableCountingClasses_1994,
  title = {Gap-Definable Counting Classes},
  author = {Fenner, Stephen A. and Fortnow, Lance J. and Kurtz, Stuart A.},
  year = {1994},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {48},
  number = {1},
  pages = {116--148},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(05)80024-8},
  url = {https://www.sciencedirect.com/science/article/pii/S0022000005800248},
  urldate = {2025-02-24},
  abstract = {The function class \#P lacks an important closure property: it is not closed under subtraction. To remedy this problem, we introduce the function class GapP as a natural alternative to \#P. GapP is the closure of \#P under subtraction and has all the other useful closure properties of \#P as well. We show that most previously studied counting classes, including PP, C=P, and ModkP, are ``gap-definable,'' i.e., definable using the values of GapP functions alone. We show that there is a smallest gap-definable class, SPP, which is still large enough to contain Few. We also show that SPP consists of exactly those languages low for GapP, and thus SPP languages are low for any gap-definable class. These results unify and improve earlier disparate results of J. Cai and L. Hemachandra (Math. Systems Theory 23, No. 2 (1990), 95--106) and J. K{\"o}bler et al. (J. Comput. System Sci. 44, No. 2 (1992), 272--286). We show further that any countable collection of languages is contained in a unique minimum gap-definable class, which implies that the gap-definable classes form a lattice under inclusion. Subtraction seems necessary for this result, since nothing similar is known for the \#P-definable classes.},
  file = {/Users/christosdemetriou/Zotero/storage/7A3M6RVE/S0022000005800248.html}
}

@article{fortnow_StatusNPProblem_2009,
  title = {The Status of the {{P}} versus {{NP}} Problem},
  author = {Fortnow, Lance},
  year = {2009},
  month = sep,
  journal = {Commun. ACM},
  volume = {52},
  number = {9},
  pages = {78--86},
  issn = {0001-0782},
  doi = {10.1145/1562164.1562186},
  url = {https://dl.acm.org/doi/10.1145/1562164.1562186},
  urldate = {2025-02-18},
  abstract = {It's one of the fundamental mathematical problems of our time, and its importance grows with the rise of powerful computers.},
  file = {/Users/christosdemetriou/Zotero/storage/AEHCRVKS/Fortnow - 2009 - The status of the P versus NP problem.pdf}
}

@article{friedrichs_MetastabilityContainingCircuits_2018,
  title = {Metastability-{{Containing Circuits}}},
  author = {Friedrichs, Stephan and F{\"u}gger, Matthias and Lenzen, Christoph},
  year = {2018},
  month = aug,
  journal = {IEEE Transactions on Computers},
  volume = {67},
  number = {8},
  pages = {1167--1183},
  issn = {1557-9956},
  doi = {10.1109/TC.2018.2808185},
  url = {https://ieeexplore.ieee.org/document/8314764},
  urldate = {2025-07-07},
  abstract = {In digital circuits, metastability can cause deteriorated signals that neither are logical 0 nor logical 1, breaking the abstraction of Boolean logic. Synchronizers, the only traditional countermeasure, exponentially decrease the odds of maintained metastability overtime. We propose a fundamentally different approach: It is possible to deterministically contain metastability by fine-grained logical masking so that it cannot infect the entire circuit. At the heart of our approach lies a time- and value-discrete model for metastability in synchronous clocked digital circuits, in which metastability is propagated in a worst-case fashion. The proposed model permits positive results and passes the test of reproducing Marino's impossibility results. We fully classify which functions can be computed by circuits with standard registers. Regarding masking registers, we show that more functions become computable with each clock cycle, and that masking registers permit exponentially smaller circuits for some tasks. Demonstrating the applicability of our approach, we present the first fault-tolerant distributed clock synchronization algorithm that deterministically guarantees correct behavior in the presence of metastability. As a consequence, clock domains can be synchronized without using synchronizers, enabling metastability-free communication between them.},
  keywords = {clock synchronization,Clocks,Delays,Digital circuits,Integrated circuit modeling,Logic gates,logical masking,masking register,Metastability,metastability-containment,Registers,Synchronization},
  file = {/Users/christosdemetriou/Zotero/storage/HSI6V46B/Friedrichs et al. - 2018 - Metastability-Containing Circuits.pdf}
}

@misc{goldberg_SurveyPPADCompletenessComputing_2011,
  title = {A {{Survey}} of {{PPAD-Completeness}} for {{Computing Nash Equilibria}}},
  author = {Goldberg, Paul W.},
  year = {2011},
  month = mar,
  number = {arXiv:1103.2709},
  eprint = {1103.2709},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1103.2709},
  url = {http://arxiv.org/abs/1103.2709},
  urldate = {2025-02-19},
  abstract = {PPAD refers to a class of computational problems for which solutions are guaranteed to exist due to a specific combinatorial principle. The most well-known such problem is that of computing a Nash equilibrium of a game. Other examples include the search for market equilibria, and envy-free allocations in the context of cake-cutting. A problem is said to be complete for PPAD if it belongs to PPAD and can be shown to constitute one of the hardest computational challenges within that class. In this paper, I give a relatively informal overview of the proofs used in the PPAD-completeness results. The focus is on the mixed Nash equilibria guaranteed to exist by Nash's theorem. I also give an overview of some recent work that uses these ideas to show PSPACE-completeness for the computation of specific equilibria found by homotopy methods. I give a brief introduction to related problems of searching for market equilibria.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Computer Science and Game Theory},
  file = {/Users/christosdemetriou/Zotero/storage/EQUKJQ43/Goldberg - 2011 - A Survey of PPAD-Completeness for Computing Nash Equilibria.pdf;/Users/christosdemetriou/Zotero/storage/6YGQRYH3/1103.html}
}

@book{goldreich_ComputationalComplexityConceptual_2008,
  title = {Computational Complexity: A Conceptual Perspective},
  author = {Goldreich, O.},
  year = {2008},
  publisher = {Cambridge University Press},
  url = {https://books.google.co.uk/books?id=EuguvA-w5OEC},
  isbn = {978-1-139-47274-6}
}

@inproceedings{gonthier_FourColourTheorem_2008,
  title = {The {{Four Colour Theorem}}: {{Engineering}} of a {{Formal Proof}}},
  shorttitle = {The {{Four Colour Theorem}}},
  booktitle = {Computer {{Mathematics}}},
  author = {Gonthier, Georges},
  editor = {Kapur, Deepak},
  year = {2008},
  pages = {333--333},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-87827-8_28},
  abstract = {The 150 year old Four Colour Theorem is the first famous result with a proof that requires large computer calculations. Such proofs are still controversial: It is thought that computer programs cannot be reviewed with mathematical rigor.},
  isbn = {978-3-540-87827-8},
  langid = {english},
  file = {/Users/christosdemetriou/Zotero/storage/5LABFFTV/Gonthier - 2008 - The Four Colour Theorem Engineering of a Formal Proof.pdf}
}

@article{ikenmeyer_ComplexityHazardfreeCircuits_2019,
  title = {On the Complexity of Hazard-Free Circuits},
  author = {Ikenmeyer, Christian and Komarath, Balagopal and Lenzen, Christoph and Lysikov, Vladimir and Mokhov, Andrey and Sreenivasaiah, Karteek},
  year = {2019},
  month = aug,
  journal = {Journal of the ACM},
  volume = {66},
  number = {4},
  eprint = {1711.01904},
  primaryclass = {cs},
  pages = {1--20},
  issn = {0004-5411, 1557-735X},
  doi = {10.1145/3320123},
  url = {http://arxiv.org/abs/1711.01904},
  urldate = {2025-02-19},
  abstract = {The problem of constructing hazard-free Boolean circuits dates back to the 1940s and is an important problem in circuit design. Our main lower-bound result unconditionally shows the existence of functions whose circuit complexity is polynomially bounded while every hazard-free implementation is provably of exponential size. Previous lower bounds on the hazard-free complexity were only valid for depth 2 circuits. The same proof method yields that every subcubic implementation of Boolean matrix multiplication must have hazards. These results follow from a crucial structural insight: Hazard-free complexity is a natural generalization of monotone complexity to all (not necessarily monotone) Boolean functions. Thus, we can apply known monotone complexity lower bounds to find lower bounds on the hazard-free complexity. We also lift these methods from the monotone setting to prove exponential hazard-free complexity lower bounds for non-monotone functions. As our main upper-bound result we show how to efficiently convert a Boolean circuit into a bounded-bit hazard-free circuit with only a polynomially large blow-up in the number of gates. Previously, the best known method yielded exponentially large circuits in the worst case, so our algorithm gives an exponential improvement. As a side result we establish the NP-completeness of several hazard detection problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/christosdemetriou/Zotero/storage/CYNNG9Z5/Ikenmeyer et al. - 2019 - On the complexity of hazard-free circuits.pdf;/Users/christosdemetriou/Zotero/storage/Y9EUWXCD/1711.html}
}

@misc{ikenmeyer_KarchmerWigdersonGamesHazardfree_2022,
  title = {Karchmer-{{Wigderson Games}} for {{Hazard-free Computation}}},
  author = {Ikenmeyer, Christian and Komarath, Balagopal and Saurabh, Nitin},
  year = {2022},
  month = nov,
  number = {arXiv:2107.05128},
  eprint = {2107.05128},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.05128},
  url = {http://arxiv.org/abs/2107.05128},
  urldate = {2025-02-24},
  abstract = {We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games. Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion. For the multiplexer function we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth \$2\$ that has optimal depth. We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound. We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Discrete Mathematics},
  file = {/Users/christosdemetriou/Zotero/storage/B5SJKU3Y/Ikenmeyer et al. - 2022 - Karchmer-Wigderson Games for Hazard-free Computation.pdf;/Users/christosdemetriou/Zotero/storage/RWAJ4MPL/2107.html}
}

@article{ikenmeyer_PositivitySymmetricGroup_2024,
  title = {Positivity of the {{Symmetric Group Characters Is}} as {{Hard}} as the {{Polynomial Time Hierarchy}}},
  author = {Ikenmeyer, Christian and Pak, Igor and Panova, Greta},
  year = {2024},
  month = may,
  journal = {International Mathematics Research Notices},
  volume = {2024},
  number = {10},
  pages = {8442--8458},
  issn = {1073-7928},
  doi = {10.1093/imrn/rnad273},
  url = {https://doi.org/10.1093/imrn/rnad273},
  urldate = {2025-06-28},
  abstract = {We prove that deciding the vanishing of the character of the symmetric group is \${\textbackslash}textsf\{C\}\_= {\textbackslash}textsf\{P\}\$-complete. We use this hardness result to prove that the absolute value and also the square of the character are not contained in \${\textbackslash}textsf\{\#P\}\$, unless the polynomial hierarchy collapses to the second level. This rules out the existence of any (unsigned) combinatorial description for the square of the characters. As a byproduct of our proof, we conclude that deciding positivity of the character is \${\textbackslash}textsf\{PP\}\$-complete under many-one reductions, and hence \${\textbackslash}textsf\{PH\}\$-hard under Turing reductions.},
  file = {/Users/christosdemetriou/Zotero/storage/JK9QL6U4/Ikenmeyer et al. - 2024 - Positivity of the Symmetric Group Characters Is as Hard as the Polynomial Time Hierarchy.pdf;/Users/christosdemetriou/Zotero/storage/84633SR3/rnad273.html}
}

@article{ikenmeyer_VanishingKroneckerCoefficients_2017,
  title = {On Vanishing of {{Kronecker}} Coefficients},
  author = {Ikenmeyer, Christian and Mulmuley, Ketan D. and Walter, Michael},
  year = {2017},
  month = dec,
  journal = {computational complexity},
  volume = {26},
  number = {4},
  pages = {949--992},
  issn = {1420-8954},
  doi = {10.1007/s00037-017-0158-y},
  url = {https://doi.org/10.1007/s00037-017-0158-y},
  urldate = {2025-07-08},
  abstract = {We show that the problem of deciding positivity of Kronecker coefficients is NP-hard. Previously, this problem was conjectured to be in P, just as for the Littlewood--Richardson coefficients. Our result establishes in a formal way that Kronecker coefficients are more difficult than Littlewood--Richardson coefficients, unless P = NP.},
  langid = {english},
  keywords = {05E10,68Q17,Algorithm Analysis and Problem Complexity,Algorithmic Complexity,Computational Complexity,Computational Number Theory,Geometric complexity theory,K-Theory,Kronecker coefficients,Mathematics of Algorithmic Complexity,Moment polytope,NP-hard},
  file = {/Users/christosdemetriou/Zotero/storage/8E2CS2DF/Ikenmeyer et al. - 2017 - On vanishing of Kronecker coefficients.pdf}
}

@inproceedings{ikenmeyer_WhatWhatNot_2022,
  title = {What Is in \#{{P}} and What Is Not?},
  booktitle = {2022 {{IEEE}} 63rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  author = {Ikenmeyer, Christian and Pak, Igor},
  year = {2022},
  month = oct,
  pages = {860--871},
  issn = {2575-8454},
  doi = {10.1109/FOCS54457.2022.00087},
  url = {https://ieeexplore.ieee.org/abstract/document/9996676},
  urldate = {2025-04-24},
  abstract = {For several classical nonnegative integer functions we investigate if they are members of the counting complexity class \# P or not. We prove \# P membership in surprising cases, and in other cases we prove non-membership, relying on standard complexity assumptions or on oracle separations. We initiate the study of the polynomial closure properties of \# P on affine varieties, i.e., if all problem instances satisfy algebraic constraints. This is directly linked to classical combinatorial proofs of algebraic identities and inequalities. We investigate \# TFNP and obtain oracle separations that prove the strict inclusion of \# P in all standard syntactic subclasses of \# TFNP minus 1.},
  keywords = {combinatorial proofs,Complexity theory,Computer science,Counting complexity,GapP,P,Standards,Syntactics,TFNP},
  file = {/Users/christosdemetriou/Zotero/storage/4JZE8U33/Ikenmeyer and Pak - 2022 - What is in #P and what is not.pdf;/Users/christosdemetriou/Zotero/storage/Q4S4FNWL/9996676.html}
}

@misc{ji_MIPRE_2022,
  title = {{{MIP}}*={{RE}}},
  author = {Ji, Zhengfeng and Natarajan, Anand and Vidick, Thomas and Wright, John and Yuen, Henry},
  year = {2022},
  month = nov,
  number = {arXiv:2001.04383},
  eprint = {2001.04383},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.04383},
  url = {http://arxiv.org/abs/2001.04383},
  urldate = {2024-12-21},
  abstract = {We show that the class MIP* of languages that can be decided by a classical verifier interacting with multiple all-powerful quantum provers sharing entanglement is equal to the class RE of recursively enumerable languages. Our proof builds upon the quantum low-degree test of (Natarajan and Vidick, FOCS 2018) and the classical low-individual degree test of (Ji, et al., 2020) by integrating recent developments from (Natarajan and Wright, FOCS 2019) and combining them with the recursive compression framework of (Fitzsimons et al., STOC 2019). An immediate byproduct of our result is that there is an efficient reduction from the Halting Problem to the problem of deciding whether a two-player nonlocal game has entangled value \$1\$ or at most \$1/2\$. Using a known connection, undecidability of the entangled value implies a negative answer to Tsirelson's problem: we show, by providing an explicit example, that the closure \$C\_\{qa\}\$ of the set of quantum tensor product correlations is strictly included in the set \$C\_\{qc\}\$ of quantum commuting correlations. Following work of (Fritz, Rev. Math. Phys. 2012) and (Junge et al., J. Math. Phys. 2011) our results provide a refutation of Connes' embedding conjecture from the theory of von Neumann algebras.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Mathematics - Operator Algebras,Quantum Physics},
  file = {/Users/christosdemetriou/Zotero/storage/LXI76F24/Ji et al. - 2022 - MIP=RE.pdf;/Users/christosdemetriou/Zotero/storage/VBEK5L4J/2001.html}
}

@article{johnson_HowEasyLocal_1988,
  title = {How Easy Is Local Search?},
  author = {Johnson, David S. and Papadimitriou, Christos H. and Yannakakis, Mihalis},
  year = {1988},
  month = aug,
  journal = {Journal of Computer and System Sciences},
  volume = {37},
  number = {1},
  pages = {79--100},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(88)90046-3},
  url = {https://www.sciencedirect.com/science/article/pii/0022000088900463},
  urldate = {2025-07-07},
  abstract = {We investigate the complexity of finding locally optimal solutions to NP-hard combinatorial optimization problems. Local optimality arises in the context of local search algorithms, which try to find improved solutions by considering perturbations of the current solution (``neighbors'' of that solution). If no neighboring solution is better than the current solution, it is locally optimal. Finding locally optimal solutions is presumably easier than finding optimal solutions. Nevertheless, many popular local search algorithms are based on neighborhood structures for which locally optimal solutions are not known to be computable in polynomial time, either by using the local search algorithms themselves or by taking some indirect route. We define a natural class PLS consisting essentially of those local search problems for which local optimality can be verified in polynomial time, and show that there are complete problems for this class. In particular, finding a partition of a graph that is locally optimal with respect to the well-known Kernighan-Lin algorithm for graph partitioning is PLS-complete, and hence can be accomplished in polynomial time only if local optima can be found in polynomial time for all local search problems in PLS.},
  file = {/Users/christosdemetriou/Zotero/storage/Y2K6M3JG/Johnson et al. - 1988 - How easy is local search.pdf;/Users/christosdemetriou/Zotero/storage/4I79VN2K/0022000088900463.html}
}

@book{kleene_IntroductionMetamathematics_1950,
  title = {Introduction to {{Metamathematics}}},
  author = {Kleene, Stephen Cole},
  year = {1950},
  publisher = {P. Noordhoff N.V.},
  address = {Groningen},
  isbn = {978-0-923891-57-2 0-923891-57-9}
}

@book{kleene_IntroductionMetamathematics_2009,
  title = {Introduction to Metamathematics},
  author = {Kleene, S.C. and Beeson, M.},
  year = {2009},
  publisher = {Ishi Press International},
  url = {https://books.google.co.uk/books?id=HZAjPwAACAAJ},
  isbn = {978-0-923891-57-2}
}

@book{kozen_TheoryComputation_2006,
  title = {Theory of Computation},
  author = {Kozen, D.C.},
  year = {2006},
  series = {Texts in Computer Science},
  publisher = {Springer London},
  url = {https://books.google.co.uk/books?id=AolrsLBq3u0C},
  isbn = {978-1-84628-297-3},
  lccn = {2005937504}
}

@article{makar_AnalysisKroneckerProduct_1949,
  title = {On the {{Analysis}} of the {{Kronecker Product}} of {{Irreducible Representations}} of the {{Symmetric Group}}},
  author = {Makar, Ragy H.},
  year = {1949},
  month = dec,
  journal = {Proceedings of the Edinburgh Mathematical Society},
  volume = {8},
  number = {3},
  pages = {133--137},
  issn = {1464-3839, 0013-0915},
  doi = {10.1017/S0013091500002686},
  url = {https://www.cambridge.org/core/journals/proceedings-of-the-edinburgh-mathematical-society/article/on-the-analysis-of-the-kronecker-product-of-irreducible-representations-of-the-symmetric-group/FD03C910AC30747A67D32D4C304E07D2},
  urldate = {2025-07-08},
  abstract = {The Kronecker product of two irreducible matrix representations D({$\lambda$}), D({$\mu$}) of the symmetric group on n letters, furnishes a representation of that group, which is, in general reducible. The question of what irreducible representations will appear in the analysis of such products has been dealt with by Prof. F. D. Murnaghan. Indeed he has obtained the analysis of D(n - p, {$\lambda$}2, {\dots}) {\texttimes} D(n - q, {$\mu$}2, {\dots}), for the particular values, p = 1, q = 1, 2, 3, 4, 5; p = 2, q = 2, 3, 4; p = 3, q = 3, 4, applying a method which is a recurrence one, in the sense that to obtain such an analysis we have to look at some other analyses which come first in order.},
  langid = {english},
  file = {/Users/christosdemetriou/Zotero/storage/GC8QJ2GB/Makar - 1949 - On the Analysis of the Kronecker Product of Irreducible Representations of the Symmetric Group.pdf}
}

@article{marino_GeneralTheoryMetastable_1981,
  title = {General Theory of Metastable Operation},
  author = {Marino, Leonard R.},
  year = {1981},
  month = feb,
  journal = {IEEE Transactions on Computers},
  volume = {C-30},
  number = {2},
  pages = {107--115},
  issn = {1557-9956},
  doi = {10.1109/TC.1981.6312173},
  url = {https://ieeexplore.ieee.org/abstract/document/6312173},
  urldate = {2025-07-06},
  abstract = {Metastable operation is a fundamental phenomenon of sequential networks that process asynchronous inputs. Nevertheless, because of its subtle nature and the relatively low probability of its occurrence in conventional systems, this phenomenon is neither well understood nor widely appreciated. With continuing advances in digital technology, however, there is a growing interest in large-scale highly parallel systems. Such systems are likely to involve numerous high-frequency asynchronous interactions, which may result in frequent measures to prevent such failures. In recent years, a number of researchers have been working with some success to develop techniques for dealing with this failure mode. The purpose of this paper is to present a comprehensive theory of metastable operation that may lead to a better understanding of this phenomenon and provide theoretical support for further work in this area.},
  keywords = {Arbiter,asynchronous inputs,asynchronous sequential networks,Computer science,Inverters,Latches,metastable operation,reliability,Schmitt trigger,Standards,Switches,Synchronization,synchronizer,synchronous sequential networks,Trajectory}
}

@article{merkle_HidingInformationSignatures_1978,
  title = {Hiding Information and Signatures in Trapdoor Knapsacks},
  author = {Merkle, R. and Hellman, M.},
  year = {1978},
  month = sep,
  journal = {IEEE Transactions on Information Theory},
  volume = {24},
  number = {5},
  pages = {525--530},
  issn = {1557-9654},
  doi = {10.1109/TIT.1978.1055927},
  url = {https://ieeexplore.ieee.org/abstract/document/1055927},
  urldate = {2025-02-20},
  abstract = {The knapsack problem is an NP-complete combinatorial problem that is strongly believed to be computationally difficult to solve in general. Specific instances of this problem that appear very difficult to solve unless one possesses "trapdoor information" used in the design of the problem are demonstrated. Because only the designer can easily solve problems, others can send him information hidden in the solution to the problems without fear that an eavesdropper will be able to extract the information. This approach differs from usual cryptographic systems in that a secret key is not needed. Conversely, only the designer can generate signatures for messages, but anyone can easily check their authenticity.}
}

@article{mukaidono_BternaryLogicFunction_1972,
  title = {On the {{B-ternary}} Logic Function},
  author = {Mukaidono, Masao},
  year = {1972},
  month = jan,
  journal = {Trans. IECE},
  volume = {55},
  pages = {355--362}
}

@incollection{mulmuley_GeometricComplexityTheory_2003,
  title = {Geometric {{Complexity Theory}}, {{P}} vs. {{NP}} and {{Explicit Obstructions}}},
  booktitle = {Advances in {{Algebra}} and {{Geometry}}: {{University}} of {{Hyderabad Conference}} 2001},
  author = {Mulmuley, Ketan and Sohoni, Milind},
  year = {2003},
  month = jan,
  pages = {239--261},
  doi = {10.1007/978-93-86279-12-5_20},
  abstract = {Theory of computing has given rise to some fundamental mathematical problems, notably the P {$\neq$} NP conjecture, and the related lower bound problems concerning formula or circuit size. We develop an approach to these problems through geometric invariant theory. The goal of this approach is to reduce the hard nonexistence problems under consideration to tractable existence problems. Accordingly, we reduce the arithmetic (characteristic 0) version of the P {$\neq$} NP conjecture, and other related lower bound problems to proving existence of obstructions. These are representations in the homogeneous coordinate rings of orbit-closures in geometric invariant theory [MFK], of a class of points which are partially stable and whose stabilizers have special representation-theoretic properties. However, the Luna-Vust complexity [LV] of these orbit closures is quite high, in contrast with the well-understood homogeneous or almost-homogeneous-spaces, such as G/P [LLM], toric varieties [F3], and spherical embed-dings [BLV], whose Luna-Vust complexity is zero. We take a step towards explicit construction of obstructions by proving two results regarding these orbit closures. The first is a generalization of the Borel-Weil theorem for G/P to these orbit-closures. Second, we conjecture a nice representation-theoretic set of generators for their ideals, and prove a weaker version of the conjecture. Such a set of generators had earlier been given for the ideal of G/P by Lakshmibai, Seshadri, Littelmann [LS, Li3, LLM] and Kostant (cf. [PK]). Finally, using these results, we reduce, in essence, the arithmetic non-existence problems under consideration to fundmental existence and construction problems in representation theory and algebraic geometry that are conjectured to be in the complexity class P.},
  isbn = {978-81-85931-36-4}
}

@misc{mulmuley_GeometricComplexityTheory_2009,
  title = {Geometric {{Complexity Theory VI}}: The Flip via Saturated and Positive Integer Programming in Representation Theory and Algebraic Geometry},
  shorttitle = {Geometric {{Complexity Theory VI}}},
  author = {Mulmuley, Ketan D.},
  year = {2009},
  month = jan,
  number = {arXiv:0704.0229},
  eprint = {0704.0229},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.0704.0229},
  url = {http://arxiv.org/abs/0704.0229},
  urldate = {2025-02-18},
  abstract = {This article belongs to a series on geometric complexity theory (GCT), an approach to the P vs. NP and related problems through algebraic geometry and representation theory. The basic principle behind this approach is called the flip. In essence, it reduces the negative hypothesis in complexity theory (the lower bound problems), such as the P vs. NP problem in characteristic zero, to the positive hypothesis in complexity theory (the upper bound problems): specifically, to showing that the problems of deciding nonvanishing of the fundamental structural constants in representation theory and algebraic geometry, such as the well known plethysm constants--or rather certain relaxed forms of these decision probelms--belong to the complexity class P. In this article, we suggest a plan for implementing the flip, i.e., for showing that these relaxed decision problems belong to P. This is based on the reduction of the preceding complexity-theoretic positive hypotheses to mathematical positivity hypotheses: specifically, to showing that there exist positive formulae--i.e. formulae with nonnegative coefficients--for the structural constants under consideration and certain functions associated with them. These turn out be intimately related to the similar positivity properties of the Kazhdan-Lusztig polynomials and the multiplicative structural constants of the canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum groups. The known proofs of these positivity properties depend on the Riemann hypothesis over finite fields and the related results. Thus the reduction here, in conjunction with the flip, in essence, says that the validity of the P vs. NP conjecture in characteristic zero is intimately linked to the Riemann hypothesis over finite fields and related problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity},
  file = {/Users/christosdemetriou/Zotero/storage/B2QNPBF2/Mulmuley - 2009 - Geometric Complexity Theory VI the flip via saturated and positive integer programming in represent.pdf;/Users/christosdemetriou/Zotero/storage/7Q84HTIA/0704.html}
}

@article{mulmuley_VsNPGeometric_2011,
  title = {On {{P}} vs. {{NP}} and Geometric Complexity Theory: {{Dedicated}} to {{Sri Ramakrishna}}},
  shorttitle = {On {{P}} vs. {{NP}} and Geometric Complexity Theory},
  author = {Mulmuley, Ketan D.},
  year = {2011},
  month = apr,
  journal = {J. ACM},
  volume = {58},
  number = {2},
  pages = {5:1--5:26},
  issn = {0004-5411},
  doi = {10.1145/1944345.1944346},
  url = {https://dl.acm.org/doi/10.1145/1944345.1944346},
  urldate = {2025-02-19},
  abstract = {This article gives an overview of the geometric complexity theory (GCT) approach towards the P vs. NP and related problems focusing on its main complexity theoretic results. These are: (1) two concrete lower bounds, which are currently the best known lower bounds in the context of the P vs. NC and permanent vs. determinant problems, (2) the Flip Theorem, which formalizes the self-referential paradox in the P vs. NP problem, and (3) the Decomposition Theorem, which decomposes the arithmetic P vs. NP and permanent vs. determinant problems into subproblems without self-referential difficulty, consisting of positivity hypotheses in algebraic geometry and representation theory and easier hardness hypotheses.},
  file = {/Users/christosdemetriou/Zotero/storage/V4ZCVQJY/Mulmuley - 2011 - On P vs. NP and geometric complexity theory Dedicated to Sri Ramakrishna.pdf}
}

@misc{pak_WhatCombinatorialInterpretation_2022,
  title = {What Is a Combinatorial Interpretation?},
  author = {Pak, Igor},
  year = {2022},
  month = sep,
  number = {arXiv:2209.06142},
  eprint = {2209.06142},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.06142},
  url = {http://arxiv.org/abs/2209.06142},
  urldate = {2025-06-28},
  abstract = {In this survey we discuss the notion of combinatorial interpretation in the context of Algebraic Combinatorics and related areas. We approach the subject from the Computational Complexity perspective. We review many examples, state a workable definition, discuss many open problems, and present recent results on the subject.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  file = {/Users/christosdemetriou/Zotero/storage/B9YFV2Y3/Pak - 2022 - What is a combinatorial interpretation.pdf;/Users/christosdemetriou/Zotero/storage/HSY6TP39/2209.html}
}

@misc{pak_WhatCombinatorialInterpretation_2022a,
  title = {What Is a Combinatorial Interpretation?},
  author = {Pak, Igor},
  year = {2022},
  month = sep,
  number = {arXiv:2209.06142},
  eprint = {2209.06142},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.06142},
  url = {http://arxiv.org/abs/2209.06142},
  urldate = {2025-07-08},
  abstract = {In this survey we discuss the notion of combinatorial interpretation in the context of Algebraic Combinatorics and related areas. We approach the subject from the Computational Complexity perspective. We review many examples, state a workable definition, discuss many open problems, and present recent results on the subject.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Discrete Mathematics,Mathematics - Combinatorics},
  file = {/Users/christosdemetriou/Zotero/storage/JYW8QQ8Z/Pak - 2022 - What is a combinatorial interpretation.pdf;/Users/christosdemetriou/Zotero/storage/LPYU32NW/2209.html}
}

@article{papadimitriou_ComplexityParityArgument_1994,
  title = {On the Complexity of the Parity Argument and Other Inefficient Proofs of Existence},
  author = {Papadimitriou, Christos H.},
  year = {1994},
  month = jun,
  journal = {Journal of Computer and System Sciences},
  volume = {48},
  number = {3},
  pages = {498--532},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(05)80063-7},
  url = {https://www.sciencedirect.com/science/article/pii/S0022000005800637},
  urldate = {2025-02-16},
  abstract = {We define several new complexity classes of search problems, ``between'' the classes FP and FNP. These new classes are contained, along with factoring, and the class PLS, in the class TFNP of search problems in FNP that always have a witness. A problem in each of these new classes is defined in terms of an implicitly given, exponentially large graph. The existence of the solution sought is established via a simple graph-theoretic argument with an inefficiently constructive proof; for example, PLS can be thought of as corresponding to the lemma ``every dag has a sink.'' The new classes, are based on lemmata such as ``every graph has an even number of odd-degree nodes.'' They contain several important problems for which no polynomial time algorithm is presently known, including the computational versions of Sperner's lemma, Brouwer's fixpoint theorem, Ch{\'e}valley's theorem, and the Borsuk-Ulam theorem, the linear complementarity problem for P-matrices, finding a mixed equilibrium in a non-zero sum game, finding a second Hamilton circuit in a Hamiltonian cubic graph, a second Hamiltonian decomposition in a quartic graph, and others. Some of these problems are shown to be complete.},
  file = {/Users/christosdemetriou/Zotero/storage/UG2XTQVJ/Papadimitriou - 1994 - On the complexity of the parity argument and other inefficient proofs of existence.pdf;/Users/christosdemetriou/Zotero/storage/GWWQLLEZ/S0022000005800637.html}
}

@misc{pich_LocalizabilityApproximationMethod_2022,
  title = {Localizability of the Approximation Method},
  author = {Pich, Jan},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09285},
  eprint = {2212.09285},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09285},
  url = {http://arxiv.org/abs/2212.09285},
  urldate = {2025-02-19},
  abstract = {We use the approximation method of Razborov to analyze the locality barrier which arose from the investigation of the hardness magnification approach to complexity lower bounds. Adapting a limitation of the approximation method obtained by Razborov, we show that in many cases it is not possible to combine the approximation method with typical (localizable) hardness magnification theorems to derive strong circuit lower bounds. In particular, one cannot use the approximation method to derive an extremely strong constant-depth circuit lower bound and then magnify it to an \$NC{\textasciicircum}1\$ lower bound for an explicit function. To prove this we show that lower bounds obtained by the approximation method are in many cases localizable in the sense that they imply lower bounds for circuits which are allowed to use arbitrarily powerful oracles with small fan-in.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Discrete Mathematics},
  file = {/Users/christosdemetriou/Zotero/storage/NX24QECG/Pich - 2022 - Localizability of the approximation method.pdf;/Users/christosdemetriou/Zotero/storage/9G6KQWTH/2212.html}
}

@misc{pudlak_CanonicalPairsBounded_2019,
  title = {The Canonical Pairs of Bounded Depth {{Frege}} Systems},
  author = {Pudlak, Pavel},
  year = {2019},
  month = dec,
  number = {arXiv:1912.03013},
  eprint = {1912.03013},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.03013},
  url = {http://arxiv.org/abs/1912.03013},
  urldate = {2025-02-19},
  abstract = {The canonical pair of a proof system \$P\$ is the pair of disjoint NP sets where one set is the set of all satisfiable CNF formulas and the other is the set of CNF formulas that have \$P\$-proofs bounded by some polynomial. We give a combinatorial characterization of the canonical pairs of depth{\textasciitilde}\$d\$ Frege systems. Our characterization is based on certain games, introduced in this article, that are parametrized by a number{\textasciitilde}\$k\$, also called the depth. We show that the canonical pair of a depth{\textasciitilde}\$d\$ Frege system is polynomially equivalent to the pair \$(A\_\{d+2\},B\_\{d+2\})\$ where \$A\_\{d+2\}\$ (respectively, \$B\_\{d+1\}\$) are depth \{\$d+1\$\} games in which Player{\textasciitilde}I (Player II) has a positional winning strategy. Although this characterization is stated in terms of games, we will show that these combinatorial structures can be viewed as generalizations of monotone Boolean circuits. In particular, depth{\textasciitilde}1 games are essentially monotone Boolean circuits. Thus we get a generalization of the monotone feasible interpolation for Resolution, which is a property that enables one to reduce the task of proving lower bounds on the size of refutations to lower bounds on the size of monotone Boolean circuits. However, we do not have a method yet for proving lower bounds on the size of depth{\textasciitilde}\$d\$ games for \$d{$>$}1\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Mathematics - Logic},
  file = {/Users/christosdemetriou/Zotero/storage/WI6XXICC/Pudlak - 2019 - The canonical pairs of bounded depth Frege systems.pdf;/Users/christosdemetriou/Zotero/storage/UNQXMR98/1912.html}
}

@article{razborov_NaturalProofs_1997,
  title = {Natural {{Proofs}}},
  author = {Razborov, Alexander A and Rudich, Steven},
  year = {1997},
  month = aug,
  journal = {Journal of Computer and System Sciences},
  volume = {55},
  number = {1},
  pages = {24--35},
  issn = {0022-0000},
  doi = {10.1006/jcss.1997.1494},
  url = {https://www.sciencedirect.com/science/article/pii/S002200009791494X},
  urldate = {2025-02-18},
  abstract = {We introduce the notion ofnaturalproof. We argue that the known proofs of lower bounds on the complexity of explicit Boolean functions in nonmonotone models fall within our definition of natural. We show, based on a hardness assumption, that natural proofs can not prove superpolynomial lower bounds for general circuits. Without the hardness assumption, we are able to show that they can not prove exponential lower bounds (for general circuits) for the discrete logarithm problem. We show that the weaker class ofAC0-natural proofs which is sufficient to prove the parity lower bounds of Furst, Saxe, and Sipser, Yao, and H{\aa}stad is inherently incapable of proving the bounds of Razborov and Smolensky. We give some formal evidence that natural proofs are indeed natural by showing that every formal complexity measure, which can prove superpolynomial lower bounds for a single function, can do so for almost all functions, which is one of the two requirements of a natural proof in our sense.},
  file = {/Users/christosdemetriou/Zotero/storage/ZJMGP8UP/S002200009791494X.html}
}

@article{robertson_FourColourTheorem_1997,
  title = {The {{Four-Colour Theorem}}},
  author = {Robertson, Neil and Sanders, Daniel and Seymour, Paul and Thomas, Robin},
  year = {1997},
  month = may,
  journal = {Journal of Combinatorial Theory, Series B},
  volume = {70},
  number = {1},
  pages = {2--44},
  issn = {0095-8956},
  doi = {10.1006/jctb.1997.1750},
  url = {https://www.sciencedirect.com/science/article/pii/S0095895697917500},
  urldate = {2025-02-25},
  abstract = {The four-colour theorem, that every loopless planar graph admits a vertex-colouring with at most four different colours, was proved in 1976 by Appel and Haken, using a computer. Here we give another proof, still using a computer, but simpler than Appel and Haken's in several respects.},
  file = {/Users/christosdemetriou/Zotero/storage/PTW2KLDB/S0095895697917500.html}
}

@article{seiferas_TechniquesSeparatingSpace_1977,
  title = {Techniques for Separating Space Complexity Classes},
  author = {Seiferas, Joel I.},
  year = {1977},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {14},
  number = {1},
  pages = {73--99},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(77)80041-X},
  url = {https://www.sciencedirect.com/science/article/pii/S002200007780041X},
  urldate = {2025-02-24},
  abstract = {Diagonalization, cardinality, and recursive padding arguments are used to separate the Turing machine space complexity classes obtained by bounding space, number of worktape symbols, and number of worktape heads. Witness languages over a one-letter alphabet are constructed when possible.},
  file = {/Users/christosdemetriou/Zotero/storage/73TPZHBH/S002200007780041X.html}
}

@inproceedings{shary_KrawczykOperatorRevised_2004,
  title = {Krawczyk Operator Revised},
  author = {Shary, Sergey},
  year = {2004},
  month = jun
}

@inproceedings{toran_CombinatorialTechniqueSeparating_1989,
  title = {A Combinatorial Technique for Separating Counting Complexity Classes},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Tor{\'a}n, Jacobo},
  editor = {Ausiello, Giorgio and {Dezani-Ciancaglini}, Mariangiola and Della Rocca, Simonetta Ronchi},
  year = {1989},
  pages = {733--744},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0035795},
  abstract = {We introduce a new combinatorial technique to obtain relativized separations of certain complexity classes related to the idea of counting, like PP, G (exact counting), and {$\oplus$}P (parity). To demonstrate its usefulness we present three relativizations separating NP from G, NP from {$\oplus$}P and {$\oplus$}P from PP. Other separations follow from these results, and as a consequence we obtain an oracle separating PP from PSPACE, thus solving an open problem proposed by Angluin in [An,80]. From the relativized separations we obtain absolute separations for counting complexity classes with log-time bounded computation time.},
  isbn = {978-3-540-46201-9},
  langid = {english}
}

@inproceedings{toran_CombinatorialTechniqueSeparating_1989a,
  title = {A Combinatorial Technique for Separating Counting Complexity Classes},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Tor{\'a}n, Jacobo},
  editor = {Ausiello, Giorgio and {Dezani-Ciancaglini}, Mariangiola and Della Rocca, Simonetta Ronchi},
  year = {1989},
  pages = {733--744},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0035795},
  abstract = {We introduce a new combinatorial technique to obtain relativized separations of certain complexity classes related to the idea of counting, like PP, G (exact counting), and {$\oplus$}P (parity). To demonstrate its usefulness we present three relativizations separating NP from G, NP from {$\oplus$}P and {$\oplus$}P from PP. Other separations follow from these results, and as a consequence we obtain an oracle separating PP from PSPACE, thus solving an open problem proposed by Angluin in [An,80]. From the relativized separations we obtain absolute separations for counting complexity classes with log-time bounded computation time.},
  isbn = {978-3-540-46201-9},
  langid = {english}
}

@article{valiant_ComplexityComputingPermanent_1979,
  title = {The Complexity of Computing the Permanent},
  author = {Valiant, L. G.},
  year = {1979},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {8},
  number = {2},
  pages = {189--201},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(79)90044-6},
  url = {https://www.sciencedirect.com/science/article/pii/0304397579900446},
  urldate = {2025-02-18},
  abstract = {It is shown that the permanent function of (0, 1)-matrices is a complete problem for the class of counting problems associated with nondeterministic polynomial time computations. Related counting problems are also considered. The reductions used are characterized by their nontrivial use of arithmetic.},
  file = {/Users/christosdemetriou/Zotero/storage/UAUEJWET/0304397579900446.html}
}
