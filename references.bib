
@inproceedings{gonthier_four_2008,
	address = {Berlin, Heidelberg},
	title = {The {Four} {Colour} {Theorem}: {Engineering} of a {Formal} {Proof}},
	isbn = {978-3-540-87827-8},
	shorttitle = {The {Four} {Colour} {Theorem}},
	doi = {10.1007/978-3-540-87827-8_28},
	abstract = {The 150 year old Four Colour Theorem is the first famous result with a proof that requires large computer calculations. Such proofs are still controversial: It is thought that computer programs cannot be reviewed with mathematical rigor.},
	language = {en},
	booktitle = {Computer {Mathematics}},
	publisher = {Springer},
	author = {Gonthier, Georges},
	editor = {Kapur, Deepak},
	year = {2008},
	pages = {333--333},
}

@article{robertson_four-colour_1997,
	title = {The {Four}-{Colour} {Theorem}},
	volume = {70},
	issn = {0095-8956},
	url = {https://www.sciencedirect.com/science/article/pii/S0095895697917500},
	doi = {10.1006/jctb.1997.1750},
	abstract = {The four-colour theorem, that every loopless planar graph admits a vertex-colouring with at most four different colours, was proved in 1976 by Appel and Haken, using a computer. Here we give another proof, still using a computer, but simpler than Appel and Haken's in several respects.},
	number = {1},
	urldate = {2025-02-25},
	journal = {Journal of Combinatorial Theory, Series B},
	author = {Robertson, Neil and Sanders, Daniel and Seymour, Paul and Thomas, Robin},
	month = may,
	year = {1997},
	pages = {2--44},
}

@inproceedings{fearnley_complexity_2021,
	address = {New York, NY, USA},
	series = {{STOC} 2021},
	title = {The complexity of gradient descent: {CLS} = {PPAD} and {PLS}},
	isbn = {978-1-4503-8053-9},
	shorttitle = {The complexity of gradient descent},
	url = {https://doi.org/10.1145/3406325.3451052},
	doi = {10.1145/3406325.3451052},
	abstract = {We study search problems that can be solved by performing Gradient Descent on a bounded convex polytopal domain and show that this class is equal to the intersection of two well-known classes: PPAD and PLS. As our main underlying technical contribution, we show that computing a Karush-Kuhn-Tucker (KKT) point of a continuously differentiable function over the domain [0,1]2 is PPAD ∩ PLS-complete. This is the first natural problem to be shown complete for this class. Our results also imply that the class CLS (Continuous Local Search) - which was defined by Daskalakis and Papadimitriou as a more “natural” counterpart to PPAD ∩ PLS and contains many interesting problems - is itself equal to PPAD ∩ PLS.},
	urldate = {2025-02-24},
	booktitle = {Proceedings of the 53rd {Annual} {ACM} {SIGACT} {Symposium} on {Theory} of {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Fearnley, John and Goldberg, Paul W. and Hollender, Alexandros and Savani, Rahul},
	month = jun,
	year = {2021},
	pages = {46--59},
}

@misc{ikenmeyer_karchmer-wigderson_2022,
	title = {Karchmer-{Wigderson} {Games} for {Hazard}-free {Computation}},
	url = {http://arxiv.org/abs/2107.05128},
	doi = {10.48550/arXiv.2107.05128},
	abstract = {We present a Karchmer-Wigderson game to study the complexity of hazard-free formulas. This new game is both a generalization of the monotone Karchmer-Wigderson game and an analog of the classical Boolean Karchmer-Wigderson game. Therefore, it acts as a bridge between the existing monotone and general games. Using this game, we prove hazard-free formula size and depth lower bounds that are provably stronger than those possible by the standard technique of transferring results from monotone complexity in a black-box fashion. For the multiplexer function we give (1) a hazard-free formula of optimal size and (2) an improved low-depth hazard-free formula of almost optimal size and (3) a hazard-free formula with alternation depth \$2\$ that has optimal depth. We then use our optimal constructions to obtain an improved universal worst-case hazard-free formula size upper bound. We see our results as a significant step towards establishing hazard-free computation as an independent missing link between Boolean complexity and monotone complexity.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Ikenmeyer, Christian and Komarath, Balagopal and Saurabh, Nitin},
	month = nov,
	year = {2022},
	note = {arXiv:2107.05128 [cs]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Discrete Mathematics},
}

@article{seiferas_techniques_1977,
	title = {Techniques for separating space complexity classes},
	volume = {14},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200007780041X},
	doi = {10.1016/S0022-0000(77)80041-X},
	abstract = {Diagonalization, cardinality, and recursive padding arguments are used to separate the Turing machine space complexity classes obtained by bounding space, number of worktape symbols, and number of worktape heads. Witness languages over a one-letter alphabet are constructed when possible.},
	number = {1},
	urldate = {2025-02-24},
	journal = {Journal of Computer and System Sciences},
	author = {Seiferas, Joel I.},
	month = feb,
	year = {1977},
	pages = {73--99},
}

@article{fenner_gap-definable_1994,
	title = {Gap-definable counting classes},
	volume = {48},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000005800248},
	doi = {10.1016/S0022-0000(05)80024-8},
	abstract = {The function class \#P lacks an important closure property: it is not closed under subtraction. To remedy this problem, we introduce the function class GapP as a natural alternative to \#P. GapP is the closure of \#P under subtraction and has all the other useful closure properties of \#P as well. We show that most previously studied counting classes, including PP, C=P, and ModkP, are “gap-definable,” i.e., definable using the values of GapP functions alone. We show that there is a smallest gap-definable class, SPP, which is still large enough to contain Few. We also show that SPP consists of exactly those languages low for GapP, and thus SPP languages are low for any gap-definable class. These results unify and improve earlier disparate results of J. Cai and L. Hemachandra (Math. Systems Theory 23, No. 2 (1990), 95–106) and J. Köbler et al. (J. Comput. System Sci. 44, No. 2 (1992), 272–286). We show further that any countable collection of languages is contained in a unique minimum gap-definable class, which implies that the gap-definable classes form a lattice under inclusion. Subtraction seems necessary for this result, since nothing similar is known for the \#P-definable classes.},
	number = {1},
	urldate = {2025-02-24},
	journal = {Journal of Computer and System Sciences},
	author = {Fenner, Stephen A. and Fortnow, Lance J. and Kurtz, Stuart A.},
	month = feb,
	year = {1994},
	pages = {116--148},
}

@inproceedings{toran_combinatorial_1989,
	address = {Berlin, Heidelberg},
	title = {A combinatorial technique for separating counting complexity classes},
	isbn = {978-3-540-46201-9},
	doi = {10.1007/BFb0035795},
	abstract = {We introduce a new combinatorial technique to obtain relativized separations of certain complexity classes related to the idea of counting, like PP, G (exact counting), and ⊕P (parity). To demonstrate its usefulness we present three relativizations separating NP from G, NP from ⊕P and ⊕P from PP. Other separations follow from these results, and as a consequence we obtain an oracle separating PP from PSPACE, thus solving an open problem proposed by Angluin in [An,80]. From the relativized separations we obtain absolute separations for counting complexity classes with log-time bounded computation time.},
	language = {en},
	booktitle = {Automata, {Languages} and {Programming}},
	publisher = {Springer},
	author = {Torán, Jacobo},
	editor = {Ausiello, Giorgio and Dezani-Ciancaglini, Mariangiola and Della Rocca, Simonetta Ronchi},
	year = {1989},
	pages = {733--744},
}

@inproceedings{toran_combinatorial_1989-1,
	address = {Berlin, Heidelberg},
	title = {A combinatorial technique for separating counting complexity classes},
	isbn = {978-3-540-46201-9},
	doi = {10.1007/BFb0035795},
	abstract = {We introduce a new combinatorial technique to obtain relativized separations of certain complexity classes related to the idea of counting, like PP, G (exact counting), and ⊕P (parity). To demonstrate its usefulness we present three relativizations separating NP from G, NP from ⊕P and ⊕P from PP. Other separations follow from these results, and as a consequence we obtain an oracle separating PP from PSPACE, thus solving an open problem proposed by Angluin in [An,80]. From the relativized separations we obtain absolute separations for counting complexity classes with log-time bounded computation time.},
	language = {en},
	booktitle = {Automata, {Languages} and {Programming}},
	publisher = {Springer},
	author = {Torán, Jacobo},
	editor = {Ausiello, Giorgio and Dezani-Ciancaglini, Mariangiola and Della Rocca, Simonetta Ronchi},
	year = {1989},
	pages = {733--744},
}

@misc{noauthor_shell_nodate,
	title = {Shell {Integration} - {Features}},
	url = {https://ghostty.org},
	abstract = {Some Ghostty features require integrating with your shell. Ghostty
can automatically inject shell integration for bash, zsh, fish, and
elvish.},
	language = {en},
	urldate = {2025-02-24},
	journal = {Ghostty},
}

@book{goldreich2008computational,
	title = {Computational complexity: a conceptual perspective},
	isbn = {978-1-139-47274-6},
	url = {https://books.google.co.uk/books?id=EuguvA-w5OEC},
	publisher = {Cambridge University Press},
	author = {Goldreich, O.},
	year = {2008},
}

@article{chan_computational_2024,
	title = {Computational complexity of counting coincidences},
	volume = {1015},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/S0304397524003931},
	doi = {10.1016/j.tcs.2024.114776},
	abstract = {Can you decide if there is a coincidence in the numbers counting two different combinatorial objects? For example, can you decide if two regions in R3 have the same number of domino tilings? There are two versions of the problem, with 2×1×1 and 2×2×1 boxes. We prove that in both cases the coincidence problem is not in the polynomial hierarchy unless the polynomial hierarchy collapses to a finite level. While the conclusions are the same, the proofs are notably different and generalize in different directions. We proceed to explore the coincidence problem for counting independent sets and matchings in graphs, matroid bases, order ideals and linear extensions in posets, permutation patterns, and the Kronecker coefficients. We also make a number of conjectures for counting other combinatorial objects such as plane triangulations, contingency tables, standard Young tableaux, reduced factorizations and the Littlewood–Richardson coefficients.},
	urldate = {2025-02-21},
	journal = {Theoretical Computer Science},
	author = {Chan, Swee Hong and Pak, Igor},
	month = nov,
	year = {2024},
	keywords = {\#P-completeness, Domino tiling, Graph matching, Kronecker coefficient, Linear extensions of posets, Matroid bases, Order ideals of posets, Permanent, Standard Young tableau},
	pages = {114776},
}

@misc{bandyopadhyay_counting_2005,
	title = {Counting without sampling. {New} algorithms for enumeration problems using statistical physics},
	url = {http://arxiv.org/abs/math/0510471},
	doi = {10.48550/arXiv.math/0510471},
	abstract = {We propose a new type of approximate counting algorithms for the problems of enumerating the number of independent sets and proper colorings in low degree graphs with large girth. Our algorithms are not based on a commonly used Markov chain technique, but rather are inspired by developments in statistical physics in connection with correlation decay properties of Gibbs measures and its implications to uniqueness of Gibbs measures on infinite trees, reconstruction problems and local weak convergence methods. On a negative side, our algorithms provide \${\textbackslash}epsilon\$-approximations only to the logarithms of the size of a feasible set (also known as free energy in statistical physics). But on the positive side, our approach provides deterministic as opposed to probabilistic guarantee on approximations. Moreover, for some regular graphs we obtain explicit values for the counting problem. For example, we show that every 4-regular \$n\$-node graph with large girth has approximately \$(1.494...){\textasciicircum}n\$ independent sets, and in every \$r\$-regular graph with \$n\$ nodes and large girth the number of \$q{\textbackslash}geq r+1\$-proper colorings is approximately \$[q(1-\{1{\textbackslash}over q\}){\textasciicircum}\{r{\textbackslash}over 2\}]{\textasciicircum}n\$, for large \$n\$. In statistical physics terminology, we compute explicitly the limit of the log-partition function. We extend our results to random regular graphs. Our explicit results would be hard to derive via the Markov chain method.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Bandyopadhyay, Antar and Gamarnik, David},
	month = oct,
	year = {2005},
	note = {arXiv:math/0510471},
	keywords = {Mathematics - Probability},
}

@inproceedings{dyer_relative_2000,
	address = {Berlin, Heidelberg},
	title = {On the {Relative} {Complexity} of {Approximate} {Counting} {Problems}},
	isbn = {978-3-540-44436-7},
	doi = {10.1007/3-540-44436-X_12},
	abstract = {Two natural classes of counting problems that are interreducible under approximation-preserving reductions are: (i) those that admit a particular kind of efficient approximation algorithm known as an “FPRAS,” and (ii) those that are complete for \#P with respect to approximation-preserving reducibility. We describe and investigate not only these two classes but also a third class, of intermediate complexity, that is not known to be identical to (i) or (ii). The third class can be characterised as the hardest problems in a logically defined subclass of \#P.},
	language = {en},
	booktitle = {Approximation {Algorithms} for {Combinatorial} {Optimization}},
	publisher = {Springer},
	author = {Dyer, Martin and Goldberg, Leslie Ann and Greenhill, Catherine and Jerrum, Mark},
	editor = {Jansen, Klaus and Khuller, Samir},
	year = {2000},
	keywords = {Conjunctive Normal Form, Counting Problem, Relation Symbol, Satisfying Assignment, Turing Machine},
	pages = {108--119},
}

@inproceedings{valiant_holographic_2004,
	title = {Holographic algorithms},
	url = {https://ieeexplore.ieee.org/document/1366250},
	doi = {10.1109/FOCS.2004.34},
	abstract = {We introduce a new notion of efficient reduction among computational problems. Classical reductions involve gadgets that map local solutions of one problem to local solutions of another in one-to-one, or possibly many-to-one or one-to-many, fashion. Our proposed reductions allow for gadgets with many-to-many correspondences. Their objective is to preserve the sum of the local solutions. Such reductions provide a method of translating a combinatorial problem to a family of finite systems of polynomial equations with integer coefficients such that the number of solutions of the combinatorial problem can be counted in polynomial time if some system in the family has a solution over the complex numbers. We can derive polynomial time algorithms in this way for ten problems for which only exponential time algorithms were known before. General questions about complexity classes are also formulated. If the method is applied to a \#P-complete problem then we obtain families of polynomial systems such that the solvability of any one member would imply P/sup \#P/ = NC2.},
	urldate = {2025-02-20},
	booktitle = {45th {Annual} {IEEE} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Valiant, L.G.},
	month = oct,
	year = {2004},
	note = {ISSN: 0272-5428},
	keywords = {Contracts, Equations, Graph theory, Holography, Ice, Interference, National security, Physics, Polynomials, Research and development},
	pages = {306--315},
}

@article{toda_pp_1991,
	title = {{PP} is as {Hard} as the {Polynomial}-{Time} {Hierarchy}},
	volume = {20},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/10.1137/0220053},
	doi = {10.1137/0220053},
	abstract = {In this paper, it is shown that many natural counting classes, such as PP, \$C\_ =  P\$, and \$\{{\textbackslash}text\{MOD\}\}\_k \{{\textbackslash}text\{P\}\}\$, are at least as computationally hard as PH (the polynomial-time hierarchy) in the following sense: for each \$\{{\textbackslash}bf K\}\$ of the counting classes above, every set in \$\{{\textbackslash}bf K\}\$(PH) is polynomial-time randomized many-one reducible to a set in \$\{{\textbackslash}bf K\}\$ with two-sided exponentially small error probability. As a consequence of the result, it is seen that all the counting classes above are computationally harder than PH unless PH collapses to a finite level. Some other consequences are also shown.},
	number = {5},
	urldate = {2025-02-20},
	journal = {SIAM Journal on Computing},
	author = {Toda, Seinosuke},
	month = oct,
	year = {1991},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {865--877},
}

@article{merkle_hiding_1978,
	title = {Hiding information and signatures in trapdoor knapsacks},
	volume = {24},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/1055927},
	doi = {10.1109/TIT.1978.1055927},
	abstract = {The knapsack problem is an NP-complete combinatorial problem that is strongly believed to be computationally difficult to solve in general. Specific instances of this problem that appear very difficult to solve unless one possesses "trapdoor information" used in the design of the problem are demonstrated. Because only the designer can easily solve problems, others can send him information hidden in the solution to the problems without fear that an eavesdropper will be able to extract the information. This approach differs from usual cryptographic systems in that a secret key is not needed. Conversely, only the designer can generate signatures for messages, but anyone can easily check their authenticity.},
	number = {5},
	urldate = {2025-02-20},
	journal = {IEEE Transactions on Information Theory},
	author = {Merkle, R. and Hellman, M.},
	month = sep,
	year = {1978},
	note = {Conference Name: IEEE Transactions on Information Theory},
	pages = {525--530},
}

@article{berger_protein_1998,
	title = {Protein folding in the hydrophobic-hydrophilic ({HP}) model is {NP}-complete},
	volume = {5},
	issn = {1066-5277},
	doi = {10.1089/cmb.1998.5.27},
	abstract = {One of the simplest and most popular biophysical models of protein folding is the hydrophobic-hydrophilic (HP) model. The HP model abstracts the hydrophobic interaction in protein folding by labeling the amino acids as hydrophobic (H for nonpolar) or hydrophilic (P for polar). Chains of amino acids are configured as self-avoiding walks on the 3D cubic lattice, where an optimal conformation maximizes the number of adjacencies between H's. In this paper, the protein folding problem under the HP model on the cubic lattice is shown to be NP-complete. This means that the protein folding problem belongs to a large set of problems that are believed to be computationally intractable.},
	language = {eng},
	number = {1},
	journal = {Journal of Computational Biology: A Journal of Computational Molecular Cell Biology},
	author = {Berger, B. and Leighton, T.},
	year = {1998},
	pmid = {9541869},
	keywords = {Algorithms, Amino Acid Sequence, Amino Acids, Models, Chemical, Molecular Sequence Data, Protein Folding, Proteins},
	pages = {27--40},
}

@misc{pudlak_canonical_2019,
	title = {The canonical pairs of bounded depth {Frege} systems},
	url = {http://arxiv.org/abs/1912.03013},
	doi = {10.48550/arXiv.1912.03013},
	abstract = {The canonical pair of a proof system \$P\$ is the pair of disjoint NP sets where one set is the set of all satisfiable CNF formulas and the other is the set of CNF formulas that have \$P\$-proofs bounded by some polynomial. We give a combinatorial characterization of the canonical pairs of depth{\textasciitilde}\$d\$ Frege systems. Our characterization is based on certain games, introduced in this article, that are parametrized by a number{\textasciitilde}\$k\$, also called the depth. We show that the canonical pair of a depth{\textasciitilde}\$d\$ Frege system is polynomially equivalent to the pair \$(A\_\{d+2\},B\_\{d+2\})\$ where \$A\_\{d+2\}\$ (respectively, \$B\_\{d+1\}\$) are depth \{\$d+1\$\} games in which Player{\textasciitilde}I (Player II) has a positional winning strategy. Although this characterization is stated in terms of games, we will show that these combinatorial structures can be viewed as generalizations of monotone Boolean circuits. In particular, depth{\textasciitilde}1 games are essentially monotone Boolean circuits. Thus we get a generalization of the monotone feasible interpolation for Resolution, which is a property that enables one to reduce the task of proving lower bounds on the size of refutations to lower bounds on the size of monotone Boolean circuits. However, we do not have a method yet for proving lower bounds on the size of depth{\textasciitilde}\$d\$ games for \$d{\textgreater}1\$.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Pudlak, Pavel},
	month = dec,
	year = {2019},
	note = {arXiv:1912.03013 [math]},
	keywords = {Computer Science - Computational Complexity, Mathematics - Logic},
}

@inproceedings{carboni_oliveira_hardness_2018,
	title = {Hardness {Magnification} for {Natural} {Problems}},
	url = {https://ieeexplore.ieee.org/abstract/document/8555094},
	doi = {10.1109/FOCS.2018.00016},
	abstract = {We show that for several natural problems of interest, complexity lower bounds that are barely non-trivial imply super-polynomial or even exponential lower bounds in strong computational models. We term this phenomenon "hardness magnification". Our examples of hardness magnification include: 1. Let MCSP be the decision problem whose YES instances are truth tables of functions with circuit complexity at most s(n). We show that if MCSP[2{\textasciicircum}√n] cannot be solved on average with zero error by formulas of linear (or even sub-linear) size, then NP does not have polynomial-size formulas. In contrast, Hirahara and Santhanam (2017) recently showed that MCSP[2{\textasciicircum}√n] cannot be solved in the worst case by formulas of nearly quadratic size. 2. If there is a c {\textgreater} 0 such that for each positive integer d there is an ε {\textgreater} 0 such that the problem of checking if an n-vertex graph in the adjacency matrix representation has a vertex cover of size (log n){\textasciicircum}c cannot be solved by depth-d AC{\textasciicircum}0 circuits of size m{\textasciicircum}1+ε, where m = Θ(n{\textasciicircum}2), then NP does not have polynomial-size formulas. 3. Let (α, β)-MCSP[s] be the promise problem whose YES instances are truth tables of functions that are α-approximable by a circuit of size s(n), and whose NO instances are truth tables of functions that are not β-approximable by a circuit of size s(n). We show that for arbitrary 1/2 {\textless} β {\textless} α ≤ 1, if (α, β)-MCSP[2{\textasciicircum}√n] cannot be solved by randomized algorithms with random access to the input running in sublinear time, then NP is not contained in BPP. 4. If for each probabilistic quasi-linear time machine M using poly-logarithmic many random bits that is claimed to solve Satisfiability, there is a deterministic polynomial-time machine that on infinitely many input lengths n either identifies a satisfiable instance of bit-length n on which M does not accept with high probability or an unsatisfiable instance of bit-length n on which M does not reject with high probability, then NEXP is not contained in BPP. 5. Given functions s, c N → N where s {\textgreater} c, let MKtP[c, s] be the promise problem whose YES instances are strings of Kt complexity at most c(N) and NO instances are strings of Kt complexity greater than s(N). We show that if there is a δ {\textgreater} 0 such that for each ε {\textgreater} 0, MKtP[N{\textasciicircum}ε, N{\textasciicircum}ε + 5 log(N)] requires Boolean circuits of size N{\textasciicircum}1+δ, then EXP is not contained in SIZE (poly). For each of the cases of magnification above, we observe that standard hardness assumptions imply much stronger lower bounds for these problems than we require for magnification. We further explore magnification as an avenue to proving strong lower bounds, and argue that magnification circumvents the "natural proofs" barrier of Razborov and Rudich (1997). Examining some standard proof techniques, we find that they fall just short of proving lower bounds via magnification. As one of our main open problems, we ask whether there are other meta-mathematical barriers to proving lower bounds that rule out approaches combining magnification with known techniques.},
	urldate = {2025-02-19},
	booktitle = {2018 {IEEE} 59th {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS})},
	author = {Carboni Oliveira, Igor and Santhanam, Rahul},
	month = oct,
	year = {2018},
	note = {ISSN: 2575-8454},
	keywords = {Complexity theory, Computational modeling, Computer science, Cryptography, Integrated circuit modeling, Probabilistic logic, Standards, circuit complexity, computational complexity, hardness magnification, lower bounds, minimum circuit size problem, satisfiability, time bounded Kolmogorov complexity, vertex cover},
	pages = {65--76},
}

@misc{pich_localizability_2022,
	title = {Localizability of the approximation method},
	url = {http://arxiv.org/abs/2212.09285},
	doi = {10.48550/arXiv.2212.09285},
	abstract = {We use the approximation method of Razborov to analyze the locality barrier which arose from the investigation of the hardness magnification approach to complexity lower bounds. Adapting a limitation of the approximation method obtained by Razborov, we show that in many cases it is not possible to combine the approximation method with typical (localizable) hardness magnification theorems to derive strong circuit lower bounds. In particular, one cannot use the approximation method to derive an extremely strong constant-depth circuit lower bound and then magnify it to an \$NC{\textasciicircum}1\$ lower bound for an explicit function. To prove this we show that lower bounds obtained by the approximation method are in many cases localizable in the sense that they imply lower bounds for circuits which are allowed to use arbitrarily powerful oracles with small fan-in.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Pich, Jan},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09285 [cs]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Discrete Mathematics},
}

@book{arteche_proof_2024,
	title = {From proof complexity to circuit complexity via interactive protocols},
	volume = {297},
	isbn = {978-3-95977-322-5},
	url = {https://ora.ox.ac.uk/objects/uuid:d471a6fc-bed4-4f85-bf12-96abf752768c},
	abstract = {{\textless}p{\textgreater}Folklore in complexity theory suspects that circuit lower bounds against {\textless}strong{\textgreater}NC{\textless}/strong{\textgreater}$^{\textrm{1}}$ or {\textless}strong{\textgreater}P{\textless}/strong{\textgreater}/poly, currently out of reach, are a necessary step towards proving strong proof complexity lower bounds for systems like Frege or Extended Frege. Establishing such a connection formally, however, is already daunting, as it would imply the breakthrough separation {\textless}strong{\textgreater}NEXP{\textless}/strong{\textgreater} ⊈ {\textless}strong{\textgreater}P{\textless}/strong{\textgreater}/poly, as recently observed by Pich and Santhanam [Pich and Santhanam, 2023].{\textless}/p{\textgreater} {\textless}br{\textgreater} {\textless}p{\textgreater}We show such a connection conditionally for the Implicit Extended Frege proof system (iEF) introduced by Krajíček [Krajíček, 2004], capable of formalizing most of contemporary complexity theory. In particular, we show that if iEF proves efficiently the standard derandomization assumption that a concrete Boolean function is hard on average for subexponential-size circuits, then any superpolynomial lower bound on the length of iEF proofs implies \#{\textless}strong{\textgreater}P{\textless}/strong{\textgreater} ⊈ {\textless}strong{\textgreater}FP{\textless}/strong{\textgreater}/poly (which would in turn imply, for example, {\textless}strong{\textgreater}PSPACE{\textless}/strong{\textgreater} ⊈ {\textless}strong{\textgreater}P{\textless}/strong{\textgreater}/poly). Our proof exploits the formalization inside iEF of the soundness of the sum-check protocol of Lund, Fortnow, Karloff, and Nisan [Lund et al., 1992]. This has consequences for the self-provability of circuit upper bounds in iEF. Interestingly, further improving our result seems to require progress in constructing interactive proof systems with more efficient provers.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-19},
	publisher = {Schloss Dagstuhl – Leibniz-Zentrum für Informatik},
	author = {Arteche, N. and Khaniki, E. and Pich, J. and Santhanam, R.},
	year = {2024},
	note = {ISSN: 1868-8969},
}

@book{daskalakis_complexity_2006,
	title = {The complexity of computing a {Nash} equilibrium},
	volume = {39},
	abstract = {How long does it take until economic agents converge to an equilibrium? By studying the complexity of the problem of computing a mixed Nash equilibrium in a game, we provide evidence that there are games in which convergence to such an equilibrium takes prohibitively long. Traditionally, computational problems fall into two classes: those that have a polynomial-time algorithm and those that are NP-hard. However, the concept of NP-hardness cannot be applied to the rare problems where "every instance has a solution"---for example, in the case of games Nash's theorem asserts that every game has a mixed equilibrium (now known as the Nash equilibrium, in honor of that result). We show that finding a Nash equilibrium is complete for a class of problems called PPAD, containing several other known hard problems; all problems in PPAD share the same style of proof that every instance has a solution.},
	publisher = {Association for Computing Machinery},
	author = {Daskalakis, Constantinos and Goldberg, Paul and Papadimitriou, Christos},
	month = jan,
	year = {2006},
	doi = {10.1145/1461928.1461951},
	note = {Journal Abbreviation: SIAM Journal on Computing
Pages: 78
Publication Title: SIAM Journal on Computing},
}

@incollection{mulmuley_geometric_2003,
	title = {Geometric {Complexity} {Theory}, {P} vs. {NP} and {Explicit} {Obstructions}},
	isbn = {978-81-85931-36-4},
	abstract = {Theory of computing has given rise to some fundamental mathematical problems, notably the P ≠ NP conjecture, and the related lower bound problems concerning formula or circuit size. We develop an approach to these problems through geometric invariant theory. The goal of this approach is to reduce the hard nonexistence problems under consideration to tractable existence problems. Accordingly, we reduce the arithmetic (characteristic 0) version of the P ≠ NP conjecture, and other related lower bound problems to proving existence of obstructions. These are representations in the homogeneous coordinate rings of orbit-closures in geometric invariant theory [MFK], of a class of points which are partially stable and whose stabilizers have special representation-theoretic properties. However, the Luna-Vust complexity [LV] of these orbit closures is quite high, in contrast with the well-understood homogeneous or almost-homogeneous-spaces, such as G/P [LLM], toric varieties [F3], and spherical embed-dings [BLV], whose Luna-Vust complexity is zero. We take a step towards explicit construction of obstructions by proving two results regarding these orbit closures. The first is a generalization of the Borel-Weil theorem for G/P to these orbit-closures. Second, we conjecture a nice representation-theoretic set of generators for their ideals, and prove a weaker version of the conjecture. Such a set of generators had earlier been given for the ideal of G/P by Lakshmibai, Seshadri, Littelmann [LS, Li3, LLM] and Kostant (cf. [PK]). Finally, using these results, we reduce, in essence, the arithmetic non-existence problems under consideration to fundmental existence and construction problems in representation theory and algebraic geometry that are conjectured to be in the complexity class P.},
	booktitle = {Advances in {Algebra} and {Geometry}: {University} of {Hyderabad} {Conference} 2001},
	author = {Mulmuley, Ketan and Sohoni, Milind},
	month = jan,
	year = {2003},
	doi = {10.1007/978-93-86279-12-5_20},
	pages = {239--261},
}

@article{ikenmeyer_complexity_2019,
	title = {On the complexity of hazard-free circuits},
	volume = {66},
	issn = {0004-5411, 1557-735X},
	url = {http://arxiv.org/abs/1711.01904},
	doi = {10.1145/3320123},
	abstract = {The problem of constructing hazard-free Boolean circuits dates back to the 1940s and is an important problem in circuit design. Our main lower-bound result unconditionally shows the existence of functions whose circuit complexity is polynomially bounded while every hazard-free implementation is provably of exponential size. Previous lower bounds on the hazard-free complexity were only valid for depth 2 circuits. The same proof method yields that every subcubic implementation of Boolean matrix multiplication must have hazards. These results follow from a crucial structural insight: Hazard-free complexity is a natural generalization of monotone complexity to all (not necessarily monotone) Boolean functions. Thus, we can apply known monotone complexity lower bounds to find lower bounds on the hazard-free complexity. We also lift these methods from the monotone setting to prove exponential hazard-free complexity lower bounds for non-monotone functions. As our main upper-bound result we show how to efficiently convert a Boolean circuit into a bounded-bit hazard-free circuit with only a polynomially large blow-up in the number of gates. Previously, the best known method yielded exponentially large circuits in the worst case, so our algorithm gives an exponential improvement. As a side result we establish the NP-completeness of several hazard detection problems.},
	number = {4},
	urldate = {2025-02-19},
	journal = {Journal of the ACM},
	author = {Ikenmeyer, Christian and Komarath, Balagopal and Lenzen, Christoph and Lysikov, Vladimir and Mokhov, Andrey and Sreenivasaiah, Karteek},
	month = aug,
	year = {2019},
	note = {arXiv:1711.01904 [cs]},
	keywords = {Computer Science - Computational Complexity},
	pages = {1--20},
}

@article{chen_settling_2009,
	title = {Settling the complexity of computing two-player {Nash} equilibria},
	volume = {56},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/1516512.1516516},
	doi = {10.1145/1516512.1516516},
	abstract = {We prove that Bimatrix, the problem of finding a Nash equilibrium in a two-player game, is complete for the complexity class PPAD (Polynomial Parity Argument, Directed version) introduced by Papadimitriou in 1991.Our result, building upon the work of Daskalakis et al. [2006a] on the complexity of four-player Nash equilibria, settles a long standing open problem in algorithmic game theory. It also serves as a starting point for a series of results concerning the complexity of two-player Nash equilibria. In particular, we prove the following theorems:—Bimatrix does not have a fully polynomial-time approximation scheme unless every problem in PPAD is solvable in polynomial time.—The smoothed complexity of the classic Lemke-Howson algorithm and, in fact, of any algorithm for Bimatrix is not polynomial unless every problem in PPAD is solvable in randomized polynomial time.Our results also have a complexity implication in mathematical economics:—Arrow-Debreu market equilibria are PPAD-hard to compute.},
	number = {3},
	urldate = {2025-02-19},
	journal = {J. ACM},
	author = {Chen, Xi and Deng, Xiaotie and Teng, Shang-Hua},
	month = may,
	year = {2009},
	pages = {14:1--14:57},
}

@misc{aaronson_why_2011,
	title = {Why {Philosophers} {Should} {Care} {About} {Computational} {Complexity}},
	url = {http://arxiv.org/abs/1108.1791},
	doi = {10.48550/arXiv.1108.1791},
	abstract = {One might think that, once we know something is computable, how efficiently it can be computed is a practical question with little further philosophical importance. In this essay, I offer a detailed case that one would be wrong. In particular, I argue that computational complexity theory -- the field that studies the resources (such as time, space, and randomness) needed to solve computational problems -- leads to new perspectives on the nature of mathematical knowledge, the strong AI debate, computationalism, the problem of logical omniscience, Hume's problem of induction, Goodman's grue riddle, the foundations of quantum mechanics, economic rationality, closed timelike curves, and several other topics of philosophical interest. I end by discussing aspects of complexity theory itself that could benefit from philosophical analysis.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Aaronson, Scott},
	month = aug,
	year = {2011},
	note = {arXiv:1108.1791 [cs]},
	keywords = {Computer Science - Computational Complexity, Quantum Physics},
}

@misc{goldberg_survey_2011,
	title = {A {Survey} of {PPAD}-{Completeness} for {Computing} {Nash} {Equilibria}},
	url = {http://arxiv.org/abs/1103.2709},
	doi = {10.48550/arXiv.1103.2709},
	abstract = {PPAD refers to a class of computational problems for which solutions are guaranteed to exist due to a specific combinatorial principle. The most well-known such problem is that of computing a Nash equilibrium of a game. Other examples include the search for market equilibria, and envy-free allocations in the context of cake-cutting. A problem is said to be complete for PPAD if it belongs to PPAD and can be shown to constitute one of the hardest computational challenges within that class. In this paper, I give a relatively informal overview of the proofs used in the PPAD-completeness results. The focus is on the mixed Nash equilibria guaranteed to exist by Nash's theorem. I also give an overview of some recent work that uses these ideas to show PSPACE-completeness for the computation of specific equilibria found by homotopy methods. I give a brief introduction to related problems of searching for market equilibria.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Goldberg, Paul W.},
	month = mar,
	year = {2011},
	note = {arXiv:1103.2709 [cs]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Computer Science and Game Theory},
}

@article{mulmuley_p_2011,
	title = {On {P} vs. {NP} and geometric complexity theory: {Dedicated} to {Sri} {Ramakrishna}},
	volume = {58},
	issn = {0004-5411},
	shorttitle = {On {P} vs. {NP} and geometric complexity theory},
	url = {https://dl.acm.org/doi/10.1145/1944345.1944346},
	doi = {10.1145/1944345.1944346},
	abstract = {This article gives an overview of the geometric complexity theory (GCT) approach towards the P vs. NP and related problems focusing on its main complexity theoretic results. These are: (1) two concrete lower bounds, which are currently the best known lower bounds in the context of the P vs. NC and permanent vs. determinant problems, (2) the Flip Theorem, which formalizes the self-referential paradox in the P vs. NP problem, and (3) the Decomposition Theorem, which decomposes the arithmetic P vs. NP and permanent vs. determinant problems into subproblems without self-referential difficulty, consisting of positivity hypotheses in algebraic geometry and representation theory and easier hardness hypotheses.},
	number = {2},
	urldate = {2025-02-19},
	journal = {J. ACM},
	author = {Mulmuley, Ketan D.},
	month = apr,
	year = {2011},
	pages = {5:1--5:26},
}

@inproceedings{aaronson_algebrization_2008,
	address = {New York, NY, USA},
	series = {{STOC} '08},
	title = {Algebrization: a new barrier in complexity theory},
	isbn = {978-1-60558-047-0},
	shorttitle = {Algebrization},
	url = {https://doi.org/10.1145/1374376.1374481},
	doi = {10.1145/1374376.1374481},
	abstract = {Any proof of P!=NP will have to overcome two barriers: relativization and natural proofs. Yet over the last decade, we have seen circuit lower bounds (for example, that PP does not have linear-size circuits) that overcome both barriers simultaneously. So the question arises of whether there is a third barrier to progress on the central questions in complexity theory.In this paper we present such a barrier, which we call algebraic relativization or algebrization. The idea is that, when we relativize some complexity class inclusion, we should give the simulating machine access not only to an oracle A, but also to a low-degree extension of A over a finite field or ring.We systematically go through basic results and open problems in complexity theory to delineate the power of the new algebrization barrier. First, we show that all known non-relativizing results based on arithmetization -- both inclusions such as IP=PSPACE and MIP=NEXP, and separations such as MAEXP not in P/poly -- do indeed algebrize. Second, we show that almost all of the major open problems -- including P versus NP, P versus RP, and NEXP versus P/poly -- will require non-algebrizing techniques. In some cases algebrization seems to explain exactly why progress stopped where it did: for example, why we have superlinear circuit lower bounds for PromiseMA but not for NP.Our second set of results follows from lower bounds in a new model of algebraic query complexity, which we introduce in this paper and which is interesting in its own right. Some of our lower bounds use direct combinatorial and algebraic arguments, while others stem from a surprising connection between our model and communication complexity. Using this connection, we are also able to give an MA-protocol for the Inner Product function with O(sqrt(n) log n) communication (essentially matching a lower bound of Klauck).},
	urldate = {2025-02-18},
	booktitle = {Proceedings of the fortieth annual {ACM} symposium on {Theory} of computing},
	publisher = {Association for Computing Machinery},
	author = {Aaronson, Scott and Wigderson, Avi},
	month = may,
	year = {2008},
	pages = {731--740},
}

@article{baker_relativizations_1975,
	title = {Relativizations of the \${\textbackslash}mathcal\{{P}\} = ?{\textbackslash}mathcal\{{NP}\}\$ {Question}},
	volume = {4},
	issn = {0097-5397},
	shorttitle = {Relativizations of the \${\textbackslash}mathcal\{{P}\} = ?},
	url = {https://epubs.siam.org/doi/10.1137/0204037},
	doi = {10.1137/0204037},
	abstract = {Let A be a language chosen randomly by tossing a fair coin for each string x to determine whether x belongs to A. With probability 1, each of the relativized classes LOGSPACEA, PA, NPA, PPA, and PSPACEA is properly contained in the next. Also, NPA≠co-NPA with probability 1. By contrast, with probability 1 the class PA coincides with the class BPPA of languages recognized by probabilistic oracle machines with error probability uniformly bounded below 12. NPA is shown, with probability 1, to contain a PA-immune set, i.e., a set having no infinite subset in PA. The relationship of PA-immunity to p-sparseness and NPA-completeness is briefly discussed: PA-immune sets in NPA can be sparse or moderately dense, but not co-sparse. Relativization with respect to a random length-preserving permutation π, instead of a random oracle A, yields analogous results and in addition the proper containment, with probability 1, of Pπ in NPπ∩co-NPπ, which we have been unable to decide for a simple random oracle. Most of these results are shown by straightforward counting arguments, applied to oracle-dependent languages designed not to be recognizable without a large number of oracle calls. It is conjectured that all pA-invariant statements that are true with probability 1 of subrecursive language classes uniformly relativized to a random oracle are also true in the unrelativized case.},
	number = {4},
	urldate = {2025-02-18},
	journal = {SIAM Journal on Computing},
	author = {Baker, Theodore and Gill, John and Solovay, Robert},
	month = dec,
	year = {1975},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {431--442},
}

@article{razborov_natural_1997,
	title = {Natural {Proofs}},
	volume = {55},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791494X},
	doi = {10.1006/jcss.1997.1494},
	abstract = {We introduce the notion ofnaturalproof. We argue that the known proofs of lower bounds on the complexity of explicit Boolean functions in nonmonotone models fall within our definition of natural. We show, based on a hardness assumption, that natural proofs can not prove superpolynomial lower bounds for general circuits. Without the hardness assumption, we are able to show that they can not prove exponential lower bounds (for general circuits) for the discrete logarithm problem. We show that the weaker class ofAC0-natural proofs which is sufficient to prove the parity lower bounds of Furst, Saxe, and Sipser, Yao, and Håstad is inherently incapable of proving the bounds of Razborov and Smolensky. We give some formal evidence that natural proofs are indeed natural by showing that every formal complexity measure, which can prove superpolynomial lower bounds for a single function, can do so for almost all functions, which is one of the two requirements of a natural proof in our sense.},
	number = {1},
	urldate = {2025-02-18},
	journal = {Journal of Computer and System Sciences},
	author = {Razborov, Alexander A and Rudich, Steven},
	month = aug,
	year = {1997},
	pages = {24--35},
}

@inproceedings{cook_complexity_1971,
	address = {New York, NY, USA},
	series = {{STOC} '71},
	title = {The complexity of theorem-proving procedures},
	isbn = {978-1-4503-7464-4},
	url = {https://dl.acm.org/doi/10.1145/800157.805047},
	doi = {10.1145/800157.805047},
	abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
	urldate = {2025-02-18},
	booktitle = {Proceedings of the third annual {ACM} symposium on {Theory} of computing},
	publisher = {Association for Computing Machinery},
	author = {Cook, Stephen A.},
	month = may,
	year = {1971},
	pages = {151--158},
}

@article{fortnow_status_2009,
	title = {The status of the {P} versus {NP} problem},
	volume = {52},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/1562164.1562186},
	doi = {10.1145/1562164.1562186},
	abstract = {It's one of the fundamental mathematical problems of our time, and its importance grows with the rise of powerful computers.},
	number = {9},
	urldate = {2025-02-18},
	journal = {Commun. ACM},
	author = {Fortnow, Lance},
	month = sep,
	year = {2009},
	pages = {78--86},
}

@misc{mulmuley_geometric_2009,
	title = {Geometric {Complexity} {Theory} {VI}: the flip via saturated and positive integer programming in representation theory and algebraic geometry},
	shorttitle = {Geometric {Complexity} {Theory} {VI}},
	url = {http://arxiv.org/abs/0704.0229},
	doi = {10.48550/arXiv.0704.0229},
	abstract = {This article belongs to a series on geometric complexity theory (GCT), an approach to the P vs. NP and related problems through algebraic geometry and representation theory. The basic principle behind this approach is called the flip. In essence, it reduces the negative hypothesis in complexity theory (the lower bound problems), such as the P vs. NP problem in characteristic zero, to the positive hypothesis in complexity theory (the upper bound problems): specifically, to showing that the problems of deciding nonvanishing of the fundamental structural constants in representation theory and algebraic geometry, such as the well known plethysm constants--or rather certain relaxed forms of these decision probelms--belong to the complexity class P. In this article, we suggest a plan for implementing the flip, i.e., for showing that these relaxed decision problems belong to P. This is based on the reduction of the preceding complexity-theoretic positive hypotheses to mathematical positivity hypotheses: specifically, to showing that there exist positive formulae--i.e. formulae with nonnegative coefficients--for the structural constants under consideration and certain functions associated with them. These turn out be intimately related to the similar positivity properties of the Kazhdan-Lusztig polynomials and the multiplicative structural constants of the canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum groups. The known proofs of these positivity properties depend on the Riemann hypothesis over finite fields and the related results. Thus the reduction here, in conjunction with the flip, in essence, says that the validity of the P vs. NP conjecture in characteristic zero is intimately linked to the Riemann hypothesis over finite fields and related problems.},
	urldate = {2025-02-18},
	publisher = {arXiv},
	author = {Mulmuley, Ketan D.},
	month = jan,
	year = {2009},
	note = {arXiv:0704.0229 [cs]},
	keywords = {Computer Science - Computational Complexity},
}

@article{valiant_complexity_1979,
	title = {The complexity of computing the permanent},
	volume = {8},
	issn = {0304-3975},
	url = {https://www.sciencedirect.com/science/article/pii/0304397579900446},
	doi = {10.1016/0304-3975(79)90044-6},
	abstract = {It is shown that the permanent function of (0, 1)-matrices is a complete problem for the class of counting problems associated with nondeterministic polynomial time computations. Related counting problems are also considered. The reductions used are characterized by their nontrivial use of arithmetic.},
	number = {2},
	urldate = {2025-02-18},
	journal = {Theoretical Computer Science},
	author = {Valiant, L. G.},
	month = jan,
	year = {1979},
	pages = {189--201},
}

@misc{noauthor_gladstonego_nodate,
	title = {{GladstoneGo} {Login}},
	url = {https://sportwarwick.gladstonego.cloud/identity/Account/ResetPassword?linkid=71df95e5-16e9-419b-b338-3&},
	urldate = {2025-02-18},
}

@article{papadimitriou_complexity_1994,
	title = {On the complexity of the parity argument and other inefficient proofs of existence},
	volume = {48},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000005800637},
	doi = {10.1016/S0022-0000(05)80063-7},
	abstract = {We define several new complexity classes of search problems, “between” the classes FP and FNP. These new classes are contained, along with factoring, and the class PLS, in the class TFNP of search problems in FNP that always have a witness. A problem in each of these new classes is defined in terms of an implicitly given, exponentially large graph. The existence of the solution sought is established via a simple graph-theoretic argument with an inefficiently constructive proof; for example, PLS can be thought of as corresponding to the lemma “every dag has a sink.” The new classes, are based on lemmata such as “every graph has an even number of odd-degree nodes.” They contain several important problems for which no polynomial time algorithm is presently known, including the computational versions of Sperner's lemma, Brouwer's fixpoint theorem, Chévalley's theorem, and the Borsuk-Ulam theorem, the linear complementarity problem for P-matrices, finding a mixed equilibrium in a non-zero sum game, finding a second Hamilton circuit in a Hamiltonian cubic graph, a second Hamiltonian decomposition in a quartic graph, and others. Some of these problems are shown to be complete.},
	number = {3},
	urldate = {2025-02-16},
	journal = {Journal of Computer and System Sciences},
	author = {Papadimitriou, Christos H.},
	month = jun,
	year = {1994},
	pages = {498--532},
}

@book{arora_computational_2009,
	address = {Cambridge},
	title = {Computational {Complexity}: {A} {Modern} {Approach}},
	isbn = {978-0-521-42426-4},
	shorttitle = {Computational {Complexity}},
	url = {https://www.cambridge.org/core/books/computational-complexity/3453CAFDEB0B4820B186FE69A64E1086},
	abstract = {This beginning graduate textbook describes both recent achievements and classical results of computational complexity theory. Requiring essentially no background apart from mathematical maturity, the book can be used as a reference for self-study for anyone interested in complexity, including physicists, mathematicians, and other scientists, as well as a textbook for a variety of courses and seminars. More than 300 exercises are included with a selected hint set. The book starts with a broad introduction to the field and progresses to advanced results. Contents include: definition of Turing machines and basic time and space complexity classes, probabilistic algorithms, interactive proofs, cryptography, quantum computation, lower bounds for concrete computational models (decision trees, communication complexity, constant depth, algebraic and monotone circuits, proof complexity), average-case complexity and hardness amplification, derandomization and pseudorandom constructions, and the PCP theorem.},
	urldate = {2025-02-16},
	publisher = {Cambridge University Press},
	author = {Arora, Sanjeev and Barak, Boaz},
	year = {2009},
	doi = {10.1017/CBO9780511804090},
}

@inproceedings{deligkas_pure-circuit_2022,
	title = {Pure-{Circuit}: {Strong} {Inapproximability} for {PPAD}},
	shorttitle = {Pure-{Circuit}},
	url = {https://ieeexplore.ieee.org/abstract/document/9996749},
	doi = {10.1109/FOCS54457.2022.00022},
	abstract = {The current state-of-the-art methods for showing inapproximability in PPAD arise from the ε-Generalized-Circuit (ε-GCIRCUIT) problem. Rubinstein (2018) showed that there exists a small unknown constant ε for which ε-GCIRCUIT is PPAD-hard, and subsequent work has shown hardness results for other problems in PPAD by using ε-GCIRCUIT as an intermediate problem.We introduce PURE-CIRCUIT, a new intermediate problem for PPAD, which can be thought of as ε-GCIRCUIT pushed to the limit as {\textbackslash}varepsilon{\textbackslash}rightarrow 1, and we show that the problem is PPAD-complete. We then prove that ε-GCIRCUIT is PPAD-hard for all {\textbackslash}varepsilon łt 0.1 by a reduction from PURE-CIRCUIT, and thus strengthen all prior work that has used GCIRCUIT as an intermediate problem from the existential-constant regime to the large-constant regime. We show that stronger inapproximability results can be derived by a direct reduction from PURE-CIRCUIT. In particular, we prove that finding an ε-well-supported Nash equilibrium in a polymatrix game is PPAD-hard for all {\textbackslash}varepsilon łt 1/3, and that this result is tight for two-action games.},
	urldate = {2025-02-16},
	booktitle = {2022 {IEEE} 63rd {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS})},
	author = {Deligkas, Argyrios and Fearnley, John and Hollender, Alexandros and Melissourgos, Themistoklis},
	month = oct,
	year = {2022},
	note = {ISSN: 2575-8454},
	keywords = {Computer science, Games, Nash equilibrium, PPAD, TFNP, approximation, generalized circuit, polymatrix games},
	pages = {159--170},
}

@misc{ikenmeyer_what_2022,
	title = {What is in \#{P} and what is not?},
	url = {http://arxiv.org/abs/2204.13149},
	doi = {10.48550/arXiv.2204.13149},
	abstract = {For several classical nonnegative integer functions, we investigate if they are members of the counting complexity class \#P or not. We prove \#P membership in surprising cases, and in other cases we prove non-membership, relying on standard complexity assumptions or on oracle separations. We initiate the study of the polynomial closure properties of \#P on affine varieties, i.e., if all problem instances satisfy algebraic constraints. This is directly linked to classical combinatorial proofs of algebraic identities and inequalities. We investigate \#TFNP and obtain oracle separations that prove the strict inclusion of \#P in all standard syntactic subclasses of \#TFNP-1.},
	urldate = {2025-02-16},
	publisher = {arXiv},
	author = {Ikenmeyer, Christian and Pak, Igor},
	month = apr,
	year = {2022},
	note = {arXiv:2204.13149 [cs]},
	keywords = {Computer Science - Computational Complexity, Mathematics - Combinatorics},
}

@article{deligkas_pure-circuit_2024,
	title = {Pure-{Circuit}: {Tight} {Inapproximability} for {PPAD}},
	volume = {71},
	issn = {0004-5411},
	shorttitle = {Pure-{Circuit}},
	url = {https://dl.acm.org/doi/10.1145/3678166},
	doi = {10.1145/3678166},
	abstract = {The current state-of-the-art methods for showing inapproximability in PPAD arise from the ɛ-Generalized-Circuit (ɛ-GCircuit) problem. Rubinstein (2018) showed that there exists a small unknown constant ɛ for which ɛ-GCircuit is PPAD-hard, and subsequent work has shown hardness results for other problems in PPAD by using ɛ-GCircuit as an intermediate problem.We introduce Pure-Circuit, a new intermediate problem for PPAD, which can be thought of as ɛ-GCircuit pushed to the limit as ɛ → 1, and we show that the problem is PPAD-complete. We then prove that ɛ-GCircuit is PPAD-hard for all ɛ \&lt; 1/10 by a reduction from Pure-Circuit, and thus strengthen all prior work that has used GCircuit as an intermediate problem from the existential-constant regime to the large-constant regime.We show that stronger inapproximability results can be derived by reducing directly from Pure-Circuit. In particular, we prove tight inapproximability results for computing approximate Nash equilibria and approximate well-supported Nash equilibria in graphical games, for finding approximate well-supported Nash equilibria in polymatrix games, and for finding approximate equilibria in threshold games.},
	number = {5},
	urldate = {2025-02-16},
	journal = {J. ACM},
	author = {Deligkas, Argyrios and Fearnley, John and Hollender, Alexandros and Melissourgos, Themistoklis},
	month = oct,
	year = {2024},
	pages = {31:1--31:48},
}

@misc{schryen_speedup_2023,
	title = {Speedup and efficiency of computational parallelization: {A} unifying approach and asymptotic analysis},
	shorttitle = {Speedup and efficiency of computational parallelization},
	url = {http://arxiv.org/abs/2212.11223},
	doi = {10.48550/arXiv.2212.11223},
	abstract = {In high performance computing environments, we observe an ongoing increase in the available numbers of cores. This development calls for re-emphasizing performance (scalability) analysis and speedup laws as suggested in the literature (e.g., Amdahl's law and Gustafson's law), with a focus on asymptotic performance. Understanding speedup and efficiency issues of algorithmic parallelism is useful for several purposes, including the optimization of system operations, temporal predictions on the execution of a program, and the analysis of asymptotic properties and the determination of speedup bounds. However, the literature is fragmented and shows a large diversity and heterogeneity of speedup models and laws. These phenomena make it challenging to obtain an overview of the models and their relationships, to identify the determinants of performance in a given algorithmic and computational context, and, finally, to determine the applicability of performance models and laws to a particular parallel computing setting. In this work, we provide a generic speedup (and thus also efficiency) model for homogeneous computing environments. Our approach generalizes many prominent models suggested in the literature and allows showing that they can be considered special cases of a unifying approach. The genericity of the unifying speedup model is achieved through parameterization. Considering combinations of parameter ranges, we identify six different asymptotic speedup cases and eight different asymptotic efficiency cases. Jointly applying these speedup and efficiency cases, we derive eleven scalability cases, from which we build a scalability typology. Researchers can draw upon our typology to classify their speedup model and to determine the asymptotic behavior when the number of parallel processing units increases. In addition, our results may be used to address various extensions of our setting.},
	urldate = {2025-02-03},
	publisher = {arXiv},
	author = {Schryen, Guido},
	month = nov,
	year = {2023},
	note = {arXiv:2212.11223 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
}

@article{rodgers_improvements_1985,
	title = {Improvements in multiprocessor system design},
	volume = {13},
	issn = {0163-5964},
	url = {https://dl.acm.org/doi/10.1145/327070.327215},
	doi = {10.1145/327070.327215},
	number = {3},
	urldate = {2025-02-03},
	journal = {SIGARCH Comput. Archit. News},
	author = {Rodgers, David P.},
	month = jun,
	year = {1985},
	pages = {225--231},
}

@article{lorensen_marching_1987,
	title = {Marching cubes: {A} high resolution {3D} surface construction algorithm},
	volume = {21},
	issn = {0097-8930},
	shorttitle = {Marching cubes},
	url = {https://dl.acm.org/doi/10.1145/37402.37422},
	doi = {10.1145/37402.37422},
	abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
	number = {4},
	urldate = {2025-02-03},
	journal = {SIGGRAPH Comput. Graph.},
	author = {Lorensen, William E. and Cline, Harvey E.},
	month = aug,
	year = {1987},
	pages = {163--169},
}

@article{torrellas_false_1994,
	title = {False sharing and spatial locality in multiprocessor caches},
	volume = {43},
	issn = {1557-9956},
	url = {https://ieeexplore.ieee.org/abstract/document/286299},
	doi = {10.1109/12.286299},
	abstract = {The performance of the data cache in shared-memory multiprocessors has been shown to be different from that in uniprocessors. In particular, cache miss rates in multiprocessors do not show the sharp drop typical of uniprocessors when the size of the cache block increases. The resulting high cache miss rate is a cause of concern, since it can significantly limit the performance of multiprocessors. Some researchers have speculated that this effect is due to false sharing, the coherence transactions that result when different processors update different words of the same cache block in an interleaved fashion. While the analysis of six applications in the paper confirms that false sharing has a significant impact on the miss rate, the measurements also show that poor spatial locality among accesses to shared data has an even larger impact. To mitigate false sharing and to enhance spatial locality, we optimize the layout of shared data in cache blocks in a programmer-transparent manner. We show that this approach can reduce the number of misses on shared data by about 10\% on average.{\textless}{\textgreater}},
	number = {6},
	urldate = {2025-02-03},
	journal = {IEEE Transactions on Computers},
	author = {Torrellas, J. and Lam, H.S. and Hennessy, J.L.},
	month = jun,
	year = {1994},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Cache memory, Cache storage, Computer science education, Delay, Hardware, Interleaved codes, Large-scale systems, Optimizing compilers, Pensions, Programming profession},
	pages = {651--663},
}

@misc{l_2023,
	title = {Harnessing the power of {SIMD}: a spotlight on compiler limitations and memory alignment},
	url = {https://medium.com/@elinneon/harnessing-the-power-of-simd-a-spotlight-on-compiler-limitations-and-memory-alignment-51c60dfdde2e},
	author = {L, Ethan},
	month = oct,
	year = {2023},
}

@incollection{barry_chapter_2012,
	address = {Boston},
	title = {Chapter 11 - {Digital} {Signal} {Processing} {Using} {General}-{Purpose} {Processors}},
	isbn = {978-0-12-391490-3},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123914903000114},
	urldate = {2025-02-03},
	booktitle = {Modern {Embedded} {Computing}},
	publisher = {Morgan Kaufmann},
	author = {Barry, Peter and Crowley, Patrick},
	editor = {Barry, Peter and Crowley, Patrick},
	month = jan,
	year = {2012},
	doi = {10.1016/B978-0-12-391490-3.00011-4},
	pages = {317--346},
}

@misc{noauthor_best-practices_nodate,
	title = {Best-practices {\textbar} {Collapsing} {OpenMP} parallel loops},
	url = {https://co-design.pop-coe.eu/best-practices/openmp-nested-for-loops.html},
	urldate = {2025-02-03},
}

@misc{noauthor_art_nodate,
	title = {The {Art} of {HPC}},
	url = {https://theartofhpc.com/},
	urldate = {2025-02-03},
}

@article{blelloch1990prefix,
	title = {Prefix sums and their applications},
	author = {Blelloch, Guy E},
	year = {1990},
	note = {Publisher: School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA},
}

@phdthesis{beckingsale_towards_2015,
	title = {Towards {Scalable} {Adaptive} {Mesh} {Refinement} on {Future} {Parallel} {Architectures}},
	author = {Beckingsale, David},
	month = jan,
	year = {2015},
}

@inproceedings{conway_multiprocessor_1963,
	address = {New York, NY, USA},
	series = {{AFIPS} '63 ({Fall})},
	title = {A multiprocessor system design},
	isbn = {978-1-4503-7883-3},
	url = {https://dl.acm.org/doi/10.1145/1463822.1463838},
	doi = {10.1145/1463822.1463838},
	abstract = {Parallel processing is not so mysterious a concept as the dearth of algorithms which explicitly use it might suggest. As a rule of thumb, if N processes are performed and the outcome is independent of the order in which their steps are executed, provided that within each process the order of steps is preserved, then any or all of the processes can be performed simultaneously, if conflicts arising from multiple access to common storage can be resolved. All the elements of a matrix sum may be evaluated in parallel. The ith summand of all elements of a matrix product may be computed simultaneously. In an internal merge sort all strings in any pass may be created at the same time. All the coroutines of a separable program may be run concurrently.},
	urldate = {2025-02-03},
	booktitle = {Proceedings of the {November} 12-14, 1963, fall joint computer conference},
	publisher = {Association for Computing Machinery},
	author = {Conway, Melvin E.},
	month = nov,
	year = {1963},
	pages = {139--146},
}

@misc{noauthor_valgrind_nodate,
	title = {Valgrind},
	url = {https://valgrind.org/docs/manual/cl-manual.html},
	urldate = {2025-02-03},
}

@book{grossmann2007numerical,
	title = {Numerical treatment of partial differential equations},
	publisher = {Springer},
	author = {Grossmann, Christian},
	year = {2007},
}

@book{cannon_one-dimensional_1984,
	title = {The {One}-{Dimensional} {Heat} {Equation}},
	isbn = {978-0-521-30243-2},
	abstract = {This is a version of Gevrey's classical treatise on the heat equations. Included in this volume are discussions of initial and/or boundary value problems, numerical methods, free boundary problems and parameter determination problems. The material is presented as a monograph and/or information source book. After the first six chapters of standard classical material, each chapter is written as a self-contained unit except for an occasional reference to elementary definitions, theorems and lemmas in previous chapters.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Cannon, John Rozier},
	month = dec,
	year = {1984},
	note = {Google-Books-ID: XWSnBZxbz2oC},
	keywords = {Mathematics / Differential Equations / General, Mathematics / Differential Equations / Partial, Mathematics / General, Mathematics / Reference, Science / Mechanics / Thermodynamics},
}

@misc{noauthor_sign_nodate,
	title = {Sign in to {OneNote}},
	url = {https://www.onenote.com/hrd},
	urldate = {2025-01-21},
}

@inproceedings{takabi_brain_2016,
	title = {Brain {Computer} {Interface} ({BCI}) {Applications}: {Privacy} {Threats} and {Countermeasures}},
	shorttitle = {Brain {Computer} {Interface} ({BCI}) {Applications}},
	url = {https://ieeexplore.ieee.org/abstract/document/7809697},
	doi = {10.1109/CIC.2016.026},
	abstract = {In recent years, Brain-Computer Interfaces (BCIs) have gained popularity in non-medical domains such as the gaming, entertainment, personal health, and marketing industries. A growing number of companies offer various inexpensive consumer grade BCIs and some of these companies have recently introduced the concept of BCI "App stores" in order to facilitate the expansion of BCI applications and provide software development kits (SDKs) for other developers to create new applications for their devices. The BCI applications access to users' unique brainwave signals, which consequently allows them to make inferences about users' thoughts and mental processes. Since there are no specific standards that govern the development of BCI applications, its users are at the risk of privacy breaches. In this work, we perform first comprehensive analysis of BCI App stores including software development kits (SDKs), application programming interfaces (APIs), and BCI applications w.r.t privacy issues. The goal is to understand the way brainwave signals are handled by BCI applications and what threats to the privacy of users exist. Our findings show that most applications have unrestricted access to users' brainwave signals and can easily extract private information about their users without them even noticing. We discuss potential privacy threats posed by current practices used in BCI App stores and then describe some countermeasures that could be used to mitigate the privacy threats.},
	urldate = {2025-01-18},
	booktitle = {2016 {IEEE} 2nd {International} {Conference} on {Collaboration} and {Internet} {Computing} ({CIC})},
	author = {Takabi, Hassan and Bhalotiya, Anuj and Alohaly, Manar},
	month = nov,
	year = {2016},
	keywords = {Androids, Brain computer interface (BCI), Brainwave Signal, Companies, Electroencephalography, Electroencephalography (EEG), Headphones, Humanoid robots, Mobile Application, Privacy, Servers},
	pages = {102--111},
}

@incollection{silvers_fatal_2022,
	title = {A {Fatal} {Attraction} to {Normalizing}: {Treating} {Disabilities} as {Deviations} from “{Species}-{Typical}” {Functioning}},
	isbn = {978-1-00-328948-7},
	shorttitle = {A {Fatal} {Attraction} to {Normalizing}},
	abstract = {This chapter identifies and theorizes the sin of synecdoche: that of making a value judgment about a person on the basis of a single aspect of their identity. Asch and Wasserman are interested in how synecdoche factors into decisions to terminate pregnancies after the fetus is identified as having or being likely to develop some kind of disability. They argue that the sin of synecdoche involves a moral failing.},
	booktitle = {The {Disability} {Bioethics} {Reader}},
	publisher = {Routledge},
	author = {Silvers, Anita},
	year = {2022},
	note = {Num Pages: 13},
}

@misc{noauthor_robert_2001,
	title = {Robert {Byndom} v. {State} of {Arkansas}},
	url = {https://case-law.vlex.com/vid/byndom-v-state-00-890466866},
	abstract = {Robert Byndom v. State of Arkansas},
	language = {en},
	urldate = {2025-01-18},
	month = apr,
	year = {2001},
}

@misc{noauthor_byndom_nodate,
	title = {{BYNDOM} v. {STATE} (2001)},
	url = {https://caselaw.findlaw.com/court/ar-supreme-court/1337866.html},
	abstract = {Case opinion for AR Supreme Court BYNDOM v. STATE. Read the Court's full decision on FindLaw.},
	language = {en-US},
	urldate = {2025-01-18},
	journal = {Findlaw},
}

@misc{noauthor_byndom_nodate-1,
	title = {Byndom v {State}},
	url = {https://case-law.vlex.com/vid/byndom-v-state-00-890466866},
	abstract = {0: [object Object]. 1: [object Object]. 2: [object Object]. 3: [object Object]. 4: [object Object]},
	language = {en},
	urldate = {2025-01-18},
	journal = {vLex},
}

@article{dobkin_braincomputer_2007,
	title = {Brain–computer interface technology as a tool to augment plasticity and outcomes for neurological rehabilitation},
	volume = {579},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2151380/},
	doi = {10.1113/jphysiol.2006.123067},
	abstract = {Brain–computer interfaces (BCIs) are a rehabilitation tool for tetraplegic patients that aim to improve quality of life by augmenting communication, control of the environment, and self-care. The neurobiology of both rehabilitation and BCI control depends upon learning to modify the efficacy of spared neural ensembles that represent movement, sensation and cognition through progressive practice with feedback and reward. To serve patients, BCI systems must become safe, reliable, cosmetically acceptable, quickly mastered with minimal ongoing technical support, and highly accurate even in the face of mental distractions and the uncontrolled environment beyond a laboratory. BCI technologies may raise ethical concerns if their availability affects the decisions of patients who become locked-in with brain stem stroke or amyotrophic lateral sclerosis to be sustained with ventilator support. If BCI technology becomes flexible and affordable, volitional control of cortical signals could be employed for the rehabilitation of motor and cognitive impairments in hemiplegic or paraplegic patients by offering on-line feedback about cortical activity associated with mental practice, motor intention, and other neural recruitment strategies during progressive task-oriented practice. Clinical trials with measures of quality of life will be necessary to demonstrate the value of near-term and future BCI applications.},
	number = {Pt 3},
	urldate = {2025-01-18},
	journal = {The Journal of Physiology},
	author = {Dobkin, Bruce H},
	month = mar,
	year = {2007},
	pmid = {17095557},
	pmcid = {PMC2151380},
	pages = {637--642},
}

@article{soekadar_brain-machine_2014,
	title = {Brain-{Machine} {Interfaces} {In} {Neurorehabilitation} of {Stroke}.},
	volume = {83},
	doi = {10.1016/j.nbd.2014.11.025},
	abstract = {Stroke is among the leading causes of long-term disabilities leaving an increasing number of people with cognitive, affective and motor impairments depending on assistance in their daily life. While function after stroke can significantly improve in the first weeks and months, further recovery is often slow or non-existent in the more severe cases encompassing 30-50\% of all stroke victims. The neurobiological mechanisms underlying recovery in those patients are incompletely understood. However, recent studies demonstrated the brain's remarkable capacity for functional and structural plasticity and recovery even in severe chronic stroke. As all established rehabilitation strategies require some remaining motor function, there is currently no standardized and accepted treatment for patients with complete chronic muscle paralysis. The development of brain-machine interfaces (BMIs) that translate brain activity into control signals of computers or external devices provides two new strategies to overcome stroke-related motor paralysis. First, BMIs can establish continuous high-dimensional brain-control of robotic devices or functional electric stimulation (FES) to assist in daily life activities (assistive BMI). Second, BMIs could facilitate neuroplasticity, thus enhancing motor learning and motor recovery (rehabilitative BMI). Advances in sensor technology, development of non-invasive and implantable wireless BMI-systems and their combination with brain stimulation, along with evidence for BMI system's clinical efficacy suggest that BMI-related strategies will play an increasing role in neurorehabilitation of stroke.
Copyright © 2014. Published by Elsevier Inc.},
	journal = {Neurobiology of disease},
	author = {Soekadar, Surjo and Birbaumer, Niels and Slutzky, Marc and Cohen, Leonardo},
	month = dec,
	year = {2014},
}

@misc{neuralink_prime_2024,
	title = {{PRIME} {Study} {Progress} {Update} — {Second} {Participant}},
	url = {https://neuralink.com/blog/prime-study-progress-update-second-participant/},
	abstract = {Our technology is enabling our second PRIME Study participant to play video games and use CAD software. This post shares updates on his experience.},
	language = {en},
	urldate = {2025-01-18},
	journal = {Neuralink Blog},
	author = {Neuralink},
	month = aug,
	year = {2024},
}

@article{shih_brain-computer_2012,
	title = {Brain-{Computer} {Interfaces} in {Medicine}},
	volume = {87},
	issn = {0025-6196},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3497935/},
	doi = {10.1016/j.mayocp.2011.12.008},
	abstract = {Brain-computer interfaces (BCIs) acquire brain signals, analyze them, and translate them into commands that are relayed to output devices that carry out desired actions. BCIs do not use normal neuromuscular output pathways. The main goal of BCI is to replace or restore useful function to people disabled by neuromuscular disorders such as amyotrophic lateral sclerosis, cerebral palsy, stroke, or spinal cord injury. From initial demonstrations of electroencephalography-based spelling and single-neuron-based device control, researchers have gone on to use electroencephalographic, intracortical, electrocorticographic, and other brain signals for increasingly complex control of cursors, robotic arms, prostheses, wheelchairs, and other devices. Brain-computer interfaces may also prove useful for rehabilitation after stroke and for other disorders. In the future, they might augment the performance of surgeons or other medical professionals. Brain-computer interface technology is the focus of a rapidly growing research and development enterprise that is greatly exciting scientists, engineers, clinicians, and the public in general. Its future achievements will depend on advances in 3 crucial areas. Brain-computer interfaces need signal-acquisition hardware that is convenient, portable, safe, and able to function in all environments. Brain-computer interface systems need to be validated in long-term studies of real-world use by people with severe disabilities, and effective and viable models for their widespread dissemination must be implemented. Finally, the day-to-day and moment-to-moment reliability of BCI performance must be improved so that it approaches the reliability of natural muscle-based function.},
	number = {3},
	urldate = {2025-01-17},
	journal = {Mayo Clinic Proceedings},
	author = {Shih, Jerry J. and Krusienski, Dean J. and Wolpaw, Jonathan R.},
	month = mar,
	year = {2012},
	pmid = {22325364},
	pmcid = {PMC3497935},
	pages = {268--279},
}

@misc{noauthor_untitled_nodate,
	title = {Untitled presentation},
	url = {https://docs.google.com/presentation/d/13qeKBjEF4Rnlaoc5jcyWaXVKL4P0G1wnRApb9qawR5I/edit?ouid=110423806950173104606&usp=slides_home&ths=true&usp=embed_facebook},
	abstract = {DIGITAL TRANSFORMATION Strategy Plan 20XX COMPANY NAME},
	language = {en},
	urldate = {2025-01-16},
	journal = {Google Docs},
}

@article{kubler_brain-computer_2008,
	title = {Brain-computer interfaces and communication in paralysis: extinction of goal directed thinking in completely paralysed patients?},
	volume = {119},
	issn = {1388-2457},
	shorttitle = {Brain-computer interfaces and communication in paralysis},
	doi = {10.1016/j.clinph.2008.06.019},
	abstract = {OBJECTIVE: To investigate the relationship between physical impairment and brain-computer interface (BCI) performance.
METHOD: We present a meta-analysis of 29 patients with amyotrophic lateral sclerosis and six patients with other severe neurological diseases in different stages of physical impairment who were trained with a BCI. In most cases voluntary regulation of slow cortical potentials has been used as input signal for BCI-control. More recently sensorimotor rhythms and the P300 event-related brain potential were recorded.
RESULTS: A strong correlation has been found between physical impairment and BCI performance, indicating that performance worsens as impairment increases. Seven patients were in the complete locked-in state (CLIS) with no communication possible. After removal of these patients from the analysis, the relationship between physical impairment and BCI performance disappeared. The lack of a relation between physical impairment and BCI performance was confirmed when adding BCI data of patients from other BCI research groups.
CONCLUSIONS: Basic communication (yes/no) was not restored in any of the CLIS patients with a BCI. Whether locked-in patients can transfer learned brain control to the CLIS remains an open empirical question.
SIGNIFICANCE: Voluntary brain regulation for communication is possible in all stages of paralysis except the CLIS.},
	language = {eng},
	number = {11},
	journal = {Clinical Neurophysiology: Official Journal of the International Federation of Clinical Neurophysiology},
	author = {Kübler, A. and Birbaumer, N.},
	month = nov,
	year = {2008},
	pmid = {18824406},
	pmcid = {PMC2644824},
	keywords = {Adult, Aged, Brain Diseases, Communication Devices for People with Disabilities, Computer Systems, Electroencephalography, Event-Related Potentials, P300, Female, Goals, Humans, Male, Meta-Analysis as Topic, Middle Aged, Paralysis, Periodicity, Thinking, User-Computer Interface},
	pages = {2658--2666},
}

@article{hansson_implant_2005,
	title = {Implant ethics},
	volume = {31},
	issn = {0306-6800},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1734218/},
	doi = {10.1136/jme.2004.009803},
	abstract = {Implant ethics is defined here as the study of ethical aspects of the lasting introduction of technological devices into the human body. Whereas technological implants relieve us of some of the ethical problems connected with transplantation, other difficulties arise that are in need of careful analysis. A systematic approach to implant ethics is proposed. The major specific problems are identified as those concerning end of life issues (turning off devices), enhancement of human capabilities beyond normal levels, mental changes and personal identity, and cultural effects.},
	number = {9},
	urldate = {2025-01-16},
	journal = {Journal of Medical Ethics},
	author = {Hansson, S},
	month = sep,
	year = {2005},
	pmid = {16131553},
	pmcid = {PMC1734218},
	pages = {519--525},
}

@article{mikolajewska_ethical_2013,
	title = {Ethical considerations in the use of brain-computer interfaces},
	volume = {8},
	issn = {1644-3640},
	url = {https://doi.org/10.2478/s11536-013-0210-5},
	doi = {10.2478/s11536-013-0210-5},
	abstract = {Nervous system disorders are among the most severe disorders. Significant breakthroughs in contemporary clinical practice may provide brain-computer interfaces (BCIs) and neuroprostheses (NPs). The aim of this article is to investigate the extent to which the ethical considerations in the clinical application of brain-computer interfaces and associated threats are being identified. Ethical considerations and implications may significantly influence further development of BCIs and NPs. Moreover, there is significant public interest in supervising this development. Awareness of BCIs’ and NPs’ threats and limitations allow for wise planning and management in further clinical practice, especially in the area of long-term neurorehabilitation and care.},
	language = {en},
	number = {6},
	urldate = {2025-01-16},
	journal = {Central European Journal of Medicine},
	author = {Mikołajewska, Emilia and Mikołajewski, Dariusz},
	month = dec,
	year = {2013},
	keywords = {Assistive technology, Brain computer interface, Disabled people, Medical Ethics, Neuroprosthesis, Physiotherapy, Rehabilitation},
	pages = {720--724},
}

@article{vansteensel_towards_2023,
	title = {Towards clinical application of implantable brain–computer interfaces for people with late-stage {ALS}: medical and ethical considerations},
	volume = {270},
	issn = {1432-1459},
	shorttitle = {Towards clinical application of implantable brain–computer interfaces for people with late-stage {ALS}},
	url = {https://doi.org/10.1007/s00415-022-11464-6},
	doi = {10.1007/s00415-022-11464-6},
	abstract = {Individuals with amyotrophic lateral sclerosis (ALS) frequently develop speech and communication problems in the course of their disease. Currently available augmentative and alternative communication technologies do not present a solution for many people with advanced ALS, because these devices depend on residual and reliable motor activity. Brain–computer interfaces (BCIs) use neural signals for computer control and may allow people with late-stage ALS to communicate even when conventional technology falls short. Recent years have witnessed fast progression in the development and validation of implanted BCIs, which place neural signal recording electrodes in or on the cortex. Eventual widespread clinical application of implanted BCIs as an assistive communication technology for people with ALS will have significant consequences for their daily life, as well as for the clinical management of the disease, among others because of the potential interaction between the BCI and other procedures people with ALS undergo, such as tracheostomy. This article aims to facilitate responsible real-world implementation of implanted BCIs. We review the state of the art of research on implanted BCIs for communication, as well as the medical and ethical implications of the clinical application of this technology. We conclude that the contribution of all BCI stakeholders, including clinicians of the various ALS-related disciplines, will be needed to develop procedures for, and shape the process of, the responsible clinical application of implanted BCIs.},
	language = {en},
	number = {3},
	urldate = {2025-01-15},
	journal = {Journal of Neurology},
	author = {Vansteensel, Mariska J. and Klein, Eran and van Thiel, Ghislaine and Gaytant, Michael and Simmons, Zachary and Wolpaw, Jonathan R. and Vaughan, Theresa M.},
	month = mar,
	year = {2023},
	keywords = {Amyotrophic lateral sclerosis, Brain–computer interface, Clinical application, Ethics, Implant, Tracheostomy invasive ventilation},
	pages = {1323--1336},
}

@article{chandler_brain_2022,
	title = {Brain {Computer} {Interfaces} and {Communication} {Disabilities}: {Ethical}, {Legal}, and {Social} {Aspects} of {Decoding} {Speech} {From} the {Brain}},
	volume = {16},
	issn = {1662-5161},
	shorttitle = {Brain {Computer} {Interfaces} and {Communication} {Disabilities}},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.841035/full},
	doi = {10.3389/fnhum.2022.841035},
	abstract = {{\textless}p{\textgreater}A brain-computer interface technology that can decode the neural signals associated with attempted but unarticulated speech could offer a future efficient means of communication for people with severe motor impairments. Recent demonstrations have validated this approach. Here we assume that it will be possible in future to decode imagined (i.e., attempted but unarticulated) speech in people with severe motor impairments, and we consider the characteristics that could maximize the social utility of a BCI for communication. As a social interaction, communication involves the needs and goals of both speaker and listener, particularly in contexts that have significant potential consequences. We explore three high-consequence legal situations in which neurally-decoded speech could have implications: {\textless}italic{\textgreater}Testimony{\textless}/italic{\textgreater}, where decoded speech is used as evidence; {\textless}italic{\textgreater}Consent and Capacity{\textless}/italic{\textgreater}, where it may be used as a means of agency and participation such as consent to medical treatment; and {\textless}italic{\textgreater}Harm{\textless}/italic{\textgreater}, where such communications may be networked or may cause harm to others. We then illustrate how design choices might impact the social and legal acceptability of these technologies.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-01-15},
	journal = {Frontiers in Human Neuroscience},
	author = {Chandler, Jennifer A. and Van der Loos, Kiah I. and Boehnke, Susan and Beaudry, Jonas S. and Buchman, Daniel Z. and Illes, Judy},
	month = apr,
	year = {2022},
	note = {Publisher: Frontiers},
	keywords = {AAC, Augmentative and alternative communication, BCI, Brain Computer Interface, Communication, Law, Neuroethics},
}

@article{burwell_ethical_2017,
	title = {Ethical aspects of brain computer interfaces: a scoping review},
	volume = {18},
	issn = {1472-6939},
	shorttitle = {Ethical aspects of brain computer interfaces},
	url = {https://doi.org/10.1186/s12910-017-0220-y},
	doi = {10.1186/s12910-017-0220-y},
	abstract = {Brain-Computer Interface (BCI) is a set of technologies that are of increasing interest to researchers. BCI has been proposed as assistive technology for individuals who are non-communicative or paralyzed, such as those with amyotrophic lateral sclerosis or spinal cord injury. The technology has also been suggested for enhancement and entertainment uses, and there are companies currently marketing BCI devices for those purposes (e.g., gaming) as well as health-related purposes (e.g., communication). The unprecedented direct connection created by BCI between human brains and computer hardware raises various ethical, social, and legal challenges that merit further examination and discussion.},
	language = {en},
	number = {1},
	urldate = {2025-01-15},
	journal = {BMC Medical Ethics},
	author = {Burwell, Sasha and Sample, Matthew and Racine, Eric},
	month = nov,
	year = {2017},
	keywords = {Brain-computer interface, Brain-machine interface, Ethics of technology, Medical Ethics, Scoping review},
	pages = {60},
}

@incollection{abecassis_chapter_2019,
	title = {Chapter 20 - {Brain}-{Computer} {Interface} ({BCI})},
	isbn = {978-0-323-48569-2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323485692000203},
	abstract = {Brain-computer interface encompasses a number of precise technologies aimed at restoring function to the central and peripheral nervous systems via capturing raw neural signals from various cortical regions and modulating the signal into a clinically meaningful output. Here we will describe the evolution of the different approaches used from a signal-acquisition perspective, however, in a historical context. Key examples are highlighted, sampling from successful implementation in an array of disease states, including stroke, neurodegeneration, spinal cord injury, and more.},
	urldate = {2025-01-14},
	booktitle = {Functional {Neurosurgery} and {Neuromodulation}},
	publisher = {Elsevier},
	author = {Abecassis, Isaac Josh and Ko, Andrew L.},
	editor = {Raslan, Ahmed M. and Burchiel, Kim J.},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-323-48569-2.00020-3},
	keywords = {Brain-computer interface, Brain-machine interface, Closed loop, Neural engineering},
	pages = {143--152},
}

@article{bin_Karim_Mixed_Naive_Bayes_2019,
	title = {Mixed naive bayes},
	journal = {https://github.com/remykarem/mixed-naive-bayes},
	author = {bin Karim, Raimi},
	month = oct,
	year = {2019},
}

@inproceedings{john_estimating_1995,
	address = {San Francisco, CA, USA},
	series = {{UAI}'95},
	title = {Estimating continuous distributions in {Bayesian} classifiers},
	isbn = {978-1-55860-385-1},
	abstract = {When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the {Eleventh} conference on {Uncertainty} in artificial intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {John, George H. and Langley, Pat},
	month = aug,
	year = {1995},
	pages = {338--345},
}

@misc{morris-knower_libguides_nodate,
	title = {{LibGuides}: {Measuring} your research impact: {G}-{Index}},
	copyright = {Copyright Cornell University 2025},
	shorttitle = {{LibGuides}},
	url = {https://guides.library.cornell.edu/impact/author-impact-g},
	abstract = {This guide provides an introduction to the various metrics used to measure researcher and journal impact.},
	language = {en},
	urldate = {2025-01-08},
	author = {Morris-Knower, Jim},
}

@misc{noauthor_technology_nodate,
	title = {Technology {\textbar} 2024 {Stack} {Overflow} {Developer} {Survey}},
	url = {https://survey.stackoverflow.co/2024/technology/#most-popular-technologies},
	language = {en},
	urldate = {2025-01-08},
}

@article{groote_factors_2023,
	title = {Factors {Affecting} {Publication} {Impact} and {Citation} {Trends} {Over} {Time}},
	volume = {18},
	copyright = {Copyright (c) 2023 Sandra L. De Groote, Jung Mi Scoulas, Paula R. Dempsey, Felicia Barrett},
	issn = {1715-720X},
	url = {https://journals.library.ualberta.ca/eblip/index.php/EBLIP/article/view/30206},
	doi = {10.18438/eblip30206},
	abstract = {Objective – The researchers investigated whether faculty use of the references in articles had a relationship with the later impact of the publication (measured by citation counts). The paper also reported on additional factors that may influence the later impact of publications.
Methods – This researchers analyzed data for articles published by faculty at a large public university from 1995 to 2015. Data were obtained from the Scopus abstract and citation database and analyzed using SPSS27 to conduct Pearson’s correlations and regression analysis.
Results – The number of references included in publications and the number of citations articles received each year following publication have increased over time. Publications received a greater number of citations annually in their 6th to 10th years, compared to the first 5. The number of references included in an article had a weak correlation with the number of citations an article received. Grant funded articles included more references and later received more citations than non-grant funded articles. Several variables, including number of references used in an article, the number of co-authors, and whether the article was grant funded, were shown to correlate with the later impact of a publication.
Conclusion – Based on the results, researchers should seek out grant funding and generously incorporate literature into their co-authored publications to increase their publications' potential for future impact. These factors may influence article quality, resulting in more citations over time. Further research is needed to better understand their influence and the influence of other factors.},
	language = {en},
	number = {2},
	urldate = {2025-01-08},
	journal = {Evidence Based Library and Information Practice},
	author = {Groote, Sandra L. De and Scoulas, Jung Mi and Dempsey, Paula R. and Barrett, Felicia},
	month = jun,
	year = {2023},
	note = {Number: 2},
	pages = {2--16},
}

@article{poirrier_robust_2021,
	title = {Robust h-index},
	volume = {126},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-020-03857-z},
	doi = {10.1007/s11192-020-03857-z},
	abstract = {The h-index is the most used measurement of impact for researchers. Sites such as Web of Science, Google Scholar, Microsoft Academic, and Scopus leverage it to show and compare the impact of authors. The h-index can be described in simple terms: it is the highest h for which an authors has h papers with the number of cites more or equal than h. Unfortunately, some researchers, in order to increase their productivity artificially, manipulate their h-index using different techniques such as self-citation. Even though it is relatively simple to discard self-citations, every day appears more sophisticated methods to artificially increase this index. One of these methods is collaborative citations, in which a researcher A cites indiscriminately another researcher B, with whom it has a previous collaboration, increasing her/his h-index. This work presents a new robust generalization of the h-index called rh-index that minimizes the impact of new collaborative citations, maintaining the importance of their citations previous to their collaborative work. To demonstrate the usefulness of the proposed index, we analyze its effect over 600 Chilean researchers. Our results show that, while some of the most cited researchers were barely affected, demonstrating their robustness, another group of authors show a substantial reduction in comparison to their original h-index.},
	language = {en},
	number = {3},
	urldate = {2025-01-08},
	journal = {Scientometrics},
	author = {Poirrier, Maurice and Moreno, Sebastián and Huerta-Cánepa, Gonzalo},
	month = mar,
	year = {2021},
	keywords = {Collaborative citation, Robust h-index, Self-citation, h-Index, h-index manipulation},
	pages = {1969--1981},
}

@misc{sadek_svd_2012,
	title = {{SVD} {Based} {Image} {Processing} {Applications}: {State} of {The} {Art}, {Contributions} and {Research} {Challenges}},
	shorttitle = {{SVD} {Based} {Image} {Processing} {Applications}},
	url = {http://arxiv.org/abs/1211.7102},
	doi = {10.48550/arXiv.1211.7102},
	abstract = {Singular Value Decomposition (SVD) has recently emerged as a new paradigm for processing different types of images. SVD is an attractive algebraic transform for image processing applications. The paper proposes an experimental survey for the SVD as an efficient transform in image processing applications. Despite the well-known fact that SVD offers attractive properties in imaging, the exploring of using its properties in various image applications is currently at its infancy. Since the SVD has many attractive properties have not been utilized, this paper contributes in using these generous properties in newly image applications and gives a highly recommendation for more research challenges. In this paper, the SVD properties for images are experimentally presented to be utilized in developing new SVD-based image processing applications. The paper offers survey on the developed SVD based image applications. The paper also proposes some new contributions that were originated from SVD properties analysis in different image processing. The aim of this paper is to provide a better understanding of the SVD in image processing and identify important various applications and open research directions in this increasingly important area; SVD based image processing in the future research.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Sadek, Rowayda A.},
	month = nov,
	year = {2012},
	note = {arXiv:1211.7102 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
}

@article{chicco_advantages_2020,
	title = {The advantages of the {Matthews} correlation coefficient ({MCC}) over {F1} score and accuracy in binary classification evaluation},
	volume = {21},
	issn = {1471-2164},
	url = {https://doi.org/10.1186/s12864-019-6413-7},
	doi = {10.1186/s12864-019-6413-7},
	abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.},
	number = {1},
	urldate = {2025-01-07},
	journal = {BMC Genomics},
	author = {Chicco, Davide and Jurman, Giuseppe},
	month = jan,
	year = {2020},
	keywords = {Accuracy, Binary classification, Biostatistics, Confusion matrices, Dataset imbalance, F1 score, Genomics, Machine learning, Matthews correlation coefficient},
	pages = {6},
}

@misc{noauthor_semantic_nodate,
	title = {Semantic {Scholar} {\textbar} {Frequently} {Asked} {Questions}},
	url = {https://www.semanticscholar.org/faq},
	abstract = {Answers to commonly asked questions about Semantic Scholar.},
	language = {en},
	urldate = {2025-01-07},
}

@article{schober_correlation_2018,
	title = {Correlation {Coefficients}: {Appropriate} {Use} and {Interpretation}},
	volume = {126},
	issn = {0003-2999},
	shorttitle = {Correlation {Coefficients}},
	url = {https://journals.lww.com/anesthesia-analgesia/fulltext/2018/05000/correlation_coefficients__appropriate_use_and.50.aspx},
	doi = {10.1213/ANE.0000000000002864},
	abstract = {Correlation in the broadest sense is a measure of an association between variables. In correlated data, the change in the magnitude of 1 variable is associated with a change in the magnitude of another variable, either in the same (positive correlation) or in the opposite (negative correlation) direction. Most often, the term correlation is used in the context of a linear relationship between 2 continuous variables and expressed as Pearson product-moment correlation. The Pearson correlation coefficient is typically used for jointly normally distributed data (data that follow a bivariate normal distribution). For nonnormally distributed continuous data, for ordinal data, or for data with relevant outliers, a Spearman rank correlation can be used as a measure of a monotonic association. Both correlation coefficients are scaled such that they range from –1 to +1, where 0 indicates that there is no linear or monotonic association, and the relationship gets stronger and ultimately approaches a straight line (Pearson correlation) or a constantly increasing or decreasing curve (Spearman correlation) as the coefficient approaches an absolute value of 1. Hypothesis tests and confidence intervals can be used to address the statistical significance of the results and to estimate the strength of the relationship in the population from which the data were sampled. The aim of this tutorial is to guide researchers and clinicians in the appropriate use and interpretation of correlation coefficients.},
	language = {en-US},
	number = {5},
	urldate = {2025-01-07},
	journal = {Anesthesia \& Analgesia},
	author = {Schober, Patrick and Boer, Christa and Schwarte, Lothar A.},
	month = may,
	year = {2018},
	pages = {1763},
}

@misc{hassan_abir0sjr-journal-ranking_2024,
	title = {abir0/{SJR}-{Journal}-{Ranking}},
	copyright = {MIT},
	url = {https://github.com/abir0/SJR-Journal-Ranking},
	abstract = {A web scraping and visualization project on SJR and WoS journal indexes.},
	urldate = {2025-01-07},
	author = {Hassan, Abir},
	month = may,
	year = {2024},
	note = {original-date: 2023-06-25T14:26:54Z},
	keywords = {data-visualization, python, selenium, tableau, web-scraping},
}

@inproceedings{Wahle2022c,
	address = {Marseille, France},
	title = {D3: a massive dataset of scholarly metadata for analyzing the state of computer science research},
	booktitle = {Proceedings of the 13th language resources and evaluation conference},
	publisher = {European Language Resources Association},
	author = {Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif M. and Gipp, Bela},
	month = jul,
	year = {2022},
}

@inproceedings{lo_s2orc_2020,
	address = {Online},
	title = {{S2ORC}: {The} {Semantic} {Scholar} {Open} {Research} {Corpus}},
	shorttitle = {{S2ORC}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.447},
	doi = {10.18653/v1/2020.acl-main.447},
	language = {en},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney and Weld, Daniel},
	year = {2020},
	pages = {4969--4983},
}

@inproceedings{ammar_construction_2018,
	address = {New Orleans - Louisiana},
	title = {Construction of the {Literature} {Graph} in {Semantic} {Scholar}},
	url = {http://aclweb.org/anthology/N18-3011},
	doi = {10.18653/v1/N18-3011},
	language = {en},
	urldate = {2025-01-07},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 3 ({Industry} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ammar, Waleed and Groeneveld, Dirk and Bhagavatula, Chandra and Beltagy, Iz and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Elgohary, Ahmed and Feldman, Sergey and Ha, Vu and Kinney, Rodney and Kohlmeier, Sebastian and Lo, Kyle and Murray, Tyler and Ooi, Hsu-Han and Peters, Matthew and Power, Joanna and Skjonsberg, Sam and Wang, Lucy and Willhelm, Chris and Yuan, Zheng and Zuylen, Madeleine and {Oren}},
	year = {2018},
	pages = {84--91},
}

@misc{craft-morgan_research_nodate,
	title = {Research {Guides}: {Research} {Impact}: {Research} {Metrics}},
	copyright = {Copyright Ohio State University 2025},
	shorttitle = {Research {Guides}},
	url = {https://guides.osu.edu/c.php?g=608754&p=4233651},
	abstract = {Tracking and Enhancing the Impact of your Research},
	language = {en},
	urldate = {2025-01-05},
	author = {Craft-Morgan, Sheila},
}

@misc{noauthor_notitle_nodate,
	url = {https://www.atlantis-press.com/journals/jegh/metrics/cit_half_life},
	urldate = {2025-01-05},
}

@book{kumar_research_2015,
	title = {Research evaluation metrics},
	isbn = {978-92-3-100082-9},
	abstract = {Traducción parcial de la Introducción: "En la actualidad, la evaluación de la investigaciones es una cuestión que se está replanteando en todo el mundo. En algunos casos, los trabajos de investigación están generando resultados muy buenos, en la mayoría de los casos los resultados son mediocres, y en algunos casos negativos. Por todo esto, la evaluación de los resultados de la investigación se convierte en una condición sine qua non. Cuando el número de investigadores eran menos, eran los propios colegas de profesión quienes evaluaban la investigación. Con el paso del tiempo, el número de investigadores aumentó, las áreas de investigación proliferaron, los resultados de la investigación se multiplicaron. La tendencia continuó y después de la Segunda Guerra Mundial, la investigación comenzó a crecer exponencialmente. Hoy en día, incluso en una estimación moderada hay alrededor de más de un millón de investigadores y producen más de dos millón de trabajos de investigación y otros documentos por año. En este contexto, la evaluación de la investigación es una cuestión de primera importancia. Para cualquier promoción, acreditación, premio y beca puede haber decenas o cientos de nominados. De entre éstos, seleccionar el mejor candidato es una cuestión difícil de determinar. Las evaluaciones inter pares en muchos casos están demostrando ser subjetivas. En 1963 se crea Science Citation Index (SCI) que cubre la literatura científica desde 1961. Unos años después, Eugene Garfield, fundador del SCI, preparó una lista de los 50 autores científicos más citados basándose en las citas que recibía el trabajo de un autor por parte de los trabajos de otros colegas de investigación. El documento titulado "¿Pueden predecirse los ganadores del Premio Nobel? 'Fue publicado en 1968 (Garfield y Malin, 1968). En el siguiente año es decir, 1969, dos científicos que figuran en la lista, por ejemplo, Derek HR Barton y Murray Gell-Mann recibieron el codiciado premio. Esto reivindicó la utilidad del análisis de citas. Cada año, varios científicos pertenecientes al campo de la Física, Química, Fisiología y Medicina reciben el Premio Nobel. De esta manera el análisis de citas se convirtió en una herramienta útil. Sin embargo, el análisis de citas siempre tuvo críticas y múltiples fallas. Incluso Garfield comentó - "El Uso del análisis de citas de los trabajos de evaluación es una tarea difícil. Existen muchas posibilidades de error '(Garfiled, 1983). Para la evaluación de la investigación, se necesitaban algunos otros indicadores. El análisis de citas, junto con la revisión por pares garantiza el mejor juicio en innumerables casos. Pero se necesita algo que sea más exacto. La llegada de la World Wide Web (WWW) brindó la oportunidad; pues un buen número de indicadores se están generando a partir de los datos disponibles en la WWW". (Trad. Julio Alonso Arévalo. Univ. Salamanca).},
	language = {en},
	publisher = {UNESCO Publishing},
	author = {Kumar, Anup, Das},
	month = apr,
	year = {2015},
	note = {Google-Books-ID: MZlYCgAAQBAJ},
}

@misc{craft-morgan_research_nodate-1,
	title = {Research {Guides}: {Research} {Impact}: {Introduction}},
	copyright = {Copyright Ohio State University 2025},
	shorttitle = {Research {Guides}},
	url = {https://guides.osu.edu/c.php?g=608754&p=4224917},
	abstract = {Tracking and Enhancing the Impact of your Research},
	language = {en},
	urldate = {2025-01-05},
	author = {Craft-Morgan, Sheila},
}

@misc{noauthor_number_nodate,
	title = {Number of patent applications worldwide 2022},
	url = {https://www.statista.com/statistics/257610/number-of-patent-applications-worldwide/},
	abstract = {As of 2022, the number of patent applications worldwide amounted to about 3.45 million.},
	language = {en},
	urldate = {2024-12-30},
	journal = {Statista},
}

@article{fire_over-optimization_2019,
	title = {Over-optimization of academic publishing metrics: {Observing} {Goodhart}'s {Law} in action},
	volume = {8},
	shorttitle = {Over-optimization of academic publishing metrics},
	doi = {10.1093/gigascience/giz053},
	abstract = {Background
The academic publishing world is changing significantly, with ever-growing numbers of publications each year and shifting publishing patterns. However, the metrics used to measure academic success, such as the number of publications, citation number, and impact factor, have not changed for decades. Moreover, recent studies indicate that these metrics have become targets and follow Goodhart’s Law, according to which, “when a measure becomes a target, it ceases to be a good measure.”

Results
In this study, we analyzed {\textgreater}120 million papers to examine how the academic publishing world has evolved over the last century, with a deeper look into the specific field of biology. Our study shows that the validity of citation-based measures is being compromised and their usefulness is lessening. In particular, the number of publications has ceased to be a good metric as a result of longer author lists, shorter papers, and surging publication numbers. Citation-based metrics, such citation number and h-index, are likewise affected by the flood of papers, self-citations, and lengthy reference lists. Measures such as a journal’s impact factor have also ceased to be good metrics due to the soaring numbers of papers that are published in top journals, particularly from the same pool of authors. Moreover, by analyzing properties of {\textgreater}2,600 research fields, we observed that citation-based metrics are not beneficial for comparing researchers in different fields, or even in the same department.

Conclusions
Academic publishing has changed considerably; now we need to reconsider how we measure success.},
	journal = {GigaScience},
	author = {Fire, Michael and Guestrin, Carlos},
	month = jun,
	year = {2019},
}

@incollection{shaver_science_2018,
	address = {Cham},
	title = {Science {Today}},
	isbn = {978-3-319-91812-9},
	url = {https://doi.org/10.1007/978-3-319-91812-9_4},
	abstract = {90\% of all the scientists who have ever lived are alive today. By contrast, less than 7\% of all the people who have ever lived are alive today. It has been estimated that there were a few hundred scientists in the mid-1700s. If the number of scientists had increased at the same rate as the overall population, the number of scientists today would be a few thousand. Instead, according to UNESCO, there are about eight million researchers in the world today. The increase in the number of scientists over the last couple of hundred years is thousands of times the increase in the overall population. The growth rate in the number of scientists since the mid-1700s has been about 4\% per year, corresponding to a doubling time of about 18 years and far faster than the approximately 0.8\% per year growth rate for the overall population over that period. Currently in China the number of researchers is increasing at the furious rate of 6.6\% per year, while its overall population is growing at just 0.6 \% per year. There is no question that the number of scientists has increased dramatically over the last few hundred years (see Fig. 4.1).},
	language = {en},
	urldate = {2024-12-30},
	booktitle = {The {Rise} of {Science}: {From} {Prehistory} to the {Far} {Future}},
	publisher = {Springer International Publishing},
	author = {Shaver, Peter},
	editor = {Shaver, Peter},
	year = {2018},
	doi = {10.1007/978-3-319-91812-9_4},
	pages = {129--209},
}

@misc{ji_mipre_2022,
	title = {{MIP}*={RE}},
	url = {http://arxiv.org/abs/2001.04383},
	doi = {10.48550/arXiv.2001.04383},
	abstract = {We show that the class MIP* of languages that can be decided by a classical verifier interacting with multiple all-powerful quantum provers sharing entanglement is equal to the class RE of recursively enumerable languages. Our proof builds upon the quantum low-degree test of (Natarajan and Vidick, FOCS 2018) and the classical low-individual degree test of (Ji, et al., 2020) by integrating recent developments from (Natarajan and Wright, FOCS 2019) and combining them with the recursive compression framework of (Fitzsimons et al., STOC 2019). An immediate byproduct of our result is that there is an efficient reduction from the Halting Problem to the problem of deciding whether a two-player nonlocal game has entangled value \$1\$ or at most \$1/2\$. Using a known connection, undecidability of the entangled value implies a negative answer to Tsirelson's problem: we show, by providing an explicit example, that the closure \$C\_\{qa\}\$ of the set of quantum tensor product correlations is strictly included in the set \$C\_\{qc\}\$ of quantum commuting correlations. Following work of (Fritz, Rev. Math. Phys. 2012) and (Junge et al., J. Math. Phys. 2011) our results provide a refutation of Connes' embedding conjecture from the theory of von Neumann algebras.},
	urldate = {2024-12-21},
	publisher = {arXiv},
	author = {Ji, Zhengfeng and Natarajan, Anand and Vidick, Thomas and Wright, John and Yuen, Henry},
	month = nov,
	year = {2022},
	note = {arXiv:2001.04383 [quant-ph]},
	keywords = {Computer Science - Computational Complexity, Mathematics - Operator Algebras, Quantum Physics},
}

@article{aas_explaining_2021,
	title = {Explaining individual predictions when features are dependent: {More} accurate approximations to {Shapley} values},
	volume = {298},
	issn = {0004-3702},
	shorttitle = {Explaining individual predictions when features are dependent},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221000539},
	doi = {10.1016/j.artint.2021.103502},
	abstract = {Explaining complex or seemingly simple machine learning models is an important practical problem. We want to explain individual predictions from such models by learning simple, interpretable explanations. Shapley value is a game theoretic concept that can be used for this purpose. The Shapley value framework has a series of desirable theoretical properties, and can in principle handle any predictive model. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes that the features are independent. Since Shapley values currently suffer from inclusion of unrealistic data instances when features are correlated, the explanations may be very misleading. This is the case even if a simple linear model is used for predictions. In this paper, we extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with various degrees of feature dependence, where our method gives more accurate approximations to the true Shapley values.},
	urldate = {2024-11-26},
	journal = {Artificial Intelligence},
	author = {Aas, Kjersti and Jullum, Martin and Løland, Anders},
	month = sep,
	year = {2021},
	keywords = {Dependence, Feature attribution, Kernel SHAP, Shapley values},
	pages = {103502},
}

@misc{zhang_interpretable_2018,
	title = {Interpretable {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1710.00935},
	doi = {10.48550/arXiv.1710.00935},
	abstract = {This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
	month = feb,
	year = {2018},
	note = {arXiv:1710.00935},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ngiam_multimodal_2011,
	title = {Multimodal deep learning},
	url = {http://ai.stanford.edu/~ang/papers/icml11-MultimodalDeepLearning.pdf},
	urldate = {2024-11-26},
	booktitle = {Proceedings of the 28th international conference on machine learning ({ICML}-11)},
	author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y.},
	year = {2011},
	pages = {689--696},
}

@book{ngiam_multimodal_2011-1,
	title = {Multimodal {Deep} {Learning}},
	abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning. 1.},
	author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew},
	month = jan,
	year = {2011},
	note = {Journal Abbreviation: Proceedings of the 28th International Conference on Machine Learning, ICML 2011
Pages: 696
Publication Title: Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
}

@article{ngiam_multimodal_nodate,
	title = {Multimodal {Deep} {Learning}},
	abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classiﬁer is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classiﬁcation, demonstrating best published visual speech classiﬁcation on AVLetters and eﬀective shared representation learning.},
	language = {en},
	author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
}

@article{jiang_autosurv_2024,
	title = {Autosurv: interpretable deep learning framework for cancer survival analysis incorporating clinical and multi-omics data},
	volume = {8},
	copyright = {2024 The Author(s)},
	issn = {2397-768X},
	shorttitle = {Autosurv},
	url = {https://www.nature.com/articles/s41698-023-00494-6},
	doi = {10.1038/s41698-023-00494-6},
	abstract = {Accurate prognosis for cancer patients can provide critical information for optimizing treatment plans and improving life quality. Combining omics data and demographic/clinical information can offer a more comprehensive view of cancer prognosis than using omics or clinical data alone and can also reveal the underlying disease mechanisms at the molecular level. In this study, we developed and validated a deep learning framework to extract information from high-dimensional gene expression and miRNA expression data and conduct prognosis prediction for breast cancer and ovarian-cancer patients using multiple independent multi-omics datasets. Our model achieved significantly better prognosis prediction than the current machine learning and deep learning approaches in various settings. Moreover, an interpretation method was applied to tackle the “black-box” nature of deep neural networks and we identified features (i.e., genes, miRNA, demographic/clinical variables) that were important to distinguish predicted high- and low-risk patients. The significance of the identified features was partially supported by previous studies.},
	language = {en},
	number = {1},
	urldate = {2024-11-26},
	journal = {npj Precision Oncology},
	author = {Jiang, Lindong and Xu, Chao and Bai, Yuntong and Liu, Anqi and Gong, Yun and Wang, Yu-Ping and Deng, Hong-Wen},
	month = jan,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer, Computational biology and bioinformatics},
	pages = {1--16},
}

@inproceedings{tadesse_cardiovascular_2019,
	title = {Cardiovascular disease diagnosis using cross-domain transfer learning},
	url = {https://ieeexplore.ieee.org/abstract/document/8857737},
	doi = {10.1109/EMBC.2019.8857737},
	abstract = {While cardiovascular diseases (CVDs) are commonly diagnosed by cardiologists via inspecting electrocardiogram (ECG) waveforms, these decisions can be supported by a data-driven approach, which may automate this process. An automatic diagnostic approach often employs hand-crafted features extracted from ECG waveforms. These features, however, do not generalise well, challenged by variation in acquisition settings such as sampling rate and mounting points. Existing deep learning (DL) approaches, on the other hand, extract features from ECG automatically but require construction of dedicated networks that require huge data and computational resource if trained from scratch. Here we propose an end-to-end trainable cross-domain transfer learning for CVD classification from ECG waveforms, by utilising existing vision-based CNN frameworks as feature extractors, followed by ECG feature learning layers. Because these frameworks are designed for image inputs, we employ a stacked spectrogram representation of multi-lead ECG waveforms as a preprocessing step. We also proposed a fusion of multiple ECG leads, using plausible stacking arrangements of the spectrograms, to encode their spatial relations. The proposed approach is validated on multiple ECG datasets and competitive performance is achieved.},
	urldate = {2024-11-26},
	booktitle = {2019 41st {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Tadesse, Girmaw Abebe and Zhu, Tingting and Liu, Yong and Zhou, Yingling and Chen, Jiyan and Tian, Maoyi and Clifton, David},
	month = jul,
	year = {2019},
	note = {ISSN: 1558-4615},
	keywords = {Cardiovascular Disease, Deep Learning, Electrocardiography, Feature extraction, Health Informatics, Microsoft Windows, Spectrogram, Stacking, Support vector machines, Training, Transfer Learning},
	pages = {4262--4265},
}

@inproceedings{oussidi_deep_2018,
	title = {Deep generative models: {Survey}},
	shorttitle = {Deep generative models},
	url = {https://ieeexplore.ieee.org/abstract/document/8354080},
	doi = {10.1109/ISACV.2018.8354080},
	abstract = {Generative models have found their way to the forefront of deep learning the last decade and so far, it seems that the hype will not fade away any time soon. In this paper, we give an overview of the most important building blocks of most recent revolutionary deep generative models such as RBM, DBM, DBN, VAE and GAN. We will also take a look at three of state-of-the-art generative models, namely PixelRNN, DRAW and NADE. We will delve into their unique architectures, the learning procedures and their potential and limitations. We will also review some of the known issues that arise when trying to design and train deep generative architectures using shallow ones and how different models deal with these issues. This paper is not meant to be a comprehensive study of these models, but rather a starting point for those who bear an interest in the field.},
	urldate = {2024-11-26},
	booktitle = {2018 {International} {Conference} on {Intelligent} {Systems} and {Computer} {Vision} ({ISCV})},
	author = {Oussidi, Achraf and Elhassouny, Azeddine},
	month = apr,
	year = {2018},
	keywords = {Architecture, Data models, Decoding, Neural networks, Neurons, Probabilistic logic, Training},
	pages = {1--8},
}

@article{guo_deep_2020,
	title = {Deep {Group}-{Shuffling} {Dual} {Random} {Walks} {With} {Label} {Smoothing} for {Person} {Reidentification}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9015960},
	doi = {10.1109/ACCESS.2020.2976849},
	abstract = {Person reidentification (ReID) is a challenging task of finding a target pedestrian in a gallery set collected from multiple nonoverlapping camera views. Recently, state-of-the-art ReID performance has been achieved via an end-to-end trainable deep neural network framework, which integrates convolution feature extraction, similarity learning and reranking into a joint optimization framework. In such a framework, the similarity is learned via an embedding network, the reranking is conducted with a random walk, and the whole framework is optimized with a cross-entropy-based verification loss. Unfortunately, the embedding net is difficult to train well because their two-dimensional outputs mutually interfere each other when using the conventional random walk. In addition, the supervision information has not been fully exploited during the training phase due to the binary nature of the verification loss. In this paper, we propose a novel approach, called group-shuffling dual random walks with label smoothing (GSDRWLS), in which random walks are performed separately on two channels-one for positive verification and one for negative verification-and the binary verification labels are properly modified with an adaptive label smoothing technique before feeding into the verification loss in order to train the overall network effectively and to avoid the overfitting problem. Extensive experiments conducted on three large benchmark datasets, including CUHK03, Market-1501 and DukeMTMC, confirm the superior performance of our proposal.},
	urldate = {2024-11-26},
	journal = {IEEE Access},
	author = {Guo, Ruopei and Lin, Chaoqun and Li, Chun-Guang and Lin, Jiaru},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Feature extraction, Neural networks, Person reidentification, Probes, Proposals, Smoothing methods, Task analysis, Training, deep neural network, dual random walks, label smoothing},
	pages = {40018--40028},
}

@article{xin_improved_2022,
	title = {An improved transformer network for skin cancer classification},
	volume = {149},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482522006746},
	doi = {10.1016/j.compbiomed.2022.105939},
	abstract = {Background
Use of artificial intelligence to identify dermoscopic images has brought major breakthroughs in recent years to the early diagnosis and early treatment of skin cancer, the incidence of which is increasing year by year worldwide and poses a great threat to human health. Achievements have been made in the research of skin cancer image classification by using the deep backbone of the convolutional neural network (CNN). This approach, however, only extracts the features of small objects in the image, and cannot locate the important parts.
Objectives
As a result, researchers of the paper turn to vision transformers (VIT) which has demonstrated powerful performance in traditional classification tasks. The self-attention is to improve the value of important features and suppress the features that cause noise. Specifically, an improved transformer network named SkinTrans is proposed.
Innovations
To verify its efficiency, a three step procedure is followed. Firstly, a VIT network is established to verify the effectiveness of SkinTrans in skin cancer classification. Then multi-scale and overlapping sliding windows are used to serialize the image and multi-scale patch embedding is carried out which pay more attention to multi-scale features. Finally, contrastive learning is used which makes the similar data of skin cancer encode similarly so that the encoding results of different data are as different as possible.
Main results
The experiment is carried out based on two datasets, namely (1) HAM10000: a large dataset of multi-source dermatoscopic images of common skin cancers; (2)A clinical dataset of skin cancer collected by dermoscopy. The model proposed has achieved 94.3\% accuracy on HAM10000 and 94.1\% accuracy on our datasets, which verifies the efficiency of SkinTrans.
Conclusions
The transformer network has not only achieved good results in natural language but also achieved ideal results in the field of vision, which also lays a good foundation for skin cancer classification based on multimodal data. This paper is convinced that it will be of interest to dermatologists, clinical researchers, computer scientists and researchers in other related fields, and provide greater convenience for patients.},
	urldate = {2024-11-26},
	journal = {Computers in Biology and Medicine},
	author = {Xin, Chao and Liu, Zhifang and Zhao, Keyu and Miao, Linlin and Ma, Yizhao and Zhu, Xiaoxia and Zhou, Qiongyan and Wang, Songting and Li, Lingzhi and Yang, Feng and Xu, Suling and Chen, Haijiang},
	month = oct,
	year = {2022},
	keywords = {Classification, Contrastive learning, Skin cancer, Vision transformer},
	pages = {105939},
}

@article{krittanawong_artificial_2022,
	title = {Artificial {Intelligence} and {Cardiovascular} {Genetics}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1729},
	url = {https://www.mdpi.com/2075-1729/12/2/279},
	doi = {10.3390/life12020279},
	abstract = {Polygenic diseases, which are genetic disorders caused by the combined action of multiple genes, pose unique and significant challenges for the diagnosis and management of affected patients. A major goal of cardiovascular medicine has been to understand how genetic variation leads to the clinical heterogeneity seen in polygenic cardiovascular diseases (CVDs). Recent advances and emerging technologies in artificial intelligence (AI), coupled with the ever-increasing availability of next generation sequencing (NGS) technologies, now provide researchers with unprecedented possibilities for dynamic and complex biological genomic analyses. Combining these technologies may lead to a deeper understanding of heterogeneous polygenic CVDs, better prognostic guidance, and, ultimately, greater personalized medicine. Advances will likely be achieved through increasingly frequent and robust genomic characterization of patients, as well the integration of genomic data with other clinical data, such as cardiac imaging, coronary angiography, and clinical biomarkers. This review discusses the current opportunities and limitations of genomics; provides a brief overview of AI; and identifies the current applications, limitations, and future directions of AI in genomics.},
	language = {en},
	number = {2},
	urldate = {2024-11-26},
	journal = {Life},
	author = {Krittanawong, Chayakrit and Johnson, Kipp W. and Choi, Edward and Kaplin, Scott and Venner, Eric and Murugan, Mullai and Wang, Zhen and Glicksberg, Benjamin S. and Amos, Christopher I. and Schatz, Michael C. and Tang, W. H. Wilson},
	month = feb,
	year = {2022},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {AI, artificial intelligence, cardiology, cardiovascular disease, deep learning, genetics, genomics, machine learning},
	pages = {279},
}

@article{venkat_investigating_2023,
	title = {Investigating genes associated with heart failure, atrial fibrillation, and other cardiovascular diseases, and predicting disease using machine learning techniques for translational research and precision medicine},
	volume = {115},
	issn = {0888-7543},
	url = {https://www.sciencedirect.com/science/article/pii/S0888754323000289},
	doi = {10.1016/j.ygeno.2023.110584},
	abstract = {Cardiovascular disease (CVD) is the leading cause of mortality and loss of disability adjusted life years (DALYs) globally. CVDs like Heart Failure (HF) and Atrial Fibrillation (AF) are associated with physical effects on the heart muscles. As a result of the complex nature, progression, inherent genetic makeup, and heterogeneity of CVDs, personalized treatments are believed to be critical. Rightful application of artificial intelligence (AI) and machine learning (ML) approaches can lead to new insights into CVDs for providing better personalized treatments with predictive analysis and deep phenotyping. In this study we focused on implementing AI/ML techniques on RNA-seq driven gene-expression data to investigate genes associated with HF, AF, and other CVDs, and predict disease with high accuracy. The study involved generating RNA-seq data derived from the serum of consented CVD patients. Next, we processed the sequenced data using our RNA-seq pipeline and applied GVViZ for gene-disease data annotation and expression analysis. To achieve our research objectives, we developed a new Findable, Accessible, Intelligent, and Reproducible (FAIR) approach that includes a five-level biostatistical evaluation, primarily based on the Random Forest (RF) algorithm. During our AI/ML analysis, we have fitted, trained, and implemented our model to classify and distinguish high-risk CVD patients based on their age, gender, and race. With the successful execution of our model, we predicted the association of highly significant HF, AF, and other CVDs genes with demographic variables.},
	number = {2},
	urldate = {2024-11-25},
	journal = {Genomics},
	author = {Venkat, Vignesh and Abdelhalim, Habiba and DeGroat, William and Zeeshan, Saman and Ahmed, Zeeshan},
	month = mar,
	year = {2023},
	keywords = {Artificial intelligence, Atrial fibrillation, Cardiovascular diseases, Gene expression, Heart failure, Machine learning, Predictive analysis},
	pages = {110584},
}

@article{zhang_deep_2019,
	title = {Deep {Learning} for {Diagnosis} of {Chronic} {Myocardial} {Infarction} on                    {Nonenhanced} {Cardiac} {Cine} {MRI}},
	volume = {291},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/10.1148/radiol.2019182304},
	doi = {10.1148/radiol.2019182304},
	abstract = {Background Renal impairment is common in patients with coronary artery disease and,                            if severe, late gadolinium enhancement (LGE) imaging for myocardial                            infarction (MI) evaluation cannot be performed.Purpose To develop a fully automatic framework for chronic MI delineation via                            deep learning on non–contrast material–enhanced cardiac                            cine MRI.Materials and Methods In this retrospective single-center study, a deep learning model was                            developed to extract motion features from the left ventricle and                            delineate MI regions on nonenhanced cardiac cine MRI collected between                            October 2015 and March 2017. Patients with chronic MI, as well as                            healthy control patients, had both nonenhanced cardiac cine (25 phases                            per cardiac cycle) and LGE MRI examinations. Eighty percent of MRI                            examinations were used for the training data set and 20\% for the                            independent testing data set. Chronic MI regions on LGE MRI were defined                            as ground truth. Diagnostic performance was assessed by analysis of the                            area under the receiver operating characteristic curve (AUC). MI area                            and MI area percentage from nonenhanced cardiac cine and LGE MRI were                            compared by using the Pearson correlation, paired t                            test, and Bland-Altman analysis.Results Study participants included 212 patients with chronic MI (men, 171; age,                            57.2 years ± 12.5) and 87 healthy control patients (men, 42; age,                            43.3 years ± 15.5). Using the full cardiac cine MRI, the                            per-segment sensitivity and specificity for detecting chronic MI in the                            independent test set was 89.8\% and 99.1\%, respectively, with an AUC of                            0.94. There were no differences between nonenhanced cardiac cine and LGE                            MRI analyses in number of MI segments (114 vs 127, respectively;                                P = .38), per-patient MI area (6.2                                cm2 ± 2.8 vs 5.5 cm2 ± 2.3,                            respectively; P = .27; correlation coefficient,                                r = 0.88), and MI area percentage (21.5\%                            ± 17.3 vs 18.5\% ± 15.4; P = .17;                            correlation coefficient, r = 0.89).Conclusion The proposed deep learning framework on nonenhanced cardiac cine MRI                            enables the confirmation (presence), detection (position), and                            delineation (transmurality and size) of chronic myocardial infarction.                            However, future larger-scale multicenter studies are required for a full                            validation. Published under a CC BY 4.0 license. Online supplemental material is available for this                                    article. See also the editorial by Leiner in this issue.},
	number = {3},
	urldate = {2024-11-25},
	journal = {Radiology},
	author = {Zhang, Nan and Yang, Guang and Gao, Zhifan and Xu, Chenchu and Zhang, Yanping and Shi, Rui and Keegan, Jennifer and Xu, Lei and Zhang, Heye and Fan, Zhanming and Firmin, David},
	month = jun,
	year = {2019},
	note = {Publisher: Radiological Society of North America},
	pages = {606--617},
}

@article{leveque_comparative_2021,
	title = {Comparative study of the methodologies used for subjective medical image quality assessment},
	volume = {66},
	issn = {1361-6560},
	doi = {10.1088/1361-6560/ac1157},
	abstract = {Healthcare professionals have been increasingly viewing medical images and videos in their routine clinical practice, and this in a wide variety of environments. Both the perception and interpretation of medical visual information, across all branches of practice or medical specialties (e.g. diagnostic, therapeutic, or surgical medicine), career stages, and practice settings (e.g. emergency care), appear to be critical for patient care. However, medical images and videos are not self-explanatory and, therefore, need to be interpreted by humans, i.e. medical experts. In addition, various types of degradations and artifacts may appear during image acquisition or processing, and consequently affect medical imaging data. Such distortions tend to impact viewers' quality of experience, as well as their clinical practice. It is accordingly essential to better understand how medical experts perceive the quality of visual content. Thankfully, progress has been made in the recent literature towards such understanding. In this article, we present an up-to-date state-of the-art of relatively recent (i.e. not older than ten years old) existing studies on the subjective quality assessment of medical images and videos, as well as research works using task-based approaches. Furthermore, we discuss the merits and drawbacks of the methodologies used, and we provide recommendations about experimental designs and statistical processes to evaluate the perception of medical images and videos for future studies, which could then be used to optimise the visual experience of image readers in real clinical practice. Finally, we tackle the issue of the lack of available annotated medical image and video quality databases, which appear to be indispensable for the development of new dedicated objective metrics.},
	language = {eng},
	number = {15},
	journal = {Physics in Medicine and Biology},
	author = {Lévêque, Lucie and Outtas, Meriem and Liu, Hantao and Zhang, Lu},
	month = jul,
	year = {2021},
	pmid = {34225264},
	keywords = {Artifacts, Child, Databases, Factual, Diagnostic Imaging, Humans, Radiography, image quality assessment, medical imaging, objective metrics, subjective experiment, task performance},
}

@article{madani_deep_2018,
	title = {Deep echocardiography: data-efficient supervised and semi-supervised deep learning towards automated diagnosis of cardiac disease},
	volume = {1},
	copyright = {2018 The Author(s)},
	issn = {2398-6352},
	shorttitle = {Deep echocardiography},
	url = {https://www.nature.com/articles/s41746-018-0065-x},
	doi = {10.1038/s41746-018-0065-x},
	abstract = {Deep learning and computer vision algorithms can deliver highly accurate and automated interpretation of medical imaging to augment and assist clinicians. However, medical imaging presents uniquely pertinent obstacles such as a lack of accessible data or a high-cost of annotation. To address this, we developed data-efficient deep learning classifiers for prediction tasks in cardiology. Using pipeline supervised models to focus relevant structures, we achieve an accuracy of 94.4\% for 15-view still-image echocardiographic view classification and 91.2\% accuracy for binary left ventricular hypertrophy classification. We then develop semi-supervised generative adversarial network models that can learn from both labeled and unlabeled data in a generalizable fashion. We achieve greater than 80\% accuracy in view classification with only 4\% of labeled data used in solely supervised techniques and achieve 92.3\% accuracy for left ventricular hypertrophy classification. In exploring trade-offs between model type, resolution, data resources, and performance, we present a comprehensive analysis and improvements of efficient deep learning solutions for medical imaging assessment especially in cardiology.},
	language = {en},
	number = {1},
	urldate = {2024-11-25},
	journal = {npj Digital Medicine},
	author = {Madani, Ali and Ong, Jia Rui and Tibrewal, Anshul and Mofrad, Mohammad R. K.},
	month = oct,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Echocardiography, Heart failure},
	pages = {1--11},
}

@article{chen_using_2019,
	title = {Using machine learning to predict one-year cardiovascular events in patients with severe dilated cardiomyopathy},
	volume = {117},
	issn = {0720-048X},
	url = {https://www.sciencedirect.com/science/article/pii/S0720048X19302086},
	doi = {10.1016/j.ejrad.2019.06.004},
	abstract = {Purpose
Dilated cardiomyopathy (DCM) is a common form of cardiomyopathy and it is associated with poor outcomes. A poor prognosis of DCM patients with low ejection fraction has been noted in the short-term follow-up. Machine learning (ML) could aid clinicians in risk stratification and patient management after considering the correlation between numerous features and the outcomes. The present study aimed to predict the 1-year cardiovascular events in patients with severe DCM using ML, and aid clinicians in risk stratification and patient management.
Materials and Methods
The dataset used to establish the ML model was obtained from 98 patients with severe DCM (LVEF {\textless} 35\%) from two centres. Totally 32 features from clinical data were input to the ML algorithm, and the significant features highly relevant to the cardiovascular events were selected by Information gain (IG). A naive Bayes classifier was built, and its predictive performance was evaluated using the area under the curve (AUC) of the receiver operating characteristics by 10-fold cross-validation.
Results
During the 1-year follow-up, a total of 22 patients met the criterion of the study end-point. The top features with IG {\textgreater} 0.01 were selected for ML model, including left atrial size (IG = 0.240), QRS duration (IG = 0.200), and systolic blood pressure (IG = 0.151). ML performed well in predicting cardiovascular events in patients with severe DCM (AUC, 0.887 [95\% confidence interval, 0.813–0.961]).
Conclusions
ML effectively predicted risk in patients with severe DCM in 1-year follow-up, and this may direct risk stratification and patient management in the future.},
	urldate = {2024-11-25},
	journal = {European Journal of Radiology},
	author = {Chen, Rui and Lu, Aijia and Wang, Jingjing and Ma, Xiaohai and Zhao, Lei and Wu, Wanjia and Du, Zhicheng and Fei, Hongwen and Lin, Qiongwen and Yu, Zhuliang and Liu, Hui},
	month = aug,
	year = {2019},
	keywords = {Machine learning, Prognostic value, Severe dilated cardiomyopathy},
	pages = {178--183},
}

@article{martin-isla_image-based_2020,
	title = {Image-{Based} {Cardiac} {Diagnosis} {With} {Machine} {Learning}: {A} {Review}},
	volume = {7},
	issn = {2297-055X},
	shorttitle = {Image-{Based} {Cardiac} {Diagnosis} {With} {Machine} {Learning}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6992607/},
	doi = {10.3389/fcvm.2020.00001},
	abstract = {Cardiac imaging plays an important role in the diagnosis of cardiovascular disease (CVD). Until now, its role has been limited to visual and quantitative assessment of cardiac structure and function. However, with the advent of big data and machine learning, new opportunities are emerging to build artificial intelligence tools that will directly assist the clinician in the diagnosis of CVDs. This paper presents a thorough review of recent works in this field and provide the reader with a detailed presentation of the machine learning methods that can be further exploited to enable more automated, precise and early diagnosis of most CVDs.},
	urldate = {2024-11-25},
	journal = {Frontiers in Cardiovascular Medicine},
	author = {Martin-Isla, Carlos and Campello, Victor M. and Izquierdo, Cristian and Raisi-Estabragh, Zahra and Baeßler, Bettina and Petersen, Steffen E. and Lekadir, Karim},
	month = jan,
	year = {2020},
	pmid = {32039241},
	pmcid = {PMC6992607},
	pages = {1},
}

@article{abdelhalim_role_2023,
	title = {Role of genome-wide association studies, polygenic risk score and {AI}/{ML} using big data for personalized treatment to the patients with cardiovascular disease},
	volume = {1},
	url = {https://fmai.scholasticahq.com/article/117969-role-of-genome-wide-association-studies-polygenic-risk-score-and-ai-ml-using-big-data-for-personalized-treatment-to-the-patients-with-cardiovascular},
	doi = {10.2217/fmai-2023-0018},
	abstract = {Tweetable abstract Role of GWAS, polygenic risk score and AI/ML using big data for personalized treatment to the patients with cardiovascular disease},
	language = {en},
	number = {2},
	urldate = {2024-11-25},
	journal = {Future Medicine AI},
	author = {Abdelhalim, Habiba and Hunter, Rachel-Mae and DeGroat, William and Mendhe, Dinesh and Zeeshan, Saman and Ahmed, Zeeshan},
	month = nov,
	year = {2023},
	note = {Publisher: Future Medicine},
}

@article{liu_machine_2021,
	title = {Machine learning-based long-term outcome prediction in patients undergoing percutaneous coronary intervention},
	volume = {11},
	issn = {2223-3652},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8261740/},
	doi = {10.21037/cdt-21-37},
	abstract = {Background
Traditional prognostic risk assessment in patients with coronary artery disease undergoing percutaneous coronary intervention (PCI) is based on a limited selection of clinical and imaging findings. Machine learning (ML) can consider a higher number and complexity of variables and may be useful for characterising cardiovascular risk, predicting outcomes, and identifying biomarkers in large population studies.

Methods
We prospectively enrolled 9,680 consecutive patients with coronary artery disease who underwent PCI at our institution between January 2013 and December 2013. Clinical features were selected and used to train 6 different ML models (support vector machine, decision tree, random forest, gradient boosting decision tree, neural network, and logistic regression) to predict cardiovascular outcomes, 10-fold cross-validation to evaluate the performance of models.

Results
During the 5-year follow-up, 467 (4.82\%) patients died. Eighty-seven risk baseline measurements were used to train ML models. Compared with the other models, the random forest model (RF-PCI) exhibited the best performance on predicting all-cause mortality (area under the receiver operating characteristic curve: 0.71±0.04). Calibration plots demonstrated a slight overprediction for patients using the RF-PCI model (Hosmer-Lemeshow test: P{\textgreater}0.05). The top 15 features related to PCI candidates’ long-term prognosis, among which 11 were laboratory measures.

Conclusions
ML models improved the prediction of long-term all-cause mortality in patients with coronary artery disease before PCI. The performance of the RF model was better than that of the other models, providing a meaningful stratification.},
	number = {3},
	urldate = {2024-11-25},
	journal = {Cardiovascular Diagnosis and Therapy},
	author = {Liu, Shangyu and Yang, Shengwen and Xing, Anlu and Zheng, Lihui and Shen, Lishui and Tu, Bin and Yao, Yan},
	month = jun,
	year = {2021},
	pmid = {34295700},
	pmcid = {PMC8261740},
	pages = {736--743},
}

@article{cui_bidirectional_2021,
	title = {Bidirectional cross-modality unsupervised domain adaptation using generative adversarial networks for cardiac image segmentation},
	volume = {136},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482521005205},
	doi = {10.1016/j.compbiomed.2021.104726},
	abstract = {Background: A novel Generative Adversarial Networks (GAN) based bidirectional cross-modality unsupervised domain adaptation (GBCUDA) framework is developed for cardiac image segmentation, which can effectively tackle the problem of network’s segmentation performance degradation when adapting to the target domain without ground truth labels.
Method: GBCUDA uses GAN for image alignment, applies adversarial learning to extract image features, and gradually enhances the domain invariance of extracted features. The shared encoder performs an end-to-end learning task in which features that differ between the two domains complement each other. The selfattention mechanism is incorporated to the GAN network, which can generate details based on the prompts of all feature positions. Furthermore, spectrum normalization is implemented to stabilize the training of GAN, and knowledge distillation loss is introduced to process high-level feature-maps in order to better complete the crossmode segmentation task.
Results: The effectiveness of our proposed unsupervised domain adaptation framework is tested over the MultiModality Whole Heart Segmentation (MM-WHS) Challenge 2017 dataset. The proposed method is able to improve the average Dice from 74.1\% to 81.5\% for the four cardiac substructures, and reduce the average symmetric surface distance (ASD) from 7.0 to 5.8 over CT images. For MRI images, our proposed framework trained on CT images gives the average Dice of 59.2\% and reduces the average ASD from 5.7 to 4.9.
Conclusions: The evaluation results demonstrate our method’s effectiveness on domain adaptation and the superiority to the current state-of-the-art domain adaptation methods.},
	language = {en},
	urldate = {2024-11-25},
	journal = {Computers in Biology and Medicine},
	author = {Cui, Hengfei and Yuwen, Chang and Jiang, Lei and Xia, Yong and Zhang, Yanning},
	month = sep,
	year = {2021},
	pages = {104726},
}

@book{altman1988roughly,
	series = {Technical report},
	title = {Roughly sorting: {Sequential} and parallel approach},
	url = {https://books.google.co.uk/books?id=0JwWHAAACAAJ},
	publisher = {University of Kentucky, Department of Computer Science},
	author = {Altman, T. and Igarashi, Y.},
	year = {1988},
}

@article{lindsey_transformative_2015,
	title = {Transformative {Impact} of {Proteomics} on {Cardiovascular} {Health} and {Disease}: {A} {Scientific} {Statement} {From} the {American} {Heart} {Association}},
	volume = {132},
	issn = {1524-4539},
	shorttitle = {Transformative {Impact} of {Proteomics} on {Cardiovascular} {Health} and {Disease}},
	doi = {10.1161/CIR.0000000000000226},
	abstract = {The year 2014 marked the 20th anniversary of the coining of the term proteomics. The purpose of this scientific statement is to summarize advances over this period that have catalyzed our capacity to address the experimental, translational, and clinical implications of proteomics as applied to cardiovascular health and disease and to evaluate the current status of the field. Key successes that have energized the field are delineated; opportunities for proteomics to drive basic science research, facilitate clinical translation, and establish diagnostic and therapeutic healthcare algorithms are discussed; and challenges that remain to be solved before proteomic technologies can be readily translated from scientific discoveries to meaningful advances in cardiovascular care are addressed. Proteomics is the result of disruptive technologies, namely, mass spectrometry and database searching, which drove protein analysis from 1 protein at a time to protein mixture analyses that enable large-scale analysis of proteins and facilitate paradigm shifts in biological concepts that address important clinical questions. Over the past 20 years, the field of proteomics has matured, yet it is still developing rapidly. The scope of this statement will extend beyond the reaches of a typical review article and offer guidance on the use of next-generation proteomics for future scientific discovery in the basic research laboratory and clinical settings.},
	language = {eng},
	number = {9},
	journal = {Circulation},
	author = {Lindsey, Merry L. and Mayr, Manuel and Gomes, Aldrin V. and Delles, Christian and Arrell, D. Kent and Murphy, Anne M. and Lange, Richard A. and Costello, Catherine E. and Jin, Yu-Fang and Laskowitz, Daniel T. and Sam, Flora and Terzic, Andre and Van Eyk, Jennifer and Srinivas, Pothur R. and {American Heart Association Council on Functional Genomics and Translational Biology, Council on Cardiovascular Disease in the Young, Council on Clinical Cardiology, Council on Cardiovascular and Stroke Nursing, Council on Hypertension, and Stroke Council}},
	month = sep,
	year = {2015},
	pmid = {26195497},
	keywords = {AHA Scientific Statements, American Heart Association, Cardiovascular Diseases, Cardiovascular System, Health Status, Humans, Proteomics, United States, biomarkers, mass spectrometry, proteome, systems biology, translational research},
	pages = {852--872},
}

@article{zhang_deep_2019-1,
	title = {Deep {Learning} for {Diagnosis} of {Chronic} {Myocardial} {Infarction} on                    {Nonenhanced} {Cardiac} {Cine} {MRI}},
	volume = {291},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/10.1148/radiol.2019182304},
	doi = {10.1148/radiol.2019182304},
	abstract = {Background Renal impairment is common in patients with coronary artery disease and,                            if severe, late gadolinium enhancement (LGE) imaging for myocardial                            infarction (MI) evaluation cannot be performed.Purpose To develop a fully automatic framework for chronic MI delineation via                            deep learning on non–contrast material–enhanced cardiac                            cine MRI.Materials and Methods In this retrospective single-center study, a deep learning model was                            developed to extract motion features from the left ventricle and                            delineate MI regions on nonenhanced cardiac cine MRI collected between                            October 2015 and March 2017. Patients with chronic MI, as well as                            healthy control patients, had both nonenhanced cardiac cine (25 phases                            per cardiac cycle) and LGE MRI examinations. Eighty percent of MRI                            examinations were used for the training data set and 20\% for the                            independent testing data set. Chronic MI regions on LGE MRI were defined                            as ground truth. Diagnostic performance was assessed by analysis of the                            area under the receiver operating characteristic curve (AUC). MI area                            and MI area percentage from nonenhanced cardiac cine and LGE MRI were                            compared by using the Pearson correlation, paired t                            test, and Bland-Altman analysis.Results Study participants included 212 patients with chronic MI (men, 171; age,                            57.2 years ± 12.5) and 87 healthy control patients (men, 42; age,                            43.3 years ± 15.5). Using the full cardiac cine MRI, the                            per-segment sensitivity and specificity for detecting chronic MI in the                            independent test set was 89.8\% and 99.1\%, respectively, with an AUC of                            0.94. There were no differences between nonenhanced cardiac cine and LGE                            MRI analyses in number of MI segments (114 vs 127, respectively;                                P = .38), per-patient MI area (6.2                                cm2 ± 2.8 vs 5.5 cm2 ± 2.3,                            respectively; P = .27; correlation coefficient,                                r = 0.88), and MI area percentage (21.5\%                            ± 17.3 vs 18.5\% ± 15.4; P = .17;                            correlation coefficient, r = 0.89).Conclusion The proposed deep learning framework on nonenhanced cardiac cine MRI                            enables the confirmation (presence), detection (position), and                            delineation (transmurality and size) of chronic myocardial infarction.                            However, future larger-scale multicenter studies are required for a full                            validation. Published under a CC BY 4.0 license. Online supplemental material is available for this                                    article. See also the editorial by Leiner in this issue.},
	number = {3},
	urldate = {2024-11-22},
	journal = {Radiology},
	author = {Zhang, Nan and Yang, Guang and Gao, Zhifan and Xu, Chenchu and Zhang, Yanping and Shi, Rui and Keegan, Jennifer and Xu, Lei and Zhang, Heye and Fan, Zhanming and Firmin, David},
	month = jun,
	year = {2019},
	note = {Publisher: Radiological Society of North America},
	pages = {606--617},
}

@article{virani_heart_2021,
	title = {Heart {Disease} and {Stroke} {Statistics}—2021 {Update}},
	volume = {143},
	url = {https://www.ahajournals.org/doi/10.1161/CIR.0000000000000950},
	doi = {10.1161/CIR.0000000000000950},
	abstract = {Background:

The American Heart Association, in conjunction with the National Institutes of Health, annually reports the most up-to-date statistics related to heart disease, stroke, and cardiovascular risk factors, including core health behaviors (smoking, physical activity, diet, and weight) and health factors (cholesterol, blood pressure, and glucose control) that contribute to cardiovascular health. The Statistical Update presents the latest data on a range of major clinical heart and circulatory disease conditions (including stroke, congenital heart disease, rhythm disorders, subclinical atherosclerosis, coronary heart disease, heart failure, valvular disease, venous disease, and peripheral artery disease) and the associated outcomes (including quality of care, procedures, and economic costs).
Methods:

The American Heart Association, through its Statistics Committee, continuously monitors and evaluates sources of data on heart disease and stroke in the United States to provide the most current information available in the annual Statistical Update. The 2021 Statistical Update is the product of a full year’s worth of effort by dedicated volunteer clinicians and scientists, committed government professionals, and American Heart Association staff members. This year’s edition includes data on the monitoring and benefits of cardiovascular health in the population, an enhanced focus on social determinants of health, adverse pregnancy outcomes, vascular contributions to brain health, the global burden of cardiovascular disease, and further evidence-based approaches to changing behaviors related to cardiovascular disease.
Results:

Each of the 27 chapters in the Statistical Update focuses on a different topic related to heart disease and stroke statistics.
Conclusions:

The Statistical Update represents a critical resource for the lay public, policy makers, media professionals, clinicians, health care administrators, researchers, health advocates, and others seeking the best available data on these factors and conditions.},
	number = {8},
	urldate = {2024-11-22},
	journal = {Circulation},
	author = {Virani, Salim S. and Alonso, Alvaro and Aparicio, Hugo J. and Benjamin, Emelia J. and Bittencourt, Marcio S. and Callaway, Clifton W. and Carson, April P. and Chamberlain, Alanna M. and Cheng, Susan and Delling, Francesca N. and Elkind, Mitchell S.V. and Evenson, Kelly R. and Ferguson, Jane F. and Gupta, Deepak K. and Khan, Sadiya S. and Kissela, Brett M. and Knutson, Kristen L. and Lee, Chong D. and Lewis, Tené T. and Liu, Junxiu and Loop, Matthew Shane and Lutsey, Pamela L. and Ma, Jun and Mackey, Jason and Martin, Seth S. and Matchar, David B. and Mussolino, Michael E. and Navaneethan, Sankar D. and Perak, Amanda Marma and Roth, Gregory A. and Samad, Zainab and Satou, Gary M. and Schroeder, Emily B. and Shah, Svati H. and Shay, Christina M. and Stokes, Andrew and VanWagner, Lisa B. and Wang, Nae-Yuh and Tsao, Connie W. and {On behalf of the American Heart Association Council on Epidemiology and Prevention Statistics Committee and Stroke Statistics Subcommittee}},
	month = feb,
	year = {2021},
	note = {Publisher: American Heart Association},
	pages = {e254--e743},
}

@article{avard_non-contrast_2022,
	title = {Non-contrast {Cine} {Cardiac} {Magnetic} {Resonance} image radiomics features and machine learning algorithms for myocardial infarction detection},
	volume = {141},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482521009392},
	doi = {10.1016/j.compbiomed.2021.105145},
	abstract = {Objective
Robust differentiation between infarcted and normal tissue is important for clinical diagnosis and precision medicine. The aim of this work is to investigate the radiomic features and to develop a machine learning algorithm for the differentiation of myocardial infarction (MI) and viable tissues/normal cases in the left ventricular myocardium on non-contrast Cine Cardiac Magnetic Resonance (Cine-CMR) images.
Methods
Seventy-two patients (52 with MI and 20 healthy control patients) were enrolled in this study. MR imaging was performed on a 1.5 T MRI using the following parameters: TR = 43.35 ms, TE = 1.22 ms, flip angle = 65°, temporal resolution of 30–40 ms. N4 bias field correction algorithm was applied to correct the inhomogeneity of images. All images were segmented and verified simultaneously by two cardiac imaging experts in consensus. Subsequently, features extraction was performed within the whole left ventricular myocardium (3D volume) in end-diastolic volume phase. Re-sampling to 1 × 1 × 1 mm3 voxels was performed for MR images. All intensities within the VOI of MR images were discretized to 64 bins. Radiomic features were normalized to obtain Z-scores, followed by Student's t-test statistical analysis for comparison. A p-value {\textless} 0.05 was used as a threshold for statistically significant differences and false discovery rate (FDR) correction performed to report q-value (FDR adjusted p-value). The extracted features were ranked using the MSVM-RFE algorithm, then Spearman correlation between features was performed to eliminate highly correlated features (R2 {\textgreater} 0.80). Ten different machine learning algorithms were used for classification and different metrics used for evaluation and various parameters used for models' evaluation.
Results
In univariate analysis, the highest area under the curve (AUC) of receiver operating characteristic (ROC) value was achieved for the Maximum 2D diameter slice (M2DS) shape feature (AUC = 0.88, q-value = 1.02E-7), while the average of univariate AUCs was 0.62 ± 0.08. In multivariate analysis, Logistic Regression (AUC = 0.93 ± 0.03, Accuracy = 0.86 ± 0.05, Recall = 0.87 ± 0.1, Precision = 0.93 ± 0.03 and F1 Score = 0.90 ± 0.04) and SVM (AUC = 0.92 ± 0.05, Accuracy = 0.85 ± 0.04, Recall = 0.92 ± 0.01, Precision = 0.88 ± 0.04 and F1 Score = 0.90 ± 0.02) yielded optimal performance as the best machine learning algorithm for this radiomics analysis.
Conclusion
This study demonstrated that using radiomics analysis on non-contrast Cine-CMR images enables to accurately detect MI, which could potentially be used as an alternative diagnostic method for Late Gadolinium Enhancement Cardiac Magnetic Resonance (LGE-CMR).},
	urldate = {2024-11-22},
	journal = {Computers in Biology and Medicine},
	author = {Avard, Elham and Shiri, Isaac and Hajianfar, Ghasem and Abdollahi, Hamid and Kalantari, Kiara Rezaei and Houshmand, Golnaz and Kasani, Kianosh and Bitarafan-rajabi, Ahmad and Deevband, Mohammad Reza and Oveisi, Mehrdad and Zaidi, Habib},
	month = feb,
	year = {2022},
	keywords = {Cine-CMR, Machine learning, Myocardial infarction, Radiomics},
	pages = {105145},
}

@article{liu_machine_2021-1,
	title = {Machine learning-based long-term outcome prediction in patients undergoing percutaneous coronary intervention},
	volume = {11},
	issn = {2223-3660, 2223-3652},
	url = {https://cdt.amegroups.org/article/view/69917},
	doi = {10.21037/cdt-21-37},
	abstract = {Machine learning-based long-term outcome prediction in patients undergoing percutaneous coronary intervention},
	language = {en},
	number = {3},
	urldate = {2024-11-22},
	journal = {Cardiovascular Diagnosis and Therapy},
	author = {Liu, Shangyu and Yang, Shengwen and Xing, Anlu and Zheng, Lihui and Shen, Lishui and Tu, Bin and Yao, Yan},
	month = jun,
	year = {2021},
	note = {Number: 3
Publisher: AME publishing company},
	pages = {73643--73743},
}

@article{broers_usefulness_2020,
	title = {Usefulness of a {Lifestyle} {Intervention} in {Patients} {With} {Cardiovascular} {Disease}},
	volume = {125},
	issn = {0002-9149},
	url = {https://www.sciencedirect.com/science/article/pii/S0002914919312287},
	doi = {10.1016/j.amjcard.2019.10.041},
	abstract = {The importance of modifying lifestyle factors in order to improve prognosis in cardiac patients is well-known. Current study aims to evaluate the effects of a lifestyle intervention on changes in lifestyle- and health data derived from wearable devices. Cardiac patients from Spain (n = 34) and The Netherlands (n = 36) were included in the current analysis. Data were collected for 210 days, using the Fitbit activity tracker, Beddit sleep tracker, Moves app (GPS tracker), and the Careportal home monitoring system. Locally Weighted Error Sum of Squares regression assessed trajectories of outcome variables. Linear Mixed Effects regression analysis was used to find relevant predictors of improvement deterioration of outcome measures. Analysis showed that Number of Steps and Activity Level significantly changed over time (F = 58.21, p {\textless} 0.001; F = 6.33, p = 0.01). No significant changes were observed on blood pressure, weight, and sleep efficiency. Secondary analysis revealed that being male was associated with higher activity levels (F = 12.53, p {\textless} 0.001) and higher number of steps (F = 8.44, p {\textless} 0.01). Secondary analysis revealed demographic (gender, nationality, marital status), clinical (co-morbidities, heart failure), and psychological (anxiety, depression) profiles that were associated with lifestyle measures. In conclusion results showed that physical activity increased over time and that certain subgroups of patients were more likely to have a better lifestyle behaviors based on their demographic, clinical, and psychological profile. This advocates a personalized approach in future studies in order to change lifestyle in cardiac patients.},
	number = {3},
	urldate = {2024-11-22},
	journal = {The American Journal of Cardiology},
	author = {Broers, Eva R. and Gavidia, Giovana and Wetzels, Mart and Ribas, Vicent and Ayoola, Idowu and Piera-Jimenez, Jordi and Widdershoven, Jos W. M. G. and Habibović, Mirela},
	month = feb,
	year = {2020},
	pages = {370--375},
}

@misc{noauthor_broers_nodate,
	title = {Broers: {Usefulness} of a lifestyle intervention in... - {Google} {Scholar}},
	url = {https://scholar.google.com/scholar_lookup?journal=Am.%20J.%20Cardiol.&title=Usefulness%20of%20a%20Lifestyle%20Intervention%20in%20Patients%20with%20Cardiovascular%20Disease&author=E.R.%20Broers&author=G.%20Gavidia&author=M.%20Wetzels&author=V.%20Ribas&author=I.%20Ayoola&volume=125&publication_year=2019&pages=370-375&pmid=31761149&doi=10.1016/j.amjcard.2019.10.041&},
	urldate = {2024-11-22},
}

@article{bertsimas_personalized_2020,
	title = {Personalized treatment for coronary artery disease patients: a machine learning approach},
	volume = {23},
	issn = {1572-9389},
	shorttitle = {Personalized treatment for coronary artery disease patients},
	url = {https://doi.org/10.1007/s10729-020-09522-4},
	doi = {10.1007/s10729-020-09522-4},
	abstract = {Current clinical practice guidelines for managing Coronary Artery Disease (CAD) account for general cardiovascular risk factors. However, they do not present a framework that considers personalized patient-specific characteristics. Using the electronic health records of 21,460 patients, we created data-driven models for personalized CAD management that significantly improve health outcomes relative to the standard of care. We develop binary classifiers to detect whether a patient will experience an adverse event due to CAD within a 10-year time frame. Combining the patients’ medical history and clinical examination results, we achieve 81.5\% AUC. For each treatment, we also create a series of regression models that are based on different supervised machine learning algorithms. We are able to estimate with average R2 = 0.801 the outcome of interest; the time from diagnosis to a potential adverse event (TAE). Leveraging combinations of these models, we present ML4CAD, a novel personalized prescriptive algorithm. Considering the recommendations of multiple predictive models at once, the goal of ML4CAD is to identify for every patient the therapy with the best expected TAE using a voting mechanism. We evaluate its performance by measuring the prescription effectiveness and robustness under alternative ground truths. We show that our methodology improves the expected TAE upon the current baseline by 24.11\%, increasing it from 4.56 to 5.66 years. The algorithm performs particularly well for the male (24.3\% improvement) and Hispanic (58.41\% improvement) subpopulations. Finally, we create an interactive interface, providing physicians with an intuitive, accurate, readily implementable, and effective tool.},
	language = {en},
	number = {4},
	urldate = {2024-11-22},
	journal = {Health Care Management Science},
	author = {Bertsimas, Dimitris and Orfanoudaki, Agni and Weiner, Rory B.},
	month = dec,
	year = {2020},
	keywords = {Coronary artery disease, Machine learning, Personalization, Precision medicine, Prescriptions},
	pages = {482--506},
}

@article{hoogeveen_improved_2020,
	title = {Improved cardiovascular risk prediction using targeted plasma proteomics in primary prevention},
	volume = {41},
	issn = {0195-668X},
	url = {https://doi.org/10.1093/eurheartj/ehaa648},
	doi = {10.1093/eurheartj/ehaa648},
	abstract = {In the era of personalized medicine, it is of utmost importance to be able to identify subjects at the highest cardiovascular (CV) risk. To date, single biomarkers have failed to markedly improve the estimation of CV risk. Using novel technology, simultaneous assessment of large numbers of biomarkers may hold promise to improve prediction. In the present study, we compared a protein-based risk model with a model using traditional risk factors in predicting CV events in the primary prevention setting of the European Prospective Investigation (EPIC)-Norfolk study, followed by validation in the Progressione della Lesione Intimale Carotidea (PLIC) cohort.Using the proximity extension assay, 368 proteins were measured in a nested case–control sample of 822 individuals from the EPIC-Norfolk prospective cohort study and 702 individuals from the PLIC cohort. Using tree-based ensemble and boosting methods, we constructed a protein-based prediction model, an optimized clinical risk model, and a model combining both. In the derivation cohort (EPIC-Norfolk), we defined a panel of 50 proteins, which outperformed the clinical risk model in the prediction of myocardial infarction [area under the curve (AUC) 0.754 vs. 0.730; P \&lt; 0.001] during a median follow-up of 20 years. The clinically more relevant prediction of events occurring within 3 years showed an AUC of 0.732 using the clinical risk model and an AUC of 0.803 for the protein model (P \&lt; 0.001). The predictive value of the protein panel was confirmed to be superior to the clinical risk model in the validation cohort (AUC 0.705 vs. 0.609; P \&lt; 0.001).In a primary prevention setting, a proteome-based model outperforms a model comprising clinical risk factors in predicting the risk of CV events. Validation in a large prospective primary prevention cohort is required to address the value for future clinical implementation in CV prevention.},
	number = {41},
	urldate = {2024-11-22},
	journal = {European Heart Journal},
	author = {Hoogeveen, Renate M and Pereira, João P Belo and Nurmohamed, Nick S and Zampoleri, Veronica and Bom, Michiel J and Baragetti, Andrea and Boekholdt, S Matthijs and Knaapen, Paul and Khaw, Kay-Tee and Wareham, Nicholas J and Groen, Albert K and Catapano, Alberico L and Koenig, Wolfgang and Levin, Evgeni and Stroes, Erik S G},
	month = nov,
	year = {2020},
	pages = {3998--4007},
}

@article{sengupta_cognitive_2016,
	title = {Cognitive {Machine}-{Learning} {Algorithm} for {Cardiac} {Imaging}},
	volume = {9},
	url = {https://www.ahajournals.org/doi/full/10.1161/CIRCIMAGING.115.004330},
	doi = {10.1161/CIRCIMAGING.115.004330},
	abstract = {Background—Associating a patient’s profile with the memories of prototypical patients built through previous repeat clinical experience is a key process in clinical judgment. We hypothesized that a similar process using a cognitive computing tool would be well suited for learning and recalling multidimensional attributes of speckle tracking echocardiography data sets derived from patients with known constrictive pericarditis and restrictive cardiomyopathy.Methods and Results—Clinical and echocardiographic data of 50 patients with constrictive pericarditis and 44 with restrictive cardiomyopathy were used for developing an associative memory classifier–based machine-learning algorithm. The speckle tracking echocardiography data were normalized in reference to 47 controls with no structural heart disease, and the diagnostic area under the receiver operating characteristic curve of the associative memory classifier was evaluated for differentiating constrictive pericarditis from restrictive cardiomyopathy. Using only speckle tracking echocardiography variables, associative memory classifier achieved a diagnostic area under the curve of 89.2\%, which improved to 96.2\% with addition of 4 echocardiographic variables. In comparison, the area under the curve of early diastolic mitral annular velocity and left ventricular longitudinal strain were 82.1\% and 63.7\%, respectively. Furthermore, the associative memory classifier demonstrated greater accuracy and shorter learning curves than other machine-learning approaches, with accuracy asymptotically approaching 90\% after a training fraction of 0.3 and remaining flat at higher training fractions.Conclusions—This study demonstrates feasibility of a cognitive machine-learning approach for learning and recalling patterns observed during echocardiographic evaluations. Incorporation of machine-learning algorithms in cardiac imaging may aid standardized assessments and support the quality of interpretations, particularly for novice readers with limited experience.},
	number = {6},
	urldate = {2024-11-22},
	journal = {Circulation: Cardiovascular Imaging},
	author = {Sengupta, Partho P. and Huang, Yen-Min and Bansal, Manish and Ashrafi, Ali and Fisher, Matt and Shameer, Khader and Gall, Walt and Dudley, Joel T.},
	month = jun,
	year = {2016},
	note = {Publisher: American Heart Association},
	pages = {e004330},
}

@article{banerjee_completely_2021,
	title = {A completely automated pipeline for {3D} reconstruction of human heart from {2D} cine magnetic resonance slices},
	volume = {379},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2020.0257},
	doi = {10.1098/rsta.2020.0257},
	abstract = {Cardiac magnetic resonance (CMR) imaging is a valuable modality in the diagnosis and characterization of cardiovascular diseases, since it can identify abnormalities in structure and function of the myocardium non-invasively and without the need for ionizing radiation. However, in clinical practice, it is commonly acquired as a collection of separated and independent 2D image planes, which limits its accuracy in 3D analysis. This paper presents a completely automated pipeline for generating patient-specific 3D biventricular heart models from cine magnetic resonance (MR) slices. Our pipeline automatically selects the relevant cine MR images, segments them using a deep learning-based method to extract the heart contours, and aligns the contours in 3D space correcting possible misalignments due to breathing or subject motion first using the intensity and contours information from the cine data and next with the help of a statistical shape model. Finally, the sparse 3D representation of the contours is used to generate a smooth 3D biventricular mesh. The computational pipeline is applied and evaluated in a CMR dataset of 20 healthy subjects. Our results show an average reduction of misalignment artefacts from 1.82 ± 1.60 mm to 0.72 ± 0.73 mm over 20 subjects, in terms of distance from the final reconstructed mesh. The high-resolution 3D biventricular meshes obtained with our computational pipeline are used for simulations of electrical activation patterns, showing agreement with non-invasive electrocardiographic imaging. The automatic methodologies presented here for patient-specific MR imaging-based 3D biventricular representations contribute to the efficient realization of precision medicine, enabling the enhanced interpretability of clinical data, the digital twin vision through patient-specific image-based modelling and simulation, and augmented reality applications.
This article is part of the theme issue ‘Advanced computation in cardiovascular physiology: new challenges and opportunities’.},
	number = {2212},
	urldate = {2024-11-22},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Banerjee, Abhirup and Camps, Julià and Zacur, Ernesto and Andrews, Christopher M. and Rudy, Yoram and Choudhury, Robin P. and Rodriguez, Blanca and Grau, Vicente},
	month = oct,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {ECGI, cardiac mesh reconstruction, cine MRI, electrophysiological simulation, misalignment correction},
	pages = {20200257},
}

@article{mohsen_artificial_2023,
	title = {Artificial {Intelligence}-{Based} {Methods} for {Precision} {Cardiovascular} {Medicine}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-4426},
	url = {https://www.mdpi.com/2075-4426/13/8/1268},
	doi = {10.3390/jpm13081268},
	abstract = {Precision medicine has the potential to revolutionize the way cardiovascular diseases are diagnosed, predicted, and treated by tailoring treatment strategies to the individual characteristics of each patient. Artificial intelligence (AI) has recently emerged as a promising tool for improving the accuracy and efficiency of precision cardiovascular medicine. In this scoping review, we aimed to identify and summarize the current state of the literature on the use of AI in precision cardiovascular medicine. A comprehensive search of electronic databases, including Scopes, Google Scholar, and PubMed, was conducted to identify relevant studies. After applying inclusion and exclusion criteria, a total of 28 studies were included in the review. We found that AI is being increasingly applied in various areas of cardiovascular medicine, including the diagnosis, prognosis of cardiovascular diseases, risk prediction and stratification, and treatment planning. As a result, most of these studies focused on prediction (50\%), followed by diagnosis (21\%), phenotyping (14\%), and risk stratification (14\%). A variety of machine learning models were utilized in these studies, with logistic regression being the most used (36\%), followed by random forest (32\%), support vector machine (25\%), and deep learning models such as neural networks (18\%). Other models, such as hierarchical clustering (11\%), Cox regression (11\%), and natural language processing (4\%), were also utilized. The data sources used in these studies included electronic health records (79\%), imaging data (43\%), and omics data (4\%). We found that AI is being increasingly applied in various areas of cardiovascular medicine, including the diagnosis, prognosis of cardiovascular diseases, risk prediction and stratification, and treatment planning. The results of the review showed that AI has the potential to improve the performance of cardiovascular disease diagnosis and prognosis, as well as to identify individuals at high risk of developing cardiovascular diseases. However, further research is needed to fully evaluate the clinical utility and effectiveness of AI-based approaches in precision cardiovascular medicine. Overall, our review provided a comprehensive overview of the current state of knowledge in the field of AI-based methods for precision cardiovascular medicine and offered new insights for researchers interested in this research area.},
	language = {en},
	number = {8},
	urldate = {2024-11-22},
	journal = {Journal of Personalized Medicine},
	author = {Mohsen, Farida and Al-Saadi, Balqees and Abdi, Nima and Khan, Sulaiman and Shah, Zubair},
	month = aug,
	year = {2023},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, cardiovascular diseases, machine learning, precision medicine},
	pages = {1268},
}

@article{krittanawong_artificial_2017,
	title = {Artificial {Intelligence} in {Precision} {Cardiovascular} {Medicine}},
	volume = {69},
	issn = {0735-1097},
	url = {https://www.sciencedirect.com/science/article/pii/S0735109717368456},
	doi = {10.1016/j.jacc.2017.03.571},
	abstract = {Artificial intelligence (AI) is a field of computer science that aims to mimic human thought processes, learning capacity, and knowledge storage. AI techniques have been applied in cardiovascular medicine to explore novel genotypes and phenotypes in existing diseases, improve the quality of patient care, enable cost-effectiveness, and reduce readmission and mortality rates. Over the past decade, several machine-learning techniques have been used for cardiovascular disease diagnosis and prediction. Each problem requires some degree of understanding of the problem, in terms of cardiovascular medicine and statistics, to apply the optimal machine-learning algorithm. In the near future, AI will result in a paradigm shift toward precision cardiovascular medicine. The potential of AI in cardiovascular medicine is tremendous; however, ignorance of the challenges may overshadow its potential clinical impact. This paper gives a glimpse of AI’s application in cardiovascular clinical care and discusses its potential role in facilitating precision cardiovascular medicine.},
	number = {21},
	urldate = {2024-11-22},
	journal = {Journal of the American College of Cardiology},
	author = {Krittanawong, Chayakrit and Zhang, HongJu and Wang, Zhen and Aydar, Mehmet and Kitai, Takeshi},
	month = may,
	year = {2017},
	keywords = {big data, cognitive computing, deep learning, machine learning},
	pages = {2657--2664},
}

@article{gruson_collaborative_2020,
	title = {Collaborative {AI} and {Laboratory} {Medicine} integration in precision cardiovascular medicine},
	volume = {509},
	issn = {0009-8981},
	url = {https://www.sciencedirect.com/science/article/pii/S0009898120302655},
	doi = {10.1016/j.cca.2020.06.001},
	abstract = {Artificial Intelligence (AI) is a broad term that combines computation with sophisticated mathematical models and in turn allows the development of complex algorithms which are capable to simulate human intelligence such as problem solving and learning. It is devised to promote a significant paradigm shift in the most diverse areas of medical knowledge. On the other hand, Cardiology is a vast field dealing with diseases relating to the heart, the circulatory system, and includes coronary heart disease, cerebrovascular disease, rheumatic heart disease and other conditions. AI has emerged as a promising tool in cardiovascular medicine which is aimed in augmenting the effectiveness of the cardiologist and to extend better quality to patients. It has the ability to support decision‑making and improve diagnostic and prognostic performance. Attempt has also been made to explore novel genotypes and phenotypes in existing cardiovascular diseases, improve the quality of patient care, to enablecost-effectiveness with reducereadmissionand mortality rates. Our review addresses the integration of AI and laboratory medicine as an accelerator of personalization care associated with the precision and the need of value creation services in cardiovascular medicine.},
	urldate = {2024-11-22},
	journal = {Clinica Chimica Acta},
	author = {Gruson, Damien and Bernardini, Sergio and Dabla, Pradeep Kumar and Gouget, Bernard and Stankovic, Sanja},
	month = oct,
	year = {2020},
	keywords = {Artificial intelligence, Biomarkers, Cardiology, Data, Laboratory, Machine learning, Personalized},
	pages = {67--71},
}

@incollection{kattan_expert_2001,
	address = {Oxford},
	title = {Expert {Systems} in {Medicine}},
	isbn = {978-0-08-043076-8},
	url = {https://www.sciencedirect.com/science/article/pii/B0080430767005568},
	abstract = {Medical expert systems are designed to improve patient care by optimizing medical decision making. The distinguishing feature of medical expert systems is that they make recommendations based on input data; they are differentiated from decision support systems in that the latter are designed to help clinicians make decisions rather than actually make the recommendation, which is what an expert system does. This recommendation is essentially a prediction (of diagnosis or prognosis) or prescription (i.e., a treatment recommendation). The key issue in measuring the progress made thus far in medical expert systems is that of efficacy evaluation. For diagnostic or prognostic problems, the question is whether these systems have been able to predict more accurately than human experts. The literature suggests that expert systems are at least as accurate as human experts. For systems that make treatment recommendations, have medical expert systems truly improved patient outcomes? Again, it appears that validated expert systems make recommendations that are at least as good as those made by human experts, but the literature base is smaller. When trying to answer either of these questions, numerous issues need to be considered.},
	urldate = {2024-11-22},
	booktitle = {International {Encyclopedia} of the {Social} \& {Behavioral} {Sciences}},
	publisher = {Pergamon},
	author = {Kattan, M. W.},
	editor = {Smelser, Neil J. and Baltes, Paul B.},
	month = jan,
	year = {2001},
	doi = {10.1016/B0-08-043076-7/00556-8},
	pages = {5135--5139},
}

@article{mc_namara_cardiovascular_2019,
	title = {Cardiovascular disease as a leading cause of death: how are pharmacists getting involved?},
	volume = {8},
	issn = {null},
	shorttitle = {Cardiovascular disease as a leading cause of death},
	url = {https://www.tandfonline.com/doi/abs/10.2147/IPRP.S133088},
	doi = {10.2147/IPRP.S133088},
	abstract = {Cardiovascular diseases (CVDs) are a leading cause of death globally. This article explores the evidence surrounding community pharmacist interventions to reduce cardiovascular events and related mortality and to improve the management of CVD risk factors. We summarize a range of systematic reviews and leading randomized controlled trials and provide critical appraisal. Major observations are that very few trials directly measure clinical outcomes, potentially owing to a range of challenges in this regard. By contrast, there is an extensive, high-quality evidence to suggest that improvements can be achieved for key CVD risk factors such as hypertension, dyslipidemia, tobacco use, and elevated hemoglobin A1c. The heterogeneity of interventions tested and considerable variation of the context under which implementation occurred suggest that caution is warranted in the interpretation of meta-analyses. It is highly important to generate evidence for pharmacist interventions in developing countries where a majority of the global CVD burden will be experienced in the near future. A growing capacity for clinical registry trials and data linkage might allow future research to collect clinical outcomes data more often.},
	urldate = {2024-11-22},
	journal = {Integrated Pharmacy Research and Practice},
	author = {Mc Namara, Kevin and Alzubaidi, Hamzah and Jackson, John Keith},
	month = feb,
	year = {2019},
	pmid = {30788283},
	note = {Publisher: Dove Medical Press
\_eprint: https://www.tandfonline.com/doi/pdf/10.2147/IPRP.S133088},
	keywords = {cardiovascular risk factors, chronic disease management, disease screening, health services, preventative health},
	pages = {1--11},
}

@misc{noauthor_what_nodate,
	title = {What will we ask to artificial intelligence for cardiovascular medicine in the next decade? - {Minerva} {Cardiology} and {Angiology} 2022 {February};70(1):92-101},
	shorttitle = {What will we ask to artificial intelligence for cardiovascular medicine in the next decade?},
	url = {https://www.minervamedica.it/en/journals/minerva-cardiology-angiology/article.php?cod=R05Y2022N01A0092},
	abstract = {What will we ask to artificial intelligence for cardiovascular medicine in the next decade?},
	language = {en},
	urldate = {2024-11-16},
}

@article{sethi_precision_2023,
	title = {Precision {Medicine} and the future of {Cardiovascular} {Diseases}: {A} {Clinically} {Oriented} {Comprehensive} {Review}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2077-0383},
	shorttitle = {Precision {Medicine} and the future of {Cardiovascular} {Diseases}},
	url = {https://www.mdpi.com/2077-0383/12/5/1799},
	doi = {10.3390/jcm12051799},
	abstract = {Cardiac diseases form the lion’s share of the global disease burden, owing to the paradigm shift to non-infectious diseases from infectious ones. The prevalence of CVDs has nearly doubled, increasing from 271 million in 1990 to 523 million in 2019. Additionally, the global trend for the years lived with disability has doubled, increasing from 17.7 million to 34.4 million over the same period. The advent of precision medicine in cardiology has ignited new possibilities for individually personalized, integrative, and patient-centric approaches to disease prevention and treatment, incorporating the standard clinical data with advanced “omics”. These data help with the phenotypically adjudicated individualization of treatment. The major objective of this review was to compile the evolving clinically relevant tools of precision medicine that can help with the evidence-based precise individualized management of cardiac diseases with the highest DALY. The field of cardiology is evolving to provide targeted therapy, which is crafted as per the “omics”, involving genomics, transcriptomics, epigenomics, proteomics, metabolomics, and microbiomics, for deep phenotyping. Research for individualizing therapy in heart diseases with the highest DALY has helped identify novel genes, biomarkers, proteins, and technologies to aid early diagnosis and treatment. Precision medicine has helped in targeted management, allowing early diagnosis, timely precise intervention, and exposure to minimal side effects. Despite these great impacts, overcoming the barriers to implementing precision medicine requires addressing the economic, cultural, technical, and socio-political issues. Precision medicine is proposed to be the future of cardiovascular medicine and holds the potential for a more efficient and personalized approach to the management of cardiovascular diseases, contrary to the standardized blanket approach.},
	language = {en},
	number = {5},
	urldate = {2024-11-12},
	journal = {Journal of Clinical Medicine},
	author = {Sethi, Yashendra and Patel, Neil and Kaka, Nirja and Kaiwan, Oroshay and Kar, Jill and Moinuddin, Arsalan and Goel, Ashish and Chopra, Hitesh and Cavalu, Simona},
	month = jan,
	year = {2023},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cardiology, heart failure, hypertension, myocardial infarction, precision cardiology, precision medicine},
	pages = {1799},
}

@article{leopold_emerging_2018,
	title = {Emerging {Role} of {Precision} {Medicine} in {Cardiovascular} {Disease}},
	volume = {122},
	url = {https://www.ahajournals.org/doi/full/10.1161/CIRCRESAHA.117.310782},
	doi = {10.1161/CIRCRESAHA.117.310782},
	abstract = {Precision medicine is an integrative approach to cardiovascular disease prevention and treatment that considers an individual’s genetics, lifestyle, and exposures as determinants of their cardiovascular health and disease phenotypes. This focus overcomes the limitations of reductionism in medicine, which presumes that all patients with the same signs of disease share a common pathophenotype and, therefore, should be treated similarly. Precision medicine incorporates standard clinical and health record data with advanced panomics (ie, transcriptomics, epigenomics, proteomics, metabolomics, and microbiomics) for deep phenotyping. These phenotypic data can then be analyzed within the framework of molecular interaction (interactome) networks to uncover previously unrecognized disease phenotypes and relationships between diseases, and to select pharmacotherapeutics or identify potential protein–drug or drug–drug interactions. In this review, we discuss the current spectrum of cardiovascular health and disease, population averages and the response of extreme phenotypes to interventions, and population-based versus high-risk treatment strategies as a pretext to understanding a precision medicine approach to cardiovascular disease prevention and therapeutic interventions. We also consider the search for resilience and Mendelian disease genes and argue against the theory of a single causal gene/gene product as a mediator of the cardiovascular disease phenotype, as well as an Erlichian magic bullet to solve cardiovascular disease. Finally, we detail the importance of deep phenotyping and interactome networks and the use of this information for rational polypharmacy. These topics highlight the urgent need for precise phenotyping to advance precision medicine as a strategy to improve cardiovascular health and prevent disease.},
	number = {9},
	urldate = {2024-11-09},
	journal = {Circulation Research},
	author = {Leopold, Jane A. and Loscalzo, Joseph},
	month = apr,
	year = {2018},
	note = {Publisher: American Heart Association},
	pages = {1302--1315},
}

@article{krittanawong_machine_2020,
	title = {Machine learning prediction in cardiovascular diseases: a meta-analysis},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Machine learning prediction in cardiovascular diseases},
	url = {https://www.nature.com/articles/s41598-020-72685-1},
	doi = {10.1038/s41598-020-72685-1},
	abstract = {Several machine learning (ML) algorithms have been increasingly utilized for cardiovascular disease prediction. We aim to assess and summarize the overall predictive ability of ML algorithms in cardiovascular diseases. A comprehensive search strategy was designed and executed within the MEDLINE, Embase, and Scopus databases from database inception through March 15, 2019. The primary outcome was a composite of the predictive ability of ML algorithms of coronary artery disease, heart failure, stroke, and cardiac arrhythmias. Of 344 total studies identified, 103 cohorts, with a total of 3,377,318 individuals, met our inclusion criteria. For the prediction of coronary artery disease, boosting algorithms had a pooled area under the curve (AUC) of 0.88 (95\% CI 0.84–0.91), and custom-built algorithms had a pooled AUC of 0.93 (95\% CI 0.85–0.97). For the prediction of stroke, support vector machine (SVM) algorithms had a pooled AUC of 0.92 (95\% CI 0.81–0.97), boosting algorithms had a pooled AUC of 0.91 (95\% CI 0.81–0.96), and convolutional neural network (CNN) algorithms had a pooled AUC of 0.90 (95\% CI 0.83–0.95). Although inadequate studies for each algorithm for meta-analytic methodology for both heart failure and cardiac arrhythmias because the confidence intervals overlap between different methods, showing no difference, SVM may outperform other algorithms in these areas. The predictive ability of ML algorithms in cardiovascular diseases is promising, particularly SVM and boosting algorithms. However, there is heterogeneity among ML algorithms in terms of multiple parameters. This information may assist clinicians in how to interpret data and implement optimal algorithms for their dataset.},
	language = {en},
	number = {1},
	urldate = {2024-11-09},
	journal = {Scientific Reports},
	author = {Krittanawong, Chayakrit and Virk, Hafeez Ul Hassan and Bangalore, Sripal and Wang, Zhen and Johnson, Kipp W. and Pinotti, Rachel and Zhang, HongJu and Kaplin, Scott and Narasimhan, Bharat and Kitai, Takeshi and Baber, Usman and Halperin, Jonathan L. and Tang, W. H. Wilson},
	month = sep,
	year = {2020},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cardiovascular diseases, Computational biology and bioinformatics, Machine learning},
	pages = {16057},
}

@article{mathur_artificial_2020,
	title = {Artificial {Intelligence}, {Machine} {Learning}, and {Cardiovascular} {Disease}},
	volume = {14},
	issn = {1179-5468},
	url = {https://doi.org/10.1177/1179546820927404},
	doi = {10.1177/1179546820927404},
	abstract = {Artificial intelligence (AI)-based applications have found widespread applications in many fields of science, technology, and medicine. The use of enhanced computing power of machines in clinical medicine and diagnostics has been under exploration since the 1960s. More recently, with the advent of advances in computing, algorithms enabling machine learning, especially deep learning networks that mimic the human brain in function, there has been renewed interest to use them in clinical medicine. In cardiovascular medicine, AI-based systems have found new applications in cardiovascular imaging, cardiovascular risk prediction, and newer drug targets. This article aims to describe different AI applications including machine learning and deep learning and their applications in cardiovascular medicine. AI-based applications have enhanced our understanding of different phenotypes of heart failure and congenital heart disease. These applications have led to newer treatment strategies for different types of cardiovascular diseases, newer approach to cardiovascular drug therapy and postmarketing survey of prescription drugs. However, there are several challenges in the clinical use of AI-based applications and interpretation of the results including data privacy, poorly selected/outdated data, selection bias, and unintentional continuance of historical biases/stereotypes in the data which can lead to erroneous conclusions. Still, AI is a transformative technology and has immense potential in health care.},
	language = {en},
	urldate = {2024-11-09},
	journal = {Clinical Medicine Insights: Cardiology},
	author = {Mathur, Pankaj and Srivastava, Shweta and Xu, Xiaowei and Mehta, Jawahar L},
	month = jan,
	year = {2020},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {1179546820927404},
}

@article{leopold_application_2020,
	title = {The application of big data to cardiovascular disease: paths to precision medicine},
	volume = {130},
	issn = {0021-9738},
	shorttitle = {The application of big data to cardiovascular disease},
	url = {https://www.jci.org/articles/view/129203},
	doi = {10.1172/JCI129203},
	language = {en},
	number = {1},
	urldate = {2024-11-09},
	journal = {The Journal of Clinical Investigation},
	author = {Leopold, Jane A. and Maron, Bradley A. and Loscalzo, Joseph},
	month = jan,
	year = {2020},
	pmid = {0},
	note = {Publisher: American Society for Clinical Investigation},
	pages = {29--38},
}

@article{degroat_discovering_2024,
	title = {Discovering biomarkers associated and predicting cardiovascular disease with high accuracy using a novel nexus of machine learning techniques for precision medicine},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-50600-8},
	doi = {10.1038/s41598-023-50600-8},
	abstract = {Personalized interventions are deemed vital given the intricate characteristics, advancement, inherent genetic composition, and diversity of cardiovascular diseases (CVDs). The appropriate utilization of artificial intelligence (AI) and machine learning (ML) methodologies can yield novel understandings of CVDs, enabling improved personalized treatments through predictive analysis and deep phenotyping. In this study, we proposed and employed a novel approach combining traditional statistics and a nexus of cutting-edge AI/ML techniques to identify significant biomarkers for our predictive engine by analyzing the complete transcriptome of CVD patients. After robust gene expression data pre-processing, we utilized three statistical tests (Pearson correlation, Chi-square test, and ANOVA) to assess the differences in transcriptomic expression and clinical characteristics between healthy individuals and CVD patients. Next, the recursive feature elimination classifier assigned rankings to transcriptomic features based on their relation to the case–control variable. The top ten percent of commonly observed significant biomarkers were evaluated using four unique ML classifiers (Random Forest, Support Vector Machine, Xtreme Gradient Boosting Decision Trees, and k-Nearest Neighbors). After optimizing hyperparameters, the ensembled models, which were implemented using a soft voting classifier, accurately differentiated between patients and healthy individuals. We have uncovered 18 transcriptomic biomarkers that are highly significant in the CVD population that were used to predict disease with up to 96\% accuracy. Additionally, we cross-validated our results with clinical records collected from patients in our cohort. The identified biomarkers served as potential indicators for early detection of CVDs. With its successful implementation, our newly developed predictive engine provides a valuable framework for identifying patients with CVDs based on their biomarker profiles.},
	language = {en},
	number = {1},
	urldate = {2024-11-09},
	journal = {Scientific Reports},
	author = {DeGroat, William and Abdelhalim, Habiba and Patel, Kush and Mendhe, Dinesh and Zeeshan, Saman and Ahmed, Zeeshan},
	month = jan,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cardiovascular diseases, Gene expression, Genomics, Predictive medicine},
	pages = {1},
}

@article{degroat_multimodal_2024,
	title = {Multimodal {AI}/{ML} for discovering novel biomarkers and predicting disease using multi-omics profiles of patients with cardiovascular diseases},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-78553-6},
	doi = {10.1038/s41598-024-78553-6},
	abstract = {Cardiovascular diseases (CVDs) are complex, multifactorial conditions that require personalized assessment and treatment. Advancements in multi-omics technologies, namely RNA sequencing and whole-genome sequencing, have provided translational researchers with a comprehensive view of the human genome. The efficient synthesis and analysis of this data through integrated approach that characterizes genetic variants alongside expression patterns linked to emerging phenotypes, can reveal novel biomarkers and enable the segmentation of patient populations based on personalized risk factors. In this study, we present a cutting-edge methodology rooted in the integration of traditional bioinformatics, classical statistics, and multimodal machine learning techniques. Our approach has the potential to uncover the intricate mechanisms underlying CVD, enabling patient-specific risk and response profiling. We sourced transcriptomic expression data and single nucleotide polymorphisms (SNPs) from both CVD patients and healthy controls. By integrating these multi-omics datasets with clinical demographic information, we generated patient-specific profiles. Utilizing a robust feature selection approach, we identified a signature of 27 transcriptomic features and SNPs that are effective predictors of CVD. Differential expression analysis, combined with minimum redundancy maximum relevance feature selection, highlighted biomarkers that explain the disease phenotype. This approach prioritizes both biological relevance and efficiency in machine learning. We employed Combination Annotation Dependent Depletion scores and allele frequencies to identify variants with pathogenic characteristics in CVD patients. Classification models trained on this signature demonstrated high-accuracy predictions for CVD. The best performing of these models was an XGBoost classifier optimized via Bayesian hyperparameter tuning, which was able to correctly classify all patients in our test dataset. Using SHapley Additive exPlanations, we created risk assessments for patients, offering further contextualization of these predictions in a clinical setting. Across the cohort, RPL36AP37 and HBA1 were scored as the most important biomarkers for predicting CVDs. A comprehensive literature review revealed that a substantial portion of the diagnostic biomarkers identified have previously been associated with CVD. The framework we propose in this study is unbiased and generalizable to other diseases and disorders.},
	language = {en},
	number = {1},
	urldate = {2024-11-09},
	journal = {Scientific Reports},
	author = {DeGroat, William and Abdelhalim, Habiba and Peker, Elizabeth and Sheth, Neev and Narayanan, Rishabh and Zeeshan, Saman and Liang, Bruce T. and Ahmed, Zeeshan},
	month = nov,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cardiovascular diseases, Personalized medicine},
	pages = {26503},
}

@article{kuiken_artificial_nodate,
	title = {Artificial {Intelligence} in the {Biological} {Sciences}: {Uses}, {Safety}, {Security}, and {Oversight}},
	abstract = {Artificial Intelligence in the Biological Sciences: Uses, Safety, Security, and Oversight Artificial intelligence (AI) is a term generally thought of as computerized systems that work and react in ways commonly thought to require intelligence. AI technologies, methodologies, and applications can be used throughout the biological sciences and biology research and development (R\&D), including in engineering biology (e.g., the application of engineering principles and the use of systematic design tools to reprogram cellular systems for a specific functional output). This has enabled R\&D advances across multiple application areas and industries. For example, AI can be used to analyze genomic data (e.g., DNA sequences) to determine the genetic basis of a particular trait and potentially uncover genetic markers linked with those traits. It has also been used in combination with biological design tools to aid in characterizing proteins (e.g., 3-D structure) and for designing new chemical structures that can enable specific medical applications, including for drug discovery. AI can also be used across the scientific R\&D process, including the design of laboratory experiments, protocols to run certain laboratory equipment, and other “de-skilling” aspects of scientific research. The convergence of AI and other technologies associated with biology can lower technical and knowledge barriers and increase the number of actors with certain capabilities. These capabilities have potential for beneficial uses while at the same time raising certain biosafety and biosecurity concerns. For example, some have argued that using AI for biological design can be repurposed or misused to potentially produce biological and chemical compounds of concern.},
	language = {en},
	author = {Kuiken, Todd},
}

@article{bhardwaj_artificial_2022,
	title = {Artificial {Intelligence} in {Biological} {Sciences}},
	volume = {12},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC9505413/},
	doi = {10.3390/life12091430},
	abstract = {Artificial intelligence (AI), currently a cutting-edge concept, has the potential to improve the quality of life of human beings. The fields of AI and biological research are becoming more intertwined, and methods for extracting and applying the ...},
	language = {en},
	number = {9},
	urldate = {2024-11-09},
	journal = {Life},
	author = {Bhardwaj, Abhaya and Kishore, Shristi and Pandey, Dhananjay K.},
	month = sep,
	year = {2022},
	pmid = {36143468},
	pages = {1430},
}

@article{noauthor_embedding_2024,
	title = {Embedding {AI} in biology},
	volume = {21},
	copyright = {2024 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-024-02391-7},
	doi = {10.1038/s41592-024-02391-7},
	abstract = {Advanced artificial intelligence approaches are rapidly transforming how biological data are acquired and analyzed.},
	language = {en},
	number = {8},
	urldate = {2024-11-09},
	journal = {Nature Methods},
	month = aug,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cancer, Genetic engineering, Genomics, Imaging, Immunology, Machine learning, Microscopy, Neuroscience, Protein structure predictions},
	pages = {1365--1366},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2024-11-09},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
}
